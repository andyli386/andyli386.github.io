[{"title":"Handwritten digit recognition with tflearn exercise","date":"2017-02-14T09:41:20.000Z","path":"2017/02/14/Handwritten-digit-recognition-with-tflearn-exercise/","text":"Handwritten Number Recognition with TFLearn and MNISTIn this notebook, we’ll be building a neural network that recognizes handwritten numbers 0-9. This kind of neural network is used in a variety of real-world applications including: recognizing phone numbers and sorting postal mail by address. To build the network, we’ll be using the MNIST data set, which consists of images of handwritten numbers and their correct labels 0-9. We’ll be using TFLearn, a high-level library built on top of TensorFlow to build the neural network. We’ll start off by importing all the modules we’ll need, then load the data, and finally build the network. 12345# Import Numpy, TensorFlow, TFLearn, and MNIST dataimport numpy as npimport tensorflow as tfimport tflearnimport tflearn.datasets.mnist as mnist Retrieving training and test dataThe MNIST data set already contains both training and test data. There are 55,000 data points of training data, and 10,000 points of test data. Each MNIST data point has: an image of a handwritten digit and a corresponding label (a number 0-9 that identifies the image) We’ll call the images, which will be the input to our neural network, X and their corresponding labels Y. We’re going to want our labels as one-hot vectors, which are vectors that holds mostly 0’s and one 1. It’s easiest to see this in a example. As a one-hot vector, the number 0 is represented as [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], and 4 is represented as [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. Flattened dataFor this example, we’ll be using flattened data or a representation of MNIST images in one dimension rather than two. So, each handwritten number image, which is 28x28 pixels, will be represented as a one dimensional array of 784 pixel values. Flattening the data throws away information about the 2D structure of the image, but it simplifies our data so that all of the training data can be contained in one array whose shape is [55000, 784]; the first dimension is the number of training images and the second dimension is the number of pixels in each image. This is the kind of data that is easy to analyze using a simple neural network. 12import tensorflow as tf;print (tf.__version__) 0.12.1 123# Retrieve the training and test datamnist.SOURCE_URL = 'https://web.archive.org/web/20160117040036/http://yann.lecun.com/exdb/mnist/'trainX, trainY, testX, testY = mnist.load_data(one_hot=True) Extracting mnist/train-images-idx3-ubyte.gz Extracting mnist/train-labels-idx1-ubyte.gz Extracting mnist/t10k-images-idx3-ubyte.gz Extracting mnist/t10k-labels-idx1-ubyte.gz Visualize the training dataProvided below is a function that will help you visualize the MNIST data. By passing in the index of a training example, the function show_digit will display that training image along with it’s corresponding label in the title. 123456789101112131415# Visualizing the dataimport matplotlib.pyplot as plt%matplotlib inline# Function for displaying a training image by it's index in the MNIST setdef show_digit(index): label = trainY[index].argmax(axis=0) # Reshape 784 array into 28x28 image image = trainX[index].reshape([28,28]) plt.title('Training data, index: %d, Label: %d' % (index, label)) plt.imshow(image, cmap='gray_r') plt.show() # Display the first (index 0) training imageshow_digit(1) Building the networkTFLearn lets you build the network by defining the layers in that network. For this example, you’ll define: The input layer, which tells the network the number of inputs it should expect for each piece of MNIST data. Hidden layers, which recognize patterns in data and connect the input to the output layer, and The output layer, which defines how the network learns and outputs a label for a given image. Let’s start with the input layer; to define the input layer, you’ll define the type of data that the network expects. For example, 1net = tflearn.input_data([None, 100]) would create a network with 100 inputs. The number of inputs to your network needs to match the size of your data. For this example, we’re using 784 element long vectors to encode our input data, so we need 784 input units. Adding layersTo add new hidden layers, you use 1net = tflearn.fully_connected(net, n_units, activation=&apos;ReLU&apos;) This adds a fully connected layer where every unit (or node) in the previous layer is connected to every unit in this layer. The first argument net is the network you created in the tflearn.input_data call, it designates the input to the hidden layer. You can set the number of units in the layer with n_units, and set the activation function with the activation keyword. You can keep adding layers to your network by repeated calling tflearn.fully_connected(net, n_units). Then, to set how you train the network, use: 1net = tflearn.regression(net, optimizer=&apos;sgd&apos;, learning_rate=0.1, loss=&apos;categorical_crossentropy&apos;) Again, this is passing in the network you’ve been building. The keywords: optimizer sets the training method, here stochastic gradient descent learning_rate is the learning rate loss determines how the network error is calculated. In this example, with categorical cross-entropy. Finally, you put all this together to create the model with tflearn.DNN(net). Exercise: Below in the build_model() function, you’ll put together the network using TFLearn. You get to choose how many layers to use, how many hidden units, etc. Hint: The final output layer must have 10 output nodes (one for each digit 0-9). It’s also recommended to use a softmax activation layer as your final output layer. 123456789101112131415# Define the neural networkdef build_model(): # This resets all parameters and variables, leave this here tf.reset_default_graph() #### Your code #### # Include the input layer, hidden layer(s), and set how you want to train the model net = tflearn.input_data([None, 784]) net = tflearn.fully_connected(net, 100, activation='ReLU') #net = tflearn.fully_connected(net, 20, activation='ReLU') net = tflearn.fully_connected(net, 10, activation='softmax') net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy') # This model assumes that your network is named \"net\" model = tflearn.DNN(net) return model 12# Build the modelmodel = build_model() Training the networkNow that we’ve constructed the network, saved as the variable model, we can fit it to the data. Here we use the model.fit method. You pass in the training features trainX and the training targets trainY. Below I set validation_set=0.1 which reserves 10% of the data set as the validation set. You can also set the batch size and number of epochs with the batch_size and n_epoch keywords, respectively. Too few epochs don’t effectively train your network, and too many take a long time to execute. Choose wisely! 12# Trainingmodel.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=100, n_epoch=8) Training Step: 3959 | total loss: \u001b[1m\u001b[32m0.20021\u001b[0m\u001b[0m | time: 1.809s | SGD | epoch: 008 | loss: 0.20021 - acc: 0.9639 -- iter: 49400/49500 Training Step: 3960 | total loss: \u001b[1m\u001b[32m0.19631\u001b[0m\u001b[0m | time: 2.826s | SGD | epoch: 008 | loss: 0.19631 - acc: 0.9645 | val_loss: 0.14357 - val_acc: 0.9600 -- iter: 49500/49500 -- TestingAfter you’re satisified with the training output and accuracy, you can then run the network on the test data set to measure it’s performance! Remember, only do this after you’ve done the training and are satisfied with the results. A good result will be higher than 98% accuracy! Some simple models have been known to get up to 99.7% accuracy. 12345678# Compare the labels that our model predicts with the actual labelspredictions = (np.array(model.predict(testX))[:,0] &gt;= 0.5).astype(np.int_)# Calculate the accuracy, which is the percentage of times the predicated labels matched the actual labelstest_accuracy = np.mean(predictions == testY[:,0], axis=0)# Print out the resultprint(\"Test accuracy: \", test_accuracy) Test accuracy: 0.9951 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[]},{"title":"Sentiment analysis with TFLearn","date":"2017-02-13T11:08:53.000Z","path":"2017/02/13/Sentiment-analysis-with-TFLearn/","text":"Sentiment analysis with TFLearnIn this notebook, we’ll continue Andrew Trask’s work by building a network for sentiment analysis on the movie review data. Instead of a network written with Numpy, we’ll be using TFLearn, a high-level library built on top of TensorFlow. TFLearn makes it simpler to build networks just by defining the layers. It takes care of most of the details for you. We’ll start off by importing all the modules we’ll need, then load and prepare the data. 12345import pandas as pdimport numpy as npimport tensorflow as tfimport tflearnfrom tflearn.data_utils import to_categorical Preparing the dataFollowing along with Andrew, our goal here is to convert our reviews into word vectors. The word vectors will have elements representing words in the total vocabulary. If the second position represents the word ‘the’, for each review we’ll count up the number of times ‘the’ appears in the text and set the second position to that count. I’ll show you examples as we build the input data from the reviews data. Check out Andrew’s notebook and video for more about this. 12reviews = pd.read_csv('reviews.txt', header=None)labels = pd.read_csv('labels.txt', header=None) Counting word frequencyTo start off we’ll need to count how often each word appears in the data. We’ll use this count to create a vocabulary we’ll use to encode the review data. This resulting count is known as a bag of words. We’ll use it to select our vocabulary and build the word vectors. You should have seen how to do this in Andrew’s lesson. Try to implement it here using the Counter class. Exercise: Create the bag of words from the reviews data and assign it to total_counts. The reviews are stores in the reviews Pandas DataFrame. If you want the reviews as a Numpy array, use reviews.values. You can iterate through the rows in the DataFrame with for idx, row in reviews.iterrows(): (documentation). 123456789from collections import Countertotal_counts = Counter()for i in range(len(reviews)): for word in reviews.values[i][0].split(\" \"): total_counts[word] += 1print(\"Total words in data set: \", len(total_counts)) Total words in data set: 74074 1234567from collections import Countertotal_counts1 = Counter()for ii, (_, review) in enumerate(reviews.iterrows()): for word in review[0].split(\" \"): total_counts1[word] += 1print(\"Total words in data set: \", len(total_counts)) Total words in data set: 74074 1assert(total_counts == total_counts1) Let’s keep the first 10000 most frequent words. As Andrew noted, most of the words in the vocabulary are rarely used so they will have little effect on our predictions. Below, we’ll sort vocab by the count value and keep the 10000 most frequent words. 12vocab = sorted(total_counts, key=total_counts.get, reverse=True)[:10000]print(vocab[:60]) [&apos;&apos;, &apos;the&apos;, &apos;.&apos;, &apos;and&apos;, &apos;a&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;, &apos;br&apos;, &apos;it&apos;, &apos;in&apos;, &apos;i&apos;, &apos;this&apos;, &apos;that&apos;, &apos;s&apos;, &apos;was&apos;, &apos;as&apos;, &apos;for&apos;, &apos;with&apos;, &apos;movie&apos;, &apos;but&apos;, &apos;film&apos;, &apos;you&apos;, &apos;on&apos;, &apos;t&apos;, &apos;not&apos;, &apos;he&apos;, &apos;are&apos;, &apos;his&apos;, &apos;have&apos;, &apos;be&apos;, &apos;one&apos;, &apos;all&apos;, &apos;at&apos;, &apos;they&apos;, &apos;by&apos;, &apos;an&apos;, &apos;who&apos;, &apos;so&apos;, &apos;from&apos;, &apos;like&apos;, &apos;there&apos;, &apos;her&apos;, &apos;or&apos;, &apos;just&apos;, &apos;about&apos;, &apos;out&apos;, &apos;if&apos;, &apos;has&apos;, &apos;what&apos;, &apos;some&apos;, &apos;good&apos;, &apos;can&apos;, &apos;more&apos;, &apos;she&apos;, &apos;when&apos;, &apos;very&apos;, &apos;up&apos;, &apos;time&apos;, &apos;no&apos;] What’s the last word in our vocabulary? We can use this to judge if 10000 is too few. If the last word is pretty common, we probably need to keep more words. 1print(vocab[-1], ': ', total_counts[vocab[-1]]) excesses : 30 The last word in our vocabulary shows up in 30 reviews out of 25000. I think it’s fair to say this is a tiny proportion of reviews. We are probably fine with this number of words. Now for each review in the data, we’ll make a word vector. First we need to make a mapping of word to index, pretty easy to do with a dictionary comprehension. Exercise: Create a dictionary called word2idx that maps each word in the vocabulary to an index. The first word in vocab has index 0, the second word has index 1, and so on. 123word2idx = &#123;&#125; ## create the word-to-index dictionary herefor i, word in enumerate(vocab): word2idx[word] = i Text to vector functionNow we can write a function that converts a some text to a word vector. The function will take a string of words as input and return a vector with the words counted up. Here’s the general algorithm to do this: Initialize the word vector with np.zeros, it should be the length of the vocabulary. Split the input string of text into a list of words with .split(&#39; &#39;). For each word in that list, increment the element in the index associated with that word, which you get from word2idx. Note: Since all words aren’t in the vocab dictionary, you’ll get a key error if you run into one of those words. You can use the .get method of the word2idx dictionary to specify a default returned value when you make a key error. For example, word2idx.get(word, None) returns None if word doesn’t exist in the dictionary. 12len('The tea is for a party to celebrate ' 'the movie so she has no time for a cake') 75 12345678910def text_to_vector(text): word_vector = np.zeros(len(vocab), dtype=np.int_) for word in text.split(' '): idx = word2idx.get(word, None) if idx != None: word_vector[idx] += 1 return word_vector If you do this right, the following code should return 123456789101112131415text_to_vector(&apos;The tea is for a party to celebrate &apos; &apos;the movie so she has no time for a cake&apos;)[:65] array([0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])``` ```pythontest_example = np.array([0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])assert((text_to_vector(&apos;The tea is for a party to celebrate &apos; &apos;the movie so she has no time for a cake&apos;)[:65] == test_example).all()) Now, run through our entire review data set and convert each review to a word vector. 12345word_vectors = np.zeros((len(reviews), len(vocab)), dtype=np.int_)for ii, (_, text) in enumerate(reviews.iterrows()): #print(a, text) word_vectors[ii] = text_to_vector(text[0]) 1reviews.iterrows() &lt;generator object DataFrame.iterrows at 0x119410308&gt; 12# Printing out the first 5 word vectorsword_vectors[:5, :23] array([[ 18, 9, 27, 1, 4, 4, 6, 4, 0, 2, 2, 5, 0, 4, 1, 0, 2, 0, 0, 0, 0, 0, 0], [ 5, 4, 8, 1, 7, 3, 1, 2, 0, 4, 0, 0, 0, 1, 2, 0, 0, 1, 3, 0, 0, 0, 1], [ 78, 24, 12, 4, 17, 5, 20, 2, 8, 8, 2, 1, 1, 2, 8, 0, 5, 5, 4, 0, 2, 1, 4], [167, 53, 23, 0, 22, 23, 13, 14, 8, 10, 8, 12, 9, 4, 11, 2, 11, 5, 11, 0, 5, 3, 0], [ 19, 10, 11, 4, 6, 2, 2, 5, 0, 1, 2, 3, 1, 0, 0, 0, 3, 1, 0, 1, 0, 0, 0]]) Train, Validation, Test setsNow that we have the word_vectors, we’re ready to split our data into train, validation, and test sets. Remember that we train on the train data, use the validation data to set the hyperparameters, and at the very end measure the network performance on the test data. Here we’re using the function to_categorical from TFLearn to reshape the target data so that we’ll have two output units and can classify with a softmax activation function. We actually won’t be creating the validation set here, TFLearn will do that for us later. 12345678910111213Y = (labels=='positive').astype(np.int_)records = len(labels)shuffle = np.arange(records)np.random.shuffle(shuffle)test_fraction = 0.9train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]trainX, trainY = word_vectors[train_split,:], to_categorical(Y.values[train_split], 2)testX, testY = word_vectors[test_split,:], to_categorical(Y.values[test_split], 2) 1trainY array([[ 0., 1.], [ 0., 1.], [ 0., 1.], ..., [ 0., 1.], [ 0., 1.], [ 0., 1.]]) Building the networkTFLearn lets you build the network by defining the layers. Input layerFor the input layer, you just need to tell it how many units you have. For example, 1net = tflearn.input_data([None, 100]) would create a network with 100 input units. The first element in the list, None in this case, sets the batch size. Setting it to None here leaves it at the default batch size. The number of inputs to your network needs to match the size of your data. For this example, we’re using 10000 element long vectors to encode our input data, so we need 10000 input units. Adding layersTo add new hidden layers, you use 1net = tflearn.fully_connected(net, n_units, activation=&apos;ReLU&apos;) This adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer. The first argument net is the network you created in the tflearn.input_data call. It’s telling the network to use the output of the previous layer as the input to this layer. You can set the number of units in the layer with n_hidden, and set the activation function with the activation keyword. You can keep adding layers to your network by repeated calling net = tflearn.fully_connected(net, n_units). Output layerThe last layer you add is used as the output layer. There for, you need to set the number of units to match the target data. In this case we are predicting two classes, positive or negative sentiment. You also need to set the activation function so it’s appropriate for your model. Again, we’re trying to predict if some input data belongs to one of two classes, so we should use softmax. 1net = tflearn.fully_connected(net, 2, activation=&apos;softmax&apos;) TrainingTo set how you train the network, use 1net = tflearn.regression(net, optimizer=&apos;sgd&apos;, learning_rate=0.1, loss=&apos;categorical_crossentropy&apos;) Again, this is passing in the network you’ve been building. The keywords: optimizer sets the training method, here stochastic gradient descent learning_rate is the learning rate loss determines how the network error is calculated. In this example, with the categorical cross-entropy. Finally you put all this together to create the model with tflearn.DNN(net). So it ends up looking something like 12345net = tflearn.input_data([None, 10]) # Inputnet = tflearn.fully_connected(net, 5, activation=&apos;ReLU&apos;) # Hiddennet = tflearn.fully_connected(net, 2, activation=&apos;softmax&apos;) # Outputnet = tflearn.regression(net, optimizer=&apos;sgd&apos;, learning_rate=0.1, loss=&apos;categorical_crossentropy&apos;)model = tflearn.DNN(net) Exercise: Below in the build_model() function, you’ll put together the network using TFLearn. You get to choose how many layers to use, how many hidden units, etc. 1234567891011121314# Network buildingdef build_model(): # This resets all parameters and variables, leave this here tf.reset_default_graph() #### Your code #### net = tflearn.input_data([None, 10000]) # Input net = tflearn.fully_connected(net, 200, activation='ReLU') # Hidden net = tflearn.fully_connected(net, 25, activation='ReLU') net = tflearn.fully_connected(net, 2, activation='softmax') # Output net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy') model = tflearn.DNN(net) return model Intializing the modelNext we need to call the build_model() function to actually build the model. In my solution I haven’t included any arguments to the function, but you can add arguments so you can change parameters in the model if you want. Note: You might get a bunch of warnings here. TFLearn uses a lot of deprecated code in TensorFlow. Hopefully it gets updated to the new TensorFlow version soon. 1model = build_model() WARNING:tensorflow:From /Users/vincent/anaconda/envs/tflearn/lib/python3.5/site-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30. Instructions for updating: Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported. WARNING:tensorflow:From /Users/vincent/anaconda/envs/tflearn/lib/python3.5/site-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30. Instructions for updating: Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported. WARNING:tensorflow:From /Users/vincent/anaconda/envs/tflearn/lib/python3.5/site-packages/tflearn/helpers/trainer.py:766 in create_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30. Instructions for updating: Please switch to tf.summary.merge. WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02. WARNING:tensorflow:From /Users/vincent/anaconda/envs/tflearn/lib/python3.5/site-packages/tflearn/helpers/trainer.py:130 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02. Instructions for updating: Use `tf.global_variables_initializer` instead. Training the networkNow that we’ve constructed the network, saved as the variable model, we can fit it to the data. Here we use the model.fit method. You pass in the training features trainX and the training targets trainY. Below I set validation_set=0.1 which reserves 10% of the data set as the validation set. You can also set the batch size and number of epochs with the batch_size and n_epoch keywords, respectively. Below is the code to fit our the network to our word vectors. You can rerun model.fit to train the network further if you think you can increase the validation accuracy. Remember, all hyperparameter adjustments must be done using the validation set. Only use the test set after you’re completely done training the network. 12# Trainingmodel.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=100) Training Step: 63853 | total loss: \u001b[1m\u001b[32m0.00783\u001b[0m\u001b[0m | SGD | epoch: 100 | loss: 0.00783 - acc: 0.9984 | val_loss: 0.05305 - val_acc: 0.9827 -- iter: 20250/20250 Training Step: 63853 | total loss: \u001b[1m\u001b[32m0.00783\u001b[0m\u001b[0m | SGD | epoch: 100 | loss: 0.00783 - acc: 0.9984 | val_loss: 0.05305 - val_acc: 0.9827 -- iter: 20250/20250 -- TestingAfter you’re satisified with your hyperparameters, you can run the network on the test set to measure it’s performance. Remember, only do this after finalizing the hyperparameters. 123predictions = (np.array(model.predict(testX))[:,0] &gt;= 0.5).astype(np.int_)test_accuracy = np.mean(predictions == testY[:,0], axis=0)print(\"Test accuracy: \", test_accuracy) Test accuracy: 0.864 Try out your own text!1234text = \"This movie is so bad. It was awful and the worst\"positive_prob = model.predict([text_to_vector(text.lower())])[0][1]print('P(positive) = &#123;:.3f&#125; :'.format(positive_prob), 'Positive' if positive_prob &gt; 0.5 else 'Negative') P(positive) = 0.000 : Negative 1234text = \"This food is so delicious. It was wonderful\"positive_prob = model.predict([text_to_vector(text.lower())])[0][1]print('P(positive) = &#123;:.3f&#125; :'.format(positive_prob), 'Positive' if positive_prob &gt; 0.5 else 'Negative') P(positive) = 1.000 : Positive 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[{"name":"纳米学位","slug":"纳米学位","permalink":"http://52ml.me/tags/纳米学位/"},{"name":"DLND","slug":"DLND","permalink":"http://52ml.me/tags/DLND/"},{"name":"机器学习","slug":"机器学习","permalink":"http://52ml.me/tags/机器学习/"}]},{"title":"How To \"Frame Problems\" for a Neural Network","date":"2017-02-12T13:01:02.000Z","path":"2017/02/12/How-To-Frame-Problems-for-a-Neural-Network/","text":"Sentiment Classification &amp; How To “Frame Problems” for a Neural Networkby Andrew Trask Twitter: @iamtrask Blog: http://iamtrask.github.io What You Should Already Know neural networks, forward and back-propagation stochastic gradient descent mean squared error and train/test splits Where to Get Help if You Need it Re-watch previous Udacity Lectures Leverage the recommended Course Reading Material - Grokking Deep Learning (40% Off: traskud17) Shoot me a tweet @iamtrask Tutorial Outline: Intro: The Importance of “Framing a Problem” Curate a Dataset Developing a “Predictive Theory” PROJECT 1: Quick Theory Validation Transforming Text to Numbers PROJECT 2: Creating the Input/Output Data Putting it all together in a Neural Network PROJECT 3: Building our Neural Network Understanding Neural Noise PROJECT 4: Making Learning Faster by Reducing Noise Analyzing Inefficiencies in our Network PROJECT 5: Making our Network Train and Run Faster Further Noise Reduction PROJECT 6: Reducing Noise by Strategically Reducing the Vocabulary Analysis: What’s going on in the weights? Lesson: Curate a Dataset12345678910def pretty_print_review_and_label(i): print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")g = open('reviews.txt','r') # What we know!reviews = list(map(lambda x:x[:-1],g.readlines()))g.close()g = open('labels.txt','r') # What we WANT to know!labels = list(map(lambda x:x[:-1].upper(),g.readlines()))g.close() 1len(reviews) 25000 1reviews[0] &apos;bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life such as teachers . my years in the teaching profession lead me to believe that bromwell high s satire is much closer to reality than is teachers . the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn t &apos; 1labels[0] &apos;POSITIVE&apos; Lesson: Develop a Predictive Theory1234567print(\"labels.txt \\t : \\t reviews.txt\\n\")pretty_print_review_and_label(2137)pretty_print_review_and_label(12816)pretty_print_review_and_label(6267)pretty_print_review_and_label(21934)pretty_print_review_and_label(5297)pretty_print_review_and_label(4998) labels.txt : reviews.txt NEGATIVE : this movie is terrible but it has some good effects . ... POSITIVE : adrian pasdar is excellent is this film . he makes a fascinating woman . ... NEGATIVE : comment this movie is impossible . is terrible very improbable bad interpretat... POSITIVE : excellent episode movie ala pulp fiction . days suicides . it doesnt get more... NEGATIVE : if you haven t seen this it s terrible . it is pure trash . i saw this about ... POSITIVE : this schiffer guy is a real genius the movie is of excellent quality and both e... Project 1: Quick Theory Validation12from collections import Counterimport numpy as np 123positive_counts = Counter()negative_counts = Counter()total_counts = Counter() 123456789for i in range(len(reviews)): if(labels[i] == 'POSITIVE'): for word in reviews[i].split(\" \"): positive_counts[word] += 1 total_counts[word] += 1 else: for word in reviews[i].split(\" \"): negative_counts[word] += 1 total_counts[word] += 1 1positive_counts.most_common() [(&apos;&apos;, 550468), (&apos;the&apos;, 173324), (&apos;.&apos;, 159654), (&apos;and&apos;, 89722), (&apos;a&apos;, 83688), (&apos;of&apos;, 76855), (&apos;to&apos;, 66746), (&apos;is&apos;, 57245), (&apos;in&apos;, 50215), (&apos;br&apos;, 49235), (&apos;it&apos;, 48025), (&apos;i&apos;, 40743), (&apos;that&apos;, 35630), (&apos;this&apos;, 35080), (&apos;s&apos;, 33815), (&apos;as&apos;, 26308), (&apos;with&apos;, 23247), (&apos;for&apos;, 22416), (&apos;was&apos;, 21917), (&apos;film&apos;, 20937), (&apos;but&apos;, 20822), (&apos;movie&apos;, 19074), (&apos;his&apos;, 17227), (&apos;on&apos;, 17008), (&apos;you&apos;, 16681), (&apos;he&apos;, 16282), (&apos;are&apos;, 14807), (&apos;not&apos;, 14272), (&apos;t&apos;, 13720), (&apos;one&apos;, 13655), (&apos;have&apos;, 12587), (&apos;be&apos;, 12416), (&apos;by&apos;, 11997), (&apos;all&apos;, 11942), (&apos;who&apos;, 11464), (&apos;an&apos;, 11294), (&apos;at&apos;, 11234), (&apos;from&apos;, 10767), (&apos;her&apos;, 10474), (&apos;they&apos;, 9895), (&apos;has&apos;, 9186), (&apos;so&apos;, 9154), (&apos;like&apos;, 9038), (&apos;about&apos;, 8313), (&apos;very&apos;, 8305), (&apos;out&apos;, 8134), (&apos;there&apos;, 8057), (&apos;she&apos;, 7779), (&apos;what&apos;, 7737), (&apos;or&apos;, 7732), (&apos;good&apos;, 7720), (&apos;more&apos;, 7521), (&apos;when&apos;, 7456), (&apos;some&apos;, 7441), (&apos;if&apos;, 7285), (&apos;just&apos;, 7152), (&apos;can&apos;, 7001), (&apos;story&apos;, 6780), (&apos;time&apos;, 6515), (&apos;my&apos;, 6488), (&apos;great&apos;, 6419), (&apos;well&apos;, 6405), (&apos;up&apos;, 6321), (&apos;which&apos;, 6267), (&apos;their&apos;, 6107), (&apos;see&apos;, 6026), (&apos;also&apos;, 5550), (&apos;we&apos;, 5531), (&apos;really&apos;, 5476), (&apos;would&apos;, 5400), (&apos;will&apos;, 5218), (&apos;me&apos;, 5167), (&apos;had&apos;, 5148), (&apos;only&apos;, 5137), (&apos;him&apos;, 5018), (&apos;even&apos;, 4964), (&apos;most&apos;, 4864), (&apos;other&apos;, 4858), (&apos;were&apos;, 4782), (&apos;first&apos;, 4755), (&apos;than&apos;, 4736), (&apos;much&apos;, 4685), (&apos;its&apos;, 4622), (&apos;no&apos;, 4574), (&apos;into&apos;, 4544), (&apos;people&apos;, 4479), (&apos;best&apos;, 4319), (&apos;love&apos;, 4301), (&apos;get&apos;, 4272), (&apos;how&apos;, 4213), (&apos;life&apos;, 4199), (&apos;been&apos;, 4189), (&apos;because&apos;, 4079), (&apos;way&apos;, 4036), (&apos;do&apos;, 3941), (&apos;made&apos;, 3823), (&apos;films&apos;, 3813), (&apos;them&apos;, 3805), (&apos;after&apos;, 3800), (&apos;many&apos;, 3766), (&apos;two&apos;, 3733), (&apos;too&apos;, 3659), (&apos;think&apos;, 3655), (&apos;movies&apos;, 3586), (&apos;characters&apos;, 3560), (&apos;character&apos;, 3514), (&apos;don&apos;, 3468), (&apos;man&apos;, 3460), (&apos;show&apos;, 3432), (&apos;watch&apos;, 3424), (&apos;seen&apos;, 3414), (&apos;then&apos;, 3358), (&apos;little&apos;, 3341), (&apos;still&apos;, 3340), (&apos;make&apos;, 3303), (&apos;could&apos;, 3237), (&apos;never&apos;, 3226), (&apos;being&apos;, 3217), (&apos;where&apos;, 3173), (&apos;does&apos;, 3069), (&apos;over&apos;, 3017), (&apos;any&apos;, 3002), (&apos;while&apos;, 2899), (&apos;know&apos;, 2833), (&apos;did&apos;, 2790), (&apos;years&apos;, 2758), (&apos;here&apos;, 2740), (&apos;ever&apos;, 2734), (&apos;end&apos;, 2696), (&apos;these&apos;, 2694), (&apos;such&apos;, 2590), (&apos;real&apos;, 2568), (&apos;scene&apos;, 2567), (&apos;back&apos;, 2547), (&apos;those&apos;, 2485), (&apos;though&apos;, 2475), (&apos;off&apos;, 2463), (&apos;new&apos;, 2458), (&apos;your&apos;, 2453), (&apos;go&apos;, 2440), (&apos;acting&apos;, 2437), (&apos;plot&apos;, 2432), (&apos;world&apos;, 2429), (&apos;scenes&apos;, 2427), (&apos;say&apos;, 2414), (&apos;through&apos;, 2409), (&apos;makes&apos;, 2390), (&apos;better&apos;, 2381), (&apos;now&apos;, 2368), (&apos;work&apos;, 2346), (&apos;young&apos;, 2343), (&apos;old&apos;, 2311), (&apos;ve&apos;, 2307), (&apos;find&apos;, 2272), (&apos;both&apos;, 2248), (&apos;before&apos;, 2177), (&apos;us&apos;, 2162), (&apos;again&apos;, 2158), (&apos;series&apos;, 2153), (&apos;quite&apos;, 2143), (&apos;something&apos;, 2135), (&apos;cast&apos;, 2133), (&apos;should&apos;, 2121), (&apos;part&apos;, 2098), (&apos;always&apos;, 2088), (&apos;lot&apos;, 2087), (&apos;another&apos;, 2075), (&apos;actors&apos;, 2047), (&apos;director&apos;, 2040), (&apos;family&apos;, 2032), (&apos;between&apos;, 2016), (&apos;own&apos;, 2016), (&apos;m&apos;, 1998), (&apos;may&apos;, 1997), (&apos;same&apos;, 1972), (&apos;role&apos;, 1967), (&apos;watching&apos;, 1966), (&apos;every&apos;, 1954), (&apos;funny&apos;, 1953), (&apos;doesn&apos;, 1935), (&apos;performance&apos;, 1928), (&apos;few&apos;, 1918), (&apos;bad&apos;, 1907), (&apos;look&apos;, 1900), (&apos;re&apos;, 1884), (&apos;why&apos;, 1855), (&apos;things&apos;, 1849), (&apos;times&apos;, 1832), (&apos;big&apos;, 1815), (&apos;however&apos;, 1795), (&apos;actually&apos;, 1790), (&apos;action&apos;, 1789), (&apos;going&apos;, 1783), (&apos;bit&apos;, 1757), (&apos;comedy&apos;, 1742), (&apos;down&apos;, 1740), (&apos;music&apos;, 1738), (&apos;must&apos;, 1728), (&apos;take&apos;, 1709), (&apos;saw&apos;, 1692), (&apos;long&apos;, 1690), (&apos;right&apos;, 1688), (&apos;fun&apos;, 1686), (&apos;fact&apos;, 1684), (&apos;excellent&apos;, 1683), (&apos;around&apos;, 1674), (&apos;didn&apos;, 1672), (&apos;without&apos;, 1671), (&apos;thing&apos;, 1662), (&apos;thought&apos;, 1639), (&apos;got&apos;, 1635), (&apos;each&apos;, 1630), (&apos;day&apos;, 1614), (&apos;feel&apos;, 1597), (&apos;seems&apos;, 1596), (&apos;come&apos;, 1594), (&apos;done&apos;, 1586), (&apos;beautiful&apos;, 1580), (&apos;especially&apos;, 1572), (&apos;played&apos;, 1571), (&apos;almost&apos;, 1566), (&apos;want&apos;, 1562), (&apos;yet&apos;, 1556), (&apos;give&apos;, 1553), (&apos;pretty&apos;, 1549), (&apos;last&apos;, 1543), (&apos;since&apos;, 1519), (&apos;different&apos;, 1504), (&apos;although&apos;, 1501), (&apos;gets&apos;, 1490), (&apos;true&apos;, 1487), (&apos;interesting&apos;, 1481), (&apos;job&apos;, 1470), (&apos;enough&apos;, 1455), (&apos;our&apos;, 1454), (&apos;shows&apos;, 1447), (&apos;horror&apos;, 1441), (&apos;woman&apos;, 1439), (&apos;tv&apos;, 1400), (&apos;probably&apos;, 1398), (&apos;father&apos;, 1395), (&apos;original&apos;, 1393), (&apos;girl&apos;, 1390), (&apos;point&apos;, 1379), (&apos;plays&apos;, 1378), (&apos;wonderful&apos;, 1372), (&apos;far&apos;, 1358), (&apos;course&apos;, 1358), (&apos;john&apos;, 1350), (&apos;rather&apos;, 1340), (&apos;isn&apos;, 1328), (&apos;ll&apos;, 1326), (&apos;later&apos;, 1324), (&apos;dvd&apos;, 1324), (&apos;whole&apos;, 1310), (&apos;war&apos;, 1310), (&apos;d&apos;, 1307), (&apos;found&apos;, 1306), (&apos;away&apos;, 1306), (&apos;screen&apos;, 1305), (&apos;nothing&apos;, 1300), (&apos;year&apos;, 1297), (&apos;once&apos;, 1296), (&apos;hard&apos;, 1294), (&apos;together&apos;, 1280), (&apos;set&apos;, 1277), (&apos;am&apos;, 1277), (&apos;having&apos;, 1266), (&apos;making&apos;, 1265), (&apos;place&apos;, 1263), (&apos;might&apos;, 1260), (&apos;comes&apos;, 1260), (&apos;sure&apos;, 1253), (&apos;american&apos;, 1248), (&apos;play&apos;, 1245), (&apos;kind&apos;, 1244), (&apos;perfect&apos;, 1242), (&apos;takes&apos;, 1242), (&apos;performances&apos;, 1237), (&apos;himself&apos;, 1230), (&apos;worth&apos;, 1221), (&apos;everyone&apos;, 1221), (&apos;anyone&apos;, 1214), (&apos;actor&apos;, 1203), (&apos;three&apos;, 1201), (&apos;wife&apos;, 1196), (&apos;classic&apos;, 1192), (&apos;goes&apos;, 1186), (&apos;ending&apos;, 1178), (&apos;version&apos;, 1168), (&apos;star&apos;, 1149), (&apos;enjoy&apos;, 1146), (&apos;book&apos;, 1142), (&apos;nice&apos;, 1132), (&apos;everything&apos;, 1128), (&apos;during&apos;, 1124), (&apos;put&apos;, 1118), (&apos;seeing&apos;, 1111), (&apos;least&apos;, 1102), (&apos;house&apos;, 1100), (&apos;high&apos;, 1095), (&apos;watched&apos;, 1094), (&apos;loved&apos;, 1087), (&apos;men&apos;, 1087), (&apos;night&apos;, 1082), (&apos;anything&apos;, 1075), (&apos;believe&apos;, 1071), (&apos;guy&apos;, 1071), (&apos;top&apos;, 1063), (&apos;amazing&apos;, 1058), (&apos;hollywood&apos;, 1056), (&apos;looking&apos;, 1053), (&apos;main&apos;, 1044), (&apos;definitely&apos;, 1043), (&apos;gives&apos;, 1031), (&apos;home&apos;, 1029), (&apos;seem&apos;, 1028), (&apos;episode&apos;, 1023), (&apos;audience&apos;, 1020), (&apos;sense&apos;, 1020), (&apos;truly&apos;, 1017), (&apos;special&apos;, 1011), (&apos;second&apos;, 1009), (&apos;short&apos;, 1009), (&apos;fan&apos;, 1009), (&apos;mind&apos;, 1005), (&apos;human&apos;, 1001), (&apos;recommend&apos;, 999), (&apos;full&apos;, 996), (&apos;black&apos;, 995), (&apos;help&apos;, 991), (&apos;along&apos;, 989), (&apos;trying&apos;, 987), (&apos;small&apos;, 986), (&apos;death&apos;, 985), (&apos;friends&apos;, 981), (&apos;remember&apos;, 974), (&apos;often&apos;, 970), (&apos;said&apos;, 966), (&apos;favorite&apos;, 962), (&apos;heart&apos;, 959), (&apos;early&apos;, 957), (&apos;left&apos;, 956), (&apos;until&apos;, 955), (&apos;script&apos;, 954), (&apos;let&apos;, 954), (&apos;maybe&apos;, 937), (&apos;today&apos;, 936), (&apos;live&apos;, 934), (&apos;less&apos;, 934), (&apos;moments&apos;, 933), (&apos;others&apos;, 929), (&apos;brilliant&apos;, 926), (&apos;shot&apos;, 925), (&apos;liked&apos;, 923), (&apos;become&apos;, 916), (&apos;won&apos;, 915), (&apos;used&apos;, 910), (&apos;style&apos;, 907), (&apos;mother&apos;, 895), (&apos;lives&apos;, 894), (&apos;came&apos;, 893), (&apos;stars&apos;, 890), (&apos;cinema&apos;, 889), (&apos;looks&apos;, 885), (&apos;perhaps&apos;, 884), (&apos;read&apos;, 882), (&apos;enjoyed&apos;, 879), (&apos;boy&apos;, 875), (&apos;drama&apos;, 873), (&apos;highly&apos;, 871), (&apos;given&apos;, 870), (&apos;playing&apos;, 867), (&apos;use&apos;, 864), (&apos;next&apos;, 859), (&apos;women&apos;, 858), (&apos;fine&apos;, 857), (&apos;effects&apos;, 856), (&apos;kids&apos;, 854), (&apos;entertaining&apos;, 853), (&apos;need&apos;, 852), (&apos;line&apos;, 850), (&apos;works&apos;, 848), (&apos;someone&apos;, 847), (&apos;mr&apos;, 836), (&apos;simply&apos;, 835), (&apos;picture&apos;, 833), (&apos;children&apos;, 833), (&apos;face&apos;, 831), (&apos;keep&apos;, 831), (&apos;friend&apos;, 831), (&apos;dark&apos;, 830), (&apos;overall&apos;, 828), (&apos;certainly&apos;, 828), (&apos;minutes&apos;, 827), (&apos;wasn&apos;, 824), (&apos;history&apos;, 822), (&apos;finally&apos;, 820), (&apos;couple&apos;, 816), (&apos;against&apos;, 815), (&apos;son&apos;, 809), (&apos;understand&apos;, 808), (&apos;lost&apos;, 807), (&apos;michael&apos;, 805), (&apos;else&apos;, 801), (&apos;throughout&apos;, 798), (&apos;fans&apos;, 797), (&apos;city&apos;, 792), (&apos;reason&apos;, 789), (&apos;written&apos;, 787), (&apos;production&apos;, 787), (&apos;several&apos;, 784), (&apos;school&apos;, 783), (&apos;based&apos;, 781), (&apos;rest&apos;, 781), (&apos;try&apos;, 780), (&apos;dead&apos;, 776), (&apos;hope&apos;, 775), (&apos;strong&apos;, 768), (&apos;white&apos;, 765), (&apos;tell&apos;, 759), (&apos;itself&apos;, 758), (&apos;half&apos;, 753), (&apos;person&apos;, 749), (&apos;sometimes&apos;, 746), (&apos;past&apos;, 744), (&apos;start&apos;, 744), (&apos;genre&apos;, 743), (&apos;beginning&apos;, 739), (&apos;final&apos;, 739), (&apos;town&apos;, 738), (&apos;art&apos;, 734), (&apos;humor&apos;, 732), (&apos;game&apos;, 732), (&apos;yes&apos;, 731), (&apos;idea&apos;, 731), (&apos;late&apos;, 730), (&apos;becomes&apos;, 729), (&apos;despite&apos;, 729), (&apos;able&apos;, 726), (&apos;case&apos;, 726), (&apos;money&apos;, 723), (&apos;child&apos;, 721), (&apos;completely&apos;, 721), (&apos;side&apos;, 719), (&apos;camera&apos;, 716), (&apos;getting&apos;, 714), (&apos;instead&apos;, 712), (&apos;soon&apos;, 702), (&apos;under&apos;, 700), (&apos;viewer&apos;, 699), (&apos;age&apos;, 697), (&apos;days&apos;, 696), (&apos;stories&apos;, 696), (&apos;felt&apos;, 694), (&apos;simple&apos;, 694), (&apos;roles&apos;, 693), (&apos;video&apos;, 688), (&apos;name&apos;, 683), (&apos;either&apos;, 683), (&apos;doing&apos;, 677), (&apos;turns&apos;, 674), (&apos;wants&apos;, 671), (&apos;close&apos;, 671), (&apos;title&apos;, 669), (&apos;wrong&apos;, 668), (&apos;went&apos;, 666), (&apos;james&apos;, 665), (&apos;evil&apos;, 659), (&apos;budget&apos;, 657), (&apos;episodes&apos;, 657), (&apos;relationship&apos;, 655), (&apos;fantastic&apos;, 653), (&apos;piece&apos;, 653), (&apos;david&apos;, 651), (&apos;turn&apos;, 648), (&apos;murder&apos;, 646), (&apos;parts&apos;, 645), (&apos;brother&apos;, 644), (&apos;absolutely&apos;, 643), (&apos;head&apos;, 643), (&apos;experience&apos;, 642), (&apos;eyes&apos;, 641), (&apos;sex&apos;, 638), (&apos;direction&apos;, 637), (&apos;called&apos;, 637), (&apos;directed&apos;, 636), (&apos;lines&apos;, 634), (&apos;behind&apos;, 633), (&apos;sort&apos;, 632), (&apos;actress&apos;, 631), (&apos;lead&apos;, 630), (&apos;oscar&apos;, 628), (&apos;including&apos;, 627), (&apos;example&apos;, 627), (&apos;known&apos;, 625), (&apos;musical&apos;, 625), (&apos;chance&apos;, 621), (&apos;score&apos;, 620), (&apos;already&apos;, 619), (&apos;feeling&apos;, 619), (&apos;hit&apos;, 619), (&apos;voice&apos;, 615), (&apos;moment&apos;, 612), (&apos;living&apos;, 612), (&apos;low&apos;, 610), (&apos;supporting&apos;, 610), (&apos;ago&apos;, 609), (&apos;themselves&apos;, 608), (&apos;reality&apos;, 605), (&apos;hilarious&apos;, 605), (&apos;jack&apos;, 604), (&apos;told&apos;, 603), (&apos;hand&apos;, 601), (&apos;quality&apos;, 600), (&apos;moving&apos;, 600), (&apos;dialogue&apos;, 600), (&apos;song&apos;, 599), (&apos;happy&apos;, 599), (&apos;matter&apos;, 598), (&apos;paul&apos;, 598), (&apos;light&apos;, 594), (&apos;future&apos;, 593), (&apos;entire&apos;, 592), (&apos;finds&apos;, 591), (&apos;gave&apos;, 589), (&apos;laugh&apos;, 587), (&apos;released&apos;, 586), (&apos;expect&apos;, 584), (&apos;fight&apos;, 581), (&apos;particularly&apos;, 580), (&apos;cinematography&apos;, 579), (&apos;police&apos;, 579), (&apos;whose&apos;, 578), (&apos;type&apos;, 578), (&apos;sound&apos;, 578), (&apos;view&apos;, 573), (&apos;enjoyable&apos;, 573), (&apos;number&apos;, 572), (&apos;romantic&apos;, 572), (&apos;husband&apos;, 572), (&apos;daughter&apos;, 572), (&apos;documentary&apos;, 571), (&apos;self&apos;, 570), (&apos;superb&apos;, 569), (&apos;modern&apos;, 569), (&apos;took&apos;, 569), (&apos;robert&apos;, 569), (&apos;mean&apos;, 566), (&apos;shown&apos;, 563), (&apos;coming&apos;, 561), (&apos;important&apos;, 560), (&apos;king&apos;, 559), (&apos;leave&apos;, 559), (&apos;change&apos;, 558), (&apos;somewhat&apos;, 555), (&apos;wanted&apos;, 555), (&apos;tells&apos;, 554), (&apos;events&apos;, 552), (&apos;run&apos;, 552), (&apos;career&apos;, 552), (&apos;country&apos;, 552), (&apos;heard&apos;, 550), (&apos;season&apos;, 550), (&apos;greatest&apos;, 549), (&apos;girls&apos;, 549), (&apos;etc&apos;, 547), (&apos;care&apos;, 546), (&apos;starts&apos;, 545), (&apos;english&apos;, 542), (&apos;killer&apos;, 541), (&apos;tale&apos;, 540), (&apos;guys&apos;, 540), (&apos;totally&apos;, 540), (&apos;animation&apos;, 540), (&apos;usual&apos;, 539), (&apos;miss&apos;, 535), (&apos;opinion&apos;, 535), (&apos;easy&apos;, 531), (&apos;violence&apos;, 531), (&apos;songs&apos;, 530), (&apos;british&apos;, 528), (&apos;says&apos;, 526), (&apos;realistic&apos;, 525), (&apos;writing&apos;, 524), (&apos;writer&apos;, 522), (&apos;act&apos;, 522), (&apos;comic&apos;, 521), (&apos;thriller&apos;, 519), (&apos;television&apos;, 517), (&apos;power&apos;, 516), (&apos;ones&apos;, 515), (&apos;kid&apos;, 514), (&apos;york&apos;, 513), (&apos;novel&apos;, 513), (&apos;alone&apos;, 512), (&apos;problem&apos;, 512), (&apos;attention&apos;, 509), (&apos;involved&apos;, 508), (&apos;kill&apos;, 507), (&apos;extremely&apos;, 507), (&apos;seemed&apos;, 506), (&apos;hero&apos;, 505), (&apos;french&apos;, 505), (&apos;rock&apos;, 504), (&apos;stuff&apos;, 501), (&apos;wish&apos;, 499), (&apos;begins&apos;, 498), (&apos;taken&apos;, 497), (&apos;sad&apos;, 497), (&apos;ways&apos;, 496), (&apos;richard&apos;, 495), (&apos;knows&apos;, 494), (&apos;atmosphere&apos;, 493), (&apos;similar&apos;, 491), (&apos;surprised&apos;, 491), (&apos;taking&apos;, 491), (&apos;car&apos;, 491), (&apos;george&apos;, 490), (&apos;perfectly&apos;, 490), (&apos;across&apos;, 489), (&apos;team&apos;, 489), (&apos;eye&apos;, 489), (&apos;sequence&apos;, 489), (&apos;room&apos;, 488), (&apos;due&apos;, 488), (&apos;among&apos;, 488), (&apos;serious&apos;, 488), (&apos;powerful&apos;, 488), (&apos;strange&apos;, 487), (&apos;order&apos;, 487), (&apos;cannot&apos;, 487), (&apos;b&apos;, 487), (&apos;beauty&apos;, 486), (&apos;famous&apos;, 485), (&apos;happened&apos;, 484), (&apos;tries&apos;, 484), (&apos;herself&apos;, 484), (&apos;myself&apos;, 484), (&apos;class&apos;, 483), (&apos;four&apos;, 482), (&apos;cool&apos;, 481), (&apos;release&apos;, 479), (&apos;anyway&apos;, 479), (&apos;theme&apos;, 479), (&apos;opening&apos;, 478), (&apos;entertainment&apos;, 477), (&apos;slow&apos;, 475), (&apos;ends&apos;, 475), (&apos;unique&apos;, 475), (&apos;exactly&apos;, 475), (&apos;easily&apos;, 474), (&apos;level&apos;, 474), (&apos;o&apos;, 474), (&apos;red&apos;, 474), (&apos;interest&apos;, 472), (&apos;happen&apos;, 471), (&apos;crime&apos;, 470), (&apos;viewing&apos;, 468), (&apos;sets&apos;, 467), (&apos;memorable&apos;, 467), (&apos;stop&apos;, 466), (&apos;group&apos;, 466), (&apos;problems&apos;, 463), (&apos;dance&apos;, 463), (&apos;working&apos;, 463), (&apos;sister&apos;, 463), (&apos;message&apos;, 463), (&apos;knew&apos;, 462), (&apos;mystery&apos;, 461), (&apos;nature&apos;, 461), (&apos;bring&apos;, 460), (&apos;believable&apos;, 459), (&apos;thinking&apos;, 459), (&apos;brought&apos;, 459), (&apos;mostly&apos;, 458), (&apos;disney&apos;, 457), (&apos;couldn&apos;, 457), (&apos;society&apos;, 456), (&apos;lady&apos;, 455), (&apos;within&apos;, 455), (&apos;blood&apos;, 454), (&apos;parents&apos;, 453), (&apos;upon&apos;, 453), (&apos;viewers&apos;, 453), (&apos;meets&apos;, 452), (&apos;form&apos;, 452), (&apos;peter&apos;, 452), (&apos;tom&apos;, 452), (&apos;usually&apos;, 452), (&apos;soundtrack&apos;, 452), (&apos;local&apos;, 450), (&apos;certain&apos;, 448), (&apos;follow&apos;, 448), (&apos;whether&apos;, 447), (&apos;possible&apos;, 446), (&apos;emotional&apos;, 445), (&apos;killed&apos;, 444), (&apos;above&apos;, 444), (&apos;de&apos;, 444), (&apos;god&apos;, 443), (&apos;middle&apos;, 443), (&apos;needs&apos;, 442), (&apos;happens&apos;, 442), (&apos;flick&apos;, 442), (&apos;masterpiece&apos;, 441), (&apos;period&apos;, 440), (&apos;major&apos;, 440), (&apos;named&apos;, 439), (&apos;haven&apos;, 439), (&apos;particular&apos;, 438), (&apos;th&apos;, 438), (&apos;earth&apos;, 437), (&apos;feature&apos;, 437), (&apos;stand&apos;, 436), (&apos;words&apos;, 435), (&apos;typical&apos;, 435), (&apos;elements&apos;, 433), (&apos;obviously&apos;, 433), (&apos;romance&apos;, 431), (&apos;jane&apos;, 430), (&apos;yourself&apos;, 427), (&apos;showing&apos;, 427), (&apos;brings&apos;, 426), (&apos;fantasy&apos;, 426), (&apos;guess&apos;, 423), (&apos;america&apos;, 423), (&apos;unfortunately&apos;, 422), (&apos;huge&apos;, 422), (&apos;indeed&apos;, 421), (&apos;running&apos;, 421), (&apos;talent&apos;, 420), (&apos;stage&apos;, 419), (&apos;started&apos;, 418), (&apos;leads&apos;, 417), (&apos;sweet&apos;, 417), (&apos;japanese&apos;, 417), (&apos;poor&apos;, 416), (&apos;deal&apos;, 416), (&apos;incredible&apos;, 413), (&apos;personal&apos;, 413), (&apos;fast&apos;, 412), (&apos;became&apos;, 410), (&apos;deep&apos;, 410), (&apos;hours&apos;, 409), (&apos;giving&apos;, 408), (&apos;nearly&apos;, 408), (&apos;dream&apos;, 408), (&apos;clearly&apos;, 407), (&apos;turned&apos;, 407), (&apos;obvious&apos;, 406), (&apos;near&apos;, 406), (&apos;cut&apos;, 405), (&apos;surprise&apos;, 405), (&apos;era&apos;, 404), (&apos;body&apos;, 404), (&apos;hour&apos;, 403), (&apos;female&apos;, 403), (&apos;five&apos;, 403), (&apos;note&apos;, 399), (&apos;learn&apos;, 398), (&apos;truth&apos;, 398), (&apos;except&apos;, 397), (&apos;feels&apos;, 397), (&apos;match&apos;, 397), (&apos;tony&apos;, 397), (&apos;filmed&apos;, 394), (&apos;clear&apos;, 394), (&apos;complete&apos;, 394), (&apos;street&apos;, 393), (&apos;eventually&apos;, 393), (&apos;keeps&apos;, 393), (&apos;older&apos;, 393), (&apos;lots&apos;, 393), (&apos;buy&apos;, 392), (&apos;william&apos;, 391), (&apos;stewart&apos;, 391), (&apos;fall&apos;, 390), (&apos;joe&apos;, 390), (&apos;meet&apos;, 390), (&apos;unlike&apos;, 389), (&apos;talking&apos;, 389), (&apos;shots&apos;, 389), (&apos;rating&apos;, 389), (&apos;difficult&apos;, 389), (&apos;dramatic&apos;, 388), (&apos;means&apos;, 388), (&apos;situation&apos;, 386), (&apos;wonder&apos;, 386), (&apos;present&apos;, 386), (&apos;appears&apos;, 386), (&apos;subject&apos;, 386), (&apos;comments&apos;, 385), (&apos;general&apos;, 383), (&apos;sequences&apos;, 383), (&apos;lee&apos;, 383), (&apos;points&apos;, 382), (&apos;earlier&apos;, 382), (&apos;gone&apos;, 379), (&apos;check&apos;, 379), (&apos;suspense&apos;, 378), (&apos;recommended&apos;, 378), (&apos;ten&apos;, 378), (&apos;third&apos;, 377), (&apos;business&apos;, 377), (&apos;talk&apos;, 375), (&apos;leaves&apos;, 375), (&apos;beyond&apos;, 375), (&apos;portrayal&apos;, 374), (&apos;beautifully&apos;, 373), (&apos;single&apos;, 372), (&apos;bill&apos;, 372), (&apos;plenty&apos;, 371), (&apos;word&apos;, 371), (&apos;whom&apos;, 370), (&apos;falls&apos;, 370), (&apos;scary&apos;, 369), (&apos;non&apos;, 369), (&apos;figure&apos;, 369), (&apos;battle&apos;, 369), (&apos;using&apos;, 368), (&apos;return&apos;, 368), (&apos;doubt&apos;, 367), (&apos;add&apos;, 367), (&apos;hear&apos;, 366), (&apos;solid&apos;, 366), (&apos;success&apos;, 366), (&apos;jokes&apos;, 365), (&apos;oh&apos;, 365), (&apos;touching&apos;, 365), (&apos;political&apos;, 365), (&apos;hell&apos;, 364), (&apos;awesome&apos;, 364), (&apos;boys&apos;, 364), (&apos;sexual&apos;, 362), (&apos;recently&apos;, 362), (&apos;dog&apos;, 362), (&apos;please&apos;, 361), (&apos;wouldn&apos;, 361), (&apos;straight&apos;, 361), (&apos;features&apos;, 361), (&apos;forget&apos;, 360), (&apos;setting&apos;, 360), (&apos;lack&apos;, 360), (&apos;married&apos;, 359), (&apos;mark&apos;, 359), (&apos;social&apos;, 357), (&apos;interested&apos;, 356), (&apos;adventure&apos;, 356), (&apos;actual&apos;, 355), (&apos;terrific&apos;, 355), (&apos;sees&apos;, 355), (&apos;brothers&apos;, 355), (&apos;move&apos;, 354), (&apos;call&apos;, 354), (&apos;various&apos;, 353), (&apos;theater&apos;, 353), (&apos;dr&apos;, 353), (&apos;animated&apos;, 352), (&apos;western&apos;, 351), (&apos;baby&apos;, 350), (&apos;space&apos;, 350), (&apos;leading&apos;, 348), (&apos;disappointed&apos;, 348), (&apos;portrayed&apos;, 346), (&apos;aren&apos;, 346), (&apos;screenplay&apos;, 345), (&apos;smith&apos;, 345), (&apos;towards&apos;, 344), (&apos;hate&apos;, 344), (&apos;noir&apos;, 343), (&apos;outstanding&apos;, 342), (&apos;decent&apos;, 342), (&apos;kelly&apos;, 342), (&apos;directors&apos;, 341), (&apos;journey&apos;, 341), (&apos;none&apos;, 340), (&apos;looked&apos;, 340), (&apos;effective&apos;, 340), (&apos;storyline&apos;, 339), (&apos;caught&apos;, 339), (&apos;sci&apos;, 339), (&apos;fi&apos;, 339), (&apos;cold&apos;, 339), (&apos;mary&apos;, 339), (&apos;rich&apos;, 338), (&apos;charming&apos;, 338), (&apos;popular&apos;, 337), (&apos;rare&apos;, 337), (&apos;manages&apos;, 337), (&apos;harry&apos;, 337), (&apos;spirit&apos;, 336), (&apos;appreciate&apos;, 335), (&apos;open&apos;, 335), (&apos;moves&apos;, 334), (&apos;basically&apos;, 334), (&apos;acted&apos;, 334), (&apos;inside&apos;, 333), (&apos;boring&apos;, 333), (&apos;century&apos;, 333), (&apos;mention&apos;, 333), (&apos;deserves&apos;, 333), (&apos;subtle&apos;, 333), (&apos;pace&apos;, 333), (&apos;familiar&apos;, 332), (&apos;background&apos;, 332), (&apos;ben&apos;, 331), (&apos;creepy&apos;, 330), (&apos;supposed&apos;, 330), (&apos;secret&apos;, 329), (&apos;die&apos;, 328), (&apos;jim&apos;, 328), (&apos;question&apos;, 327), (&apos;effect&apos;, 327), (&apos;natural&apos;, 327), (&apos;impressive&apos;, 326), (&apos;rate&apos;, 326), (&apos;language&apos;, 326), (&apos;saying&apos;, 325), (&apos;intelligent&apos;, 325), (&apos;telling&apos;, 324), (&apos;realize&apos;, 324), (&apos;material&apos;, 324), (&apos;scott&apos;, 324), (&apos;singing&apos;, 323), (&apos;dancing&apos;, 322), (&apos;visual&apos;, 321), (&apos;adult&apos;, 321), (&apos;imagine&apos;, 321), (&apos;kept&apos;, 320), (&apos;office&apos;, 320), (&apos;uses&apos;, 319), (&apos;pure&apos;, 318), (&apos;wait&apos;, 318), (&apos;stunning&apos;, 318), (&apos;review&apos;, 317), (&apos;previous&apos;, 317), (&apos;copy&apos;, 317), (&apos;seriously&apos;, 317), (&apos;reading&apos;, 316), (&apos;create&apos;, 316), (&apos;hot&apos;, 316), (&apos;created&apos;, 316), (&apos;magic&apos;, 316), (&apos;somehow&apos;, 316), (&apos;stay&apos;, 315), (&apos;attempt&apos;, 315), (&apos;escape&apos;, 315), (&apos;crazy&apos;, 315), (&apos;air&apos;, 315), (&apos;frank&apos;, 315), (&apos;hands&apos;, 314), (&apos;filled&apos;, 313), (&apos;expected&apos;, 312), (&apos;average&apos;, 312), (&apos;surprisingly&apos;, 312), (&apos;complex&apos;, 311), (&apos;quickly&apos;, 310), (&apos;successful&apos;, 310), (&apos;studio&apos;, 310), (&apos;plus&apos;, 309), (&apos;male&apos;, 309), (&apos;co&apos;, 307), (&apos;images&apos;, 306), (&apos;casting&apos;, 306), (&apos;following&apos;, 306), (&apos;minute&apos;, 306), (&apos;exciting&apos;, 306), (&apos;members&apos;, 305), (&apos;follows&apos;, 305), (&apos;themes&apos;, 305), (&apos;german&apos;, 305), (&apos;reasons&apos;, 305), (&apos;e&apos;, 305), (&apos;touch&apos;, 304), (&apos;edge&apos;, 304), (&apos;free&apos;, 304), (&apos;cute&apos;, 304), (&apos;genius&apos;, 304), (&apos;outside&apos;, 303), (&apos;reviews&apos;, 302), (&apos;admit&apos;, 302), (&apos;ok&apos;, 302), (&apos;younger&apos;, 302), (&apos;fighting&apos;, 301), (&apos;odd&apos;, 301), (&apos;master&apos;, 301), (&apos;recent&apos;, 300), (&apos;thanks&apos;, 300), (&apos;break&apos;, 300), (&apos;comment&apos;, 300), (&apos;apart&apos;, 299), (&apos;emotions&apos;, 298), (&apos;lovely&apos;, 298), (&apos;begin&apos;, 298), (&apos;doctor&apos;, 297), (&apos;party&apos;, 297), (&apos;italian&apos;, 297), (&apos;la&apos;, 296), (&apos;missed&apos;, 296), ...] 123456789101112pos_neg_ratios = Counter()for term,cnt in list(total_counts.most_common()): if(cnt &gt; 100): pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1) pos_neg_ratios[term] = pos_neg_ratiofor word,ratio in pos_neg_ratios.most_common(): if(ratio &gt; 1): pos_neg_ratios[word] = np.log(ratio) else: pos_neg_ratios[word] = -np.log((1 / (ratio+0.01))) 12# words most frequently seen in a review with a \"POSITIVE\" labelpos_neg_ratios.most_common() [(&apos;edie&apos;, 4.6913478822291435), (&apos;paulie&apos;, 4.0775374439057197), (&apos;felix&apos;, 3.1527360223636558), (&apos;polanski&apos;, 2.8233610476132043), (&apos;matthau&apos;, 2.8067217286092401), (&apos;victoria&apos;, 2.6810215287142909), (&apos;mildred&apos;, 2.6026896854443837), (&apos;gandhi&apos;, 2.5389738710582761), (&apos;flawless&apos;, 2.451005098112319), (&apos;superbly&apos;, 2.2600254785752498), (&apos;perfection&apos;, 2.1594842493533721), (&apos;astaire&apos;, 2.1400661634962708), (&apos;captures&apos;, 2.0386195471595809), (&apos;voight&apos;, 2.0301704926730531), (&apos;wonderfully&apos;, 2.0218960560332353), (&apos;powell&apos;, 1.9783454248084671), (&apos;brosnan&apos;, 1.9547990964725592), (&apos;lily&apos;, 1.9203768470501485), (&apos;bakshi&apos;, 1.9029851043382795), (&apos;lincoln&apos;, 1.9014583864844796), (&apos;refreshing&apos;, 1.8551812956655511), (&apos;breathtaking&apos;, 1.8481124057791867), (&apos;bourne&apos;, 1.8478489358790986), (&apos;lemmon&apos;, 1.8458266904983307), (&apos;delightful&apos;, 1.8002701588959635), (&apos;flynn&apos;, 1.7996646487351682), (&apos;andrews&apos;, 1.7764919970972666), (&apos;homer&apos;, 1.7692866133759964), (&apos;beautifully&apos;, 1.7626953362841438), (&apos;soccer&apos;, 1.7578579175523736), (&apos;elvira&apos;, 1.7397031072720019), (&apos;underrated&apos;, 1.7197859696029656), (&apos;gripping&apos;, 1.7165360479904674), (&apos;superb&apos;, 1.7091514458966952), (&apos;delight&apos;, 1.6714733033535532), (&apos;welles&apos;, 1.6677068205580761), (&apos;sadness&apos;, 1.663505133704376), (&apos;sinatra&apos;, 1.6389967146756448), (&apos;touching&apos;, 1.637217476541176), (&apos;timeless&apos;, 1.62924053973028), (&apos;macy&apos;, 1.6211339521972916), (&apos;unforgettable&apos;, 1.6177367152487956), (&apos;favorites&apos;, 1.6158688027643908), (&apos;stewart&apos;, 1.6119987332957739), (&apos;sullivan&apos;, 1.6094379124341003), (&apos;extraordinary&apos;, 1.6094379124341003), (&apos;hartley&apos;, 1.6094379124341003), (&apos;brilliantly&apos;, 1.5950491749820008), (&apos;friendship&apos;, 1.5677652160335325), (&apos;wonderful&apos;, 1.5645425925262093), (&apos;palma&apos;, 1.5553706911638245), (&apos;magnificent&apos;, 1.54663701119507), (&apos;finest&apos;, 1.5462590108125689), (&apos;jackie&apos;, 1.5439233053234738), (&apos;ritter&apos;, 1.5404450409471491), (&apos;tremendous&apos;, 1.5184661342283736), (&apos;freedom&apos;, 1.5091151908062312), (&apos;fantastic&apos;, 1.5048433868558566), (&apos;terrific&apos;, 1.5026699370083942), (&apos;noir&apos;, 1.493925025312256), (&apos;sidney&apos;, 1.493925025312256), (&apos;outstanding&apos;, 1.4910053152089213), (&apos;pleasantly&apos;, 1.4894785973551214), (&apos;mann&apos;, 1.4894785973551214), (&apos;nancy&apos;, 1.488077055429833), (&apos;marie&apos;, 1.4825711915553104), (&apos;marvelous&apos;, 1.4739999415389962), (&apos;excellent&apos;, 1.4647538505723599), (&apos;ruth&apos;, 1.4596256342054401), (&apos;stanwyck&apos;, 1.4412101187160054), (&apos;widmark&apos;, 1.4350845252893227), (&apos;splendid&apos;, 1.4271163556401458), (&apos;chan&apos;, 1.423108334242607), (&apos;exceptional&apos;, 1.4201959127955721), (&apos;tender&apos;, 1.410986973710262), (&apos;gentle&apos;, 1.4078005663408544), (&apos;poignant&apos;, 1.4022947024663317), (&apos;gem&apos;, 1.3932148039644643), (&apos;amazing&apos;, 1.3919815802404802), (&apos;chilling&apos;, 1.3862943611198906), (&apos;fisher&apos;, 1.3862943611198906), (&apos;davies&apos;, 1.3862943611198906), (&apos;captivating&apos;, 1.3862943611198906), (&apos;darker&apos;, 1.3652409519220583), (&apos;april&apos;, 1.3499267169490159), (&apos;kelly&apos;, 1.3461743673304654), (&apos;blake&apos;, 1.3418425985490567), (&apos;overlooked&apos;, 1.329135947279942), (&apos;ralph&apos;, 1.32818673031261), (&apos;bette&apos;, 1.3156767939059373), (&apos;hoffman&apos;, 1.3150668518315229), (&apos;cole&apos;, 1.3121863889661687), (&apos;shines&apos;, 1.3049487216659381), (&apos;powerful&apos;, 1.2999662776313934), (&apos;notch&apos;, 1.2950456896547455), (&apos;remarkable&apos;, 1.2883688239495823), (&apos;pitt&apos;, 1.286210902562908), (&apos;winters&apos;, 1.2833463918674481), (&apos;vivid&apos;, 1.2762934659055623), (&apos;gritty&apos;, 1.2757524867200667), (&apos;giallo&apos;, 1.2745029551317739), (&apos;portrait&apos;, 1.2704625455947689), (&apos;innocence&apos;, 1.2694300209805796), (&apos;psychiatrist&apos;, 1.2685113254635072), (&apos;favorite&apos;, 1.2668956297860055), (&apos;ensemble&apos;, 1.2656663733312759), (&apos;stunning&apos;, 1.2622417124499117), (&apos;burns&apos;, 1.259880436264232), (&apos;garbo&apos;, 1.258954938743289), (&apos;barbara&apos;, 1.2580400255962119), (&apos;philip&apos;, 1.2527629684953681), (&apos;panic&apos;, 1.2527629684953681), (&apos;holly&apos;, 1.2527629684953681), (&apos;carol&apos;, 1.2481440226390734), (&apos;perfect&apos;, 1.246742480713785), (&apos;appreciated&apos;, 1.2462482874741743), (&apos;favourite&apos;, 1.2411123512753928), (&apos;journey&apos;, 1.2367626271489269), (&apos;rural&apos;, 1.235471471385307), (&apos;bond&apos;, 1.2321436812926323), (&apos;builds&apos;, 1.2305398317106577), (&apos;brilliant&apos;, 1.2287554137664785), (&apos;brooklyn&apos;, 1.2286654169163074), (&apos;von&apos;, 1.225175011976539), (&apos;recommended&apos;, 1.2163953243244932), (&apos;unfolds&apos;, 1.2163953243244932), (&apos;daniel&apos;, 1.20215296760895), (&apos;perfectly&apos;, 1.1971931173405572), (&apos;crafted&apos;, 1.1962507582320256), (&apos;prince&apos;, 1.1939224684724346), (&apos;troubled&apos;, 1.192138346678933), (&apos;consequences&apos;, 1.1865810616140668), (&apos;haunting&apos;, 1.1814999484738773), (&apos;cinderella&apos;, 1.180052620608284), (&apos;alexander&apos;, 1.1759989522835299), (&apos;emotions&apos;, 1.1753049094563641), (&apos;boxing&apos;, 1.1735135968412274), (&apos;subtle&apos;, 1.1734135017508081), (&apos;curtis&apos;, 1.1649873576129823), (&apos;rare&apos;, 1.1566438362402944), (&apos;loved&apos;, 1.1563661500586044), (&apos;daughters&apos;, 1.1526795099383853), (&apos;courage&apos;, 1.1438688802562305), (&apos;dentist&apos;, 1.1426722784621401), (&apos;highly&apos;, 1.1420208631618658), (&apos;nominated&apos;, 1.1409146683587992), (&apos;tony&apos;, 1.1397491942285991), (&apos;draws&apos;, 1.1325138403437911), (&apos;everyday&apos;, 1.1306150197542835), (&apos;contrast&apos;, 1.1284652518177909), (&apos;cried&apos;, 1.1213405397456659), (&apos;fabulous&apos;, 1.1210851445201684), (&apos;ned&apos;, 1.120591195386885), (&apos;fay&apos;, 1.120591195386885), (&apos;emma&apos;, 1.1184149159642893), (&apos;sensitive&apos;, 1.113318436057805), (&apos;smooth&apos;, 1.1089750757036563), (&apos;dramas&apos;, 1.1080910326226534), (&apos;today&apos;, 1.1050431789984001), (&apos;helps&apos;, 1.1023091505494358), (&apos;inspiring&apos;, 1.0986122886681098), (&apos;jimmy&apos;, 1.0937696641923216), (&apos;awesome&apos;, 1.0931328229034842), (&apos;unique&apos;, 1.0881409888008142), (&apos;tragic&apos;, 1.0871835928444868), (&apos;intense&apos;, 1.0870514662670339), (&apos;stellar&apos;, 1.0857088838322018), (&apos;rival&apos;, 1.0822184788924332), (&apos;provides&apos;, 1.0797081340289569), (&apos;depression&apos;, 1.0782034170369026), (&apos;shy&apos;, 1.0775588794702773), (&apos;carrie&apos;, 1.076139432816051), (&apos;blend&apos;, 1.0753554265038423), (&apos;hank&apos;, 1.0736109864626924), (&apos;diana&apos;, 1.0726368022648489), (&apos;adorable&apos;, 1.0726368022648489), (&apos;unexpected&apos;, 1.0722255334949147), (&apos;achievement&apos;, 1.0668635903535293), (&apos;bettie&apos;, 1.0663514264498881), (&apos;happiness&apos;, 1.0632729222228008), (&apos;glorious&apos;, 1.0608719606852626), (&apos;davis&apos;, 1.0541605260972757), (&apos;terrifying&apos;, 1.0525211814678428), (&apos;beauty&apos;, 1.050410186850232), (&apos;ideal&apos;, 1.0479685558493548), (&apos;fears&apos;, 1.0467872208035236), (&apos;hong&apos;, 1.0438040521731147), (&apos;seasons&apos;, 1.0433496099930604), (&apos;fascinating&apos;, 1.0414538748281612), (&apos;carries&apos;, 1.0345904299031787), (&apos;satisfying&apos;, 1.0321225473992768), (&apos;definite&apos;, 1.0319209141694374), (&apos;touched&apos;, 1.0296194171811581), (&apos;greatest&apos;, 1.0248947127715422), (&apos;creates&apos;, 1.0241097613701886), (&apos;aunt&apos;, 1.023388867430522), (&apos;walter&apos;, 1.022328983918479), (&apos;spectacular&apos;, 1.0198314108149955), (&apos;portrayal&apos;, 1.0189810189761024), (&apos;ann&apos;, 1.0127808528183286), (&apos;enterprise&apos;, 1.0116009116784799), (&apos;musicals&apos;, 1.0096648026516135), (&apos;deeply&apos;, 1.0094845087721023), (&apos;incredible&apos;, 1.0061677561461084), (&apos;mature&apos;, 1.0060195018402847), (&apos;triumph&apos;, 0.99682959435816731), (&apos;margaret&apos;, 0.99682959435816731), (&apos;navy&apos;, 0.99493385919326827), (&apos;harry&apos;, 0.99176919305006062), (&apos;lucas&apos;, 0.990398704027877), (&apos;sweet&apos;, 0.98966110487955483), (&apos;joey&apos;, 0.98794672078059009), (&apos;oscar&apos;, 0.98721905111049713), (&apos;balance&apos;, 0.98649499054740353), (&apos;warm&apos;, 0.98485340331145166), (&apos;ages&apos;, 0.98449898190068863), (&apos;guilt&apos;, 0.98082925301172619), (&apos;glover&apos;, 0.98082925301172619), (&apos;carrey&apos;, 0.98082925301172619), (&apos;learns&apos;, 0.97881108885548895), (&apos;unusual&apos;, 0.97788374278196932), (&apos;sons&apos;, 0.97777581552483595), (&apos;complex&apos;, 0.97761897738147796), (&apos;essence&apos;, 0.97753435711487369), (&apos;brazil&apos;, 0.9769153536905899), (&apos;widow&apos;, 0.97650959186720987), (&apos;solid&apos;, 0.97537964824416146), (&apos;beautiful&apos;, 0.97326301262841053), (&apos;holmes&apos;, 0.97246100334120955), (&apos;awe&apos;, 0.97186058302896583), (&apos;vhs&apos;, 0.97116734209998934), (&apos;eerie&apos;, 0.97116734209998934), (&apos;lonely&apos;, 0.96873720724669754), (&apos;grim&apos;, 0.96873720724669754), (&apos;sport&apos;, 0.96825047080486615), (&apos;debut&apos;, 0.96508089604358704), (&apos;destiny&apos;, 0.96343751029985703), (&apos;thrillers&apos;, 0.96281074750904794), (&apos;tears&apos;, 0.95977584381389391), (&apos;rose&apos;, 0.95664202739772253), (&apos;feelings&apos;, 0.95551144502743635), (&apos;ginger&apos;, 0.95551144502743635), (&apos;winning&apos;, 0.95471810900804055), (&apos;stanley&apos;, 0.95387344302319799), (&apos;cox&apos;, 0.95343027882361187), (&apos;paris&apos;, 0.95278479030472663), (&apos;heart&apos;, 0.95238806924516806), (&apos;hooked&apos;, 0.95155887071161305), (&apos;comfortable&apos;, 0.94803943018873538), (&apos;mgm&apos;, 0.94446160884085151), (&apos;masterpiece&apos;, 0.94155039863339296), (&apos;themes&apos;, 0.94118828349588235), (&apos;danny&apos;, 0.93967118051821874), (&apos;anime&apos;, 0.93378388932167222), (&apos;perry&apos;, 0.93328830824272613), (&apos;joy&apos;, 0.93301752567946861), (&apos;lovable&apos;, 0.93081883243706487), (&apos;mysteries&apos;, 0.92953595862417571), (&apos;hal&apos;, 0.92953595862417571), (&apos;louis&apos;, 0.92871325187271225), (&apos;charming&apos;, 0.92520609553210742), (&apos;urban&apos;, 0.92367083917177761), (&apos;allows&apos;, 0.92183091224977043), (&apos;impact&apos;, 0.91815814604895041), (&apos;italy&apos;, 0.91629073187415511), (&apos;gradually&apos;, 0.91629073187415511), (&apos;lifestyle&apos;, 0.91629073187415511), (&apos;spy&apos;, 0.91289514287301687), (&apos;treat&apos;, 0.91193342650519937), (&apos;subsequent&apos;, 0.91056005716517008), (&apos;kennedy&apos;, 0.90981821736853763), (&apos;loving&apos;, 0.90967549275543591), (&apos;surprising&apos;, 0.90937028902958128), (&apos;quiet&apos;, 0.90648673177753425), (&apos;winter&apos;, 0.90624039602065365), (&apos;reveals&apos;, 0.90490540964902977), (&apos;raw&apos;, 0.90445627422715225), (&apos;funniest&apos;, 0.90078654533818991), (&apos;pleased&apos;, 0.89994159387262562), (&apos;norman&apos;, 0.89994159387262562), (&apos;thief&apos;, 0.89874642222324552), (&apos;season&apos;, 0.89827222637147675), (&apos;secrets&apos;, 0.89794159320595857), (&apos;colorful&apos;, 0.89705936994626756), (&apos;highest&apos;, 0.8967461358011849), (&apos;compelling&apos;, 0.89462923509297576), (&apos;danes&apos;, 0.89248008318043659), (&apos;castle&apos;, 0.88967708335606499), (&apos;kudos&apos;, 0.88889175768604067), (&apos;great&apos;, 0.88810470901464589), (&apos;baseball&apos;, 0.88730319500090271), (&apos;subtitles&apos;, 0.88730319500090271), (&apos;bleak&apos;, 0.88730319500090271), (&apos;winner&apos;, 0.88643776872447388), (&apos;tragedy&apos;, 0.88563699078315261), (&apos;todd&apos;, 0.88551907320740142), (&apos;nicely&apos;, 0.87924946019380601), (&apos;arthur&apos;, 0.87546873735389985), (&apos;essential&apos;, 0.87373111745535925), (&apos;gorgeous&apos;, 0.8731725250935497), (&apos;fonda&apos;, 0.87294029100054127), (&apos;eastwood&apos;, 0.87139541196626402), (&apos;focuses&apos;, 0.87082835779739776), (&apos;enjoyed&apos;, 0.87070195951624607), (&apos;natural&apos;, 0.86997924506912838), (&apos;intensity&apos;, 0.86835126958503595), (&apos;witty&apos;, 0.86824103423244681), (&apos;rob&apos;, 0.8642954367557748), (&apos;worlds&apos;, 0.86377269759070874), (&apos;health&apos;, 0.86113891179907498), (&apos;magical&apos;, 0.85953791528170564), (&apos;deeper&apos;, 0.85802182375017932), (&apos;lucy&apos;, 0.85618680780444956), (&apos;moving&apos;, 0.85566611005772031), (&apos;lovely&apos;, 0.85290640004681306), (&apos;purple&apos;, 0.8513711857748395), (&apos;memorable&apos;, 0.84801189112086062), (&apos;sings&apos;, 0.84729786038720367), (&apos;craig&apos;, 0.84342938360928321), (&apos;modesty&apos;, 0.84342938360928321), (&apos;relate&apos;, 0.84326559685926517), (&apos;episodes&apos;, 0.84223712084137292), (&apos;strong&apos;, 0.84167135777060931), (&apos;smith&apos;, 0.83959811108590054), (&apos;tear&apos;, 0.83704136022001441), (&apos;apartment&apos;, 0.83333115290549531), (&apos;princess&apos;, 0.83290912293510388), (&apos;disagree&apos;, 0.83290912293510388), (&apos;kung&apos;, 0.83173334384609199), (&apos;adventure&apos;, 0.83150561393278388), (&apos;columbo&apos;, 0.82667857318446791), (&apos;jake&apos;, 0.82667857318446791), (&apos;adds&apos;, 0.82485652591452319), (&apos;hart&apos;, 0.82472353834866463), (&apos;strength&apos;, 0.82417544296634937), (&apos;realizes&apos;, 0.82360006895738058), (&apos;dave&apos;, 0.8232003088081431), (&apos;childhood&apos;, 0.82208086393583857), (&apos;forbidden&apos;, 0.81989888619908913), (&apos;tight&apos;, 0.81883539572344199), (&apos;surreal&apos;, 0.8178506590609026), (&apos;manager&apos;, 0.81770990320170756), (&apos;dancer&apos;, 0.81574950265227764), (&apos;studios&apos;, 0.81093021621632877), (&apos;con&apos;, 0.81093021621632877), (&apos;miike&apos;, 0.80821651034473263), (&apos;realistic&apos;, 0.80807714723392232), (&apos;explicit&apos;, 0.80792269515237358), (&apos;kurt&apos;, 0.8060875917405409), (&apos;traditional&apos;, 0.80535917116687328), (&apos;deals&apos;, 0.80535917116687328), (&apos;holds&apos;, 0.80493858654806194), (&apos;carl&apos;, 0.80437281567016972), (&apos;touches&apos;, 0.80396154690023547), (&apos;gene&apos;, 0.80314807577427383), (&apos;albert&apos;, 0.8027669055771679), (&apos;abc&apos;, 0.80234647252493729), (&apos;cry&apos;, 0.80011930011211307), (&apos;sides&apos;, 0.7995275841185171), (&apos;develops&apos;, 0.79850769621777162), (&apos;eyre&apos;, 0.79850769621777162), (&apos;dances&apos;, 0.79694397424158891), (&apos;oscars&apos;, 0.79633141679517616), (&apos;legendary&apos;, 0.79600456599965308), (&apos;hearted&apos;, 0.79492987486988764), (&apos;importance&apos;, 0.79492987486988764), (&apos;portraying&apos;, 0.79356592830699269), (&apos;impressed&apos;, 0.79258107754813223), (&apos;waters&apos;, 0.79112758892014912), (&apos;empire&apos;, 0.79078565012386137), (&apos;edge&apos;, 0.789774016249017), (&apos;jean&apos;, 0.78845736036427028), (&apos;environment&apos;, 0.78845736036427028), (&apos;sentimental&apos;, 0.7864791203521645), (&apos;captured&apos;, 0.78623760362595729), (&apos;styles&apos;, 0.78592891401091158), (&apos;daring&apos;, 0.78592891401091158), (&apos;frank&apos;, 0.78275933924963248), (&apos;tense&apos;, 0.78275933924963248), (&apos;backgrounds&apos;, 0.78275933924963248), (&apos;matches&apos;, 0.78275933924963248), (&apos;gothic&apos;, 0.78209466657644144), (&apos;sharp&apos;, 0.7814397877056235), (&apos;achieved&apos;, 0.78015855754957497), (&apos;court&apos;, 0.77947526404844247), (&apos;steals&apos;, 0.7789140023173704), (&apos;rules&apos;, 0.77844476107184035), (&apos;colors&apos;, 0.77684619943659217), (&apos;reunion&apos;, 0.77318988823348167), (&apos;covers&apos;, 0.77139937745969345), (&apos;tale&apos;, 0.77010822169607374), (&apos;rain&apos;, 0.7683706017975328), (&apos;denzel&apos;, 0.76804848873306297), (&apos;stays&apos;, 0.76787072675588186), (&apos;blob&apos;, 0.76725515271366718), (&apos;maria&apos;, 0.76214005204689672), (&apos;conventional&apos;, 0.76214005204689672), (&apos;fresh&apos;, 0.76158434211317383), (&apos;midnight&apos;, 0.76096977689870637), (&apos;landscape&apos;, 0.75852993982279704), (&apos;animated&apos;, 0.75768570169751648), (&apos;titanic&apos;, 0.75666058628227129), (&apos;sunday&apos;, 0.75666058628227129), (&apos;spring&apos;, 0.7537718023763802), (&apos;cagney&apos;, 0.7537718023763802), (&apos;enjoyable&apos;, 0.75246375771636476), (&apos;immensely&apos;, 0.75198768058287868), (&apos;sir&apos;, 0.7507762933965817), (&apos;nevertheless&apos;, 0.75067102469813185), (&apos;driven&apos;, 0.74994477895307854), (&apos;performances&apos;, 0.74883252516063137), (&apos;memories&apos;, 0.74721440183022114), (&apos;nowadays&apos;, 0.74721440183022114), (&apos;simple&apos;, 0.74641420974143258), (&apos;golden&apos;, 0.74533293373051557), (&apos;leslie&apos;, 0.74533293373051557), (&apos;lovers&apos;, 0.74497224842453125), (&apos;relationship&apos;, 0.74484232345601786), (&apos;supporting&apos;, 0.74357803418683721), (&apos;che&apos;, 0.74262723782331497), (&apos;packed&apos;, 0.7410032017375805), (&apos;trek&apos;, 0.74021469141793106), (&apos;provoking&apos;, 0.73840377214806618), (&apos;strikes&apos;, 0.73759894313077912), (&apos;depiction&apos;, 0.73682224406260699), (&apos;emotional&apos;, 0.73678211645681524), (&apos;secretary&apos;, 0.7366322924996842), (&apos;influenced&apos;, 0.73511137965897755), (&apos;florida&apos;, 0.73511137965897755), (&apos;germany&apos;, 0.73288750920945944), (&apos;brings&apos;, 0.73142936713096229), (&apos;lewis&apos;, 0.73129894652432159), (&apos;elderly&apos;, 0.73088750854279239), (&apos;owner&apos;, 0.72743625403857748), (&apos;streets&apos;, 0.72666987259858895), (&apos;henry&apos;, 0.72642196944481741), (&apos;portrays&apos;, 0.72593700338293632), (&apos;bears&apos;, 0.7252354951114458), (&apos;china&apos;, 0.72489587887452556), (&apos;anger&apos;, 0.72439972406404984), (&apos;society&apos;, 0.72433010799663333), (&apos;available&apos;, 0.72415741730250549), (&apos;best&apos;, 0.72347034060446314), (&apos;bugs&apos;, 0.72270598280148979), (&apos;magic&apos;, 0.71878961117328299), (&apos;delivers&apos;, 0.71846498854423513), (&apos;verhoeven&apos;, 0.71846498854423513), (&apos;jim&apos;, 0.71783979315031676), (&apos;donald&apos;, 0.71667767797013937), (&apos;endearing&apos;, 0.71465338578090898), (&apos;relationships&apos;, 0.71393795022901896), (&apos;greatly&apos;, 0.71256526641704687), (&apos;charlie&apos;, 0.71024161391924534), (&apos;brad&apos;, 0.71024161391924534), (&apos;simon&apos;, 0.70967648251115578), (&apos;effectively&apos;, 0.70914752190638641), (&apos;march&apos;, 0.70774597998109789), (&apos;atmosphere&apos;, 0.70744773070214162), (&apos;influence&apos;, 0.70733181555190172), (&apos;genius&apos;, 0.706392407309966), (&apos;emotionally&apos;, 0.70556970055850243), (&apos;ken&apos;, 0.70526854109229009), (&apos;identity&apos;, 0.70484322032313651), (&apos;sophisticated&apos;, 0.70470800296102132), (&apos;dan&apos;, 0.70457587638356811), (&apos;andrew&apos;, 0.70329955202396321), (&apos;india&apos;, 0.70144598337464037), (&apos;roy&apos;, 0.69970458110610434), (&apos;surprisingly&apos;, 0.6995780708902356), (&apos;sky&apos;, 0.69780919366575667), (&apos;romantic&apos;, 0.69664981111114743), (&apos;match&apos;, 0.69566924999265523), (&apos;meets&apos;, 0.69314718055994529), (&apos;cowboy&apos;, 0.69314718055994529), (&apos;wave&apos;, 0.69314718055994529), (&apos;bitter&apos;, 0.69314718055994529), (&apos;patient&apos;, 0.69314718055994529), (&apos;stylish&apos;, 0.69314718055994529), (&apos;britain&apos;, 0.69314718055994529), (&apos;affected&apos;, 0.69314718055994529), (&apos;beatty&apos;, 0.69314718055994529), (&apos;love&apos;, 0.69198533541937324), (&apos;paul&apos;, 0.68980827929443067), (&apos;andy&apos;, 0.68846333124751902), (&apos;performance&apos;, 0.68797386327972465), (&apos;patrick&apos;, 0.68645819240914863), (&apos;unlike&apos;, 0.68546468438792907), (&apos;brooks&apos;, 0.68433655087779044), (&apos;refuses&apos;, 0.68348526964820844), (&apos;award&apos;, 0.6824518914431974), (&apos;complaint&apos;, 0.6824518914431974), (&apos;ride&apos;, 0.68229716453587952), (&apos;dawson&apos;, 0.68171848473632257), (&apos;luke&apos;, 0.68158635815886937), (&apos;wells&apos;, 0.68087708796813096), (&apos;france&apos;, 0.6804081547825156), (&apos;sports&apos;, 0.68007509899259255), (&apos;handsome&apos;, 0.68007509899259255), (&apos;directs&apos;, 0.67875844310784572), (&apos;rebel&apos;, 0.67875844310784572), (&apos;greater&apos;, 0.67605274720064523), (&apos;dreams&apos;, 0.67599410133369586), (&apos;effective&apos;, 0.67565402311242806), (&apos;interpretation&apos;, 0.67479804189174875), (&apos;works&apos;, 0.67445504754779284), (&apos;brando&apos;, 0.67445504754779284), (&apos;noble&apos;, 0.6737290947028437), (&apos;paced&apos;, 0.67314651385327573), (&apos;le&apos;, 0.67067432470788668), (&apos;master&apos;, 0.67015766233524654), (&apos;h&apos;, 0.6696166831497512), (&apos;rings&apos;, 0.66904962898088483), (&apos;easy&apos;, 0.66895995494594152), (&apos;city&apos;, 0.66820823221269321), (&apos;sunshine&apos;, 0.66782937257565544), (&apos;succeeds&apos;, 0.66647893347778397), (&apos;relations&apos;, 0.664159643686693), (&apos;england&apos;, 0.66387679825983203), (&apos;glimpse&apos;, 0.66329421741026418), (&apos;aired&apos;, 0.66268797307523675), (&apos;sees&apos;, 0.66263163663399482), (&apos;both&apos;, 0.66248336767382998), (&apos;definitely&apos;, 0.66199789483898808), (&apos;imaginative&apos;, 0.66139848224536502), (&apos;appreciate&apos;, 0.66083893732728749), (&apos;tricks&apos;, 0.66071190480679143), (&apos;striking&apos;, 0.66071190480679143), (&apos;carefully&apos;, 0.65999497324304479), (&apos;complicated&apos;, 0.65981076029235353), (&apos;perspective&apos;, 0.65962448852130173), (&apos;trilogy&apos;, 0.65877953705573755), (&apos;future&apos;, 0.65834665141052828), (&apos;lion&apos;, 0.65742909795786608), (&apos;douglas&apos;, 0.65540685257709819), (&apos;victor&apos;, 0.65540685257709819), (&apos;inspired&apos;, 0.65459851044271034), (&apos;marriage&apos;, 0.65392646740666405), (&apos;demands&apos;, 0.65392646740666405), (&apos;father&apos;, 0.65172321672194655), (&apos;page&apos;, 0.65123628494430852), (&apos;instant&apos;, 0.65058756614114943), (&apos;era&apos;, 0.6495567444850836), (&apos;ruthless&apos;, 0.64934455790155243), (&apos;saga&apos;, 0.64934455790155243), (&apos;joan&apos;, 0.64891392558311978), (&apos;joseph&apos;, 0.64841128671855386), (&apos;workers&apos;, 0.64829661439459352), (&apos;fantasy&apos;, 0.64726757480925168), (&apos;distant&apos;, 0.64551913157069074), (&apos;accomplished&apos;, 0.64551913157069074), (&apos;manhattan&apos;, 0.64435701639051324), (&apos;personal&apos;, 0.64355023942057321), (&apos;meeting&apos;, 0.64313675998528386), (&apos;individual&apos;, 0.64313675998528386), (&apos;pushing&apos;, 0.64313675998528386), (&apos;pleasant&apos;, 0.64250344774119039), (&apos;brave&apos;, 0.64185388617239469), (&apos;william&apos;, 0.64083139119578469), (&apos;hudson&apos;, 0.64077919504262937), (&apos;friendly&apos;, 0.63949446706762514), (&apos;eccentric&apos;, 0.63907995928966954), (&apos;awards&apos;, 0.63875310849414646), (&apos;jack&apos;, 0.63838309514997038), (&apos;seeking&apos;, 0.63808740337691783), (&apos;divorce&apos;, 0.63757732940513456), (&apos;colonel&apos;, 0.63757732940513456), (&apos;jane&apos;, 0.63443957973316734), (&apos;keeping&apos;, 0.63414883979798953), (&apos;gives&apos;, 0.63383568159497883), (&apos;ted&apos;, 0.63342794585832296), (&apos;animation&apos;, 0.63208692379869902), (&apos;progress&apos;, 0.6317782341836532), (&apos;larger&apos;, 0.63127177684185776), (&apos;concert&apos;, 0.63127177684185776), (&apos;nation&apos;, 0.6296337748376194), (&apos;albeit&apos;, 0.62739580299716491), (&apos;adapted&apos;, 0.62613647027698516), (&apos;discovers&apos;, 0.62542900650499444), (&apos;classic&apos;, 0.62504956428050518), (&apos;segment&apos;, 0.62335141862440335), (&apos;morgan&apos;, 0.62303761437291871), (&apos;mouse&apos;, 0.62294292188669675), (&apos;impressive&apos;, 0.62211140744319349), (&apos;artist&apos;, 0.62168821657780038), (&apos;ultimate&apos;, 0.62168821657780038), (&apos;griffith&apos;, 0.62117368093485603), (&apos;drew&apos;, 0.62082651898031915), (&apos;emily&apos;, 0.62082651898031915), (&apos;moved&apos;, 0.6197197120051281), (&apos;families&apos;, 0.61903920840622351), (&apos;profound&apos;, 0.61903920840622351), (&apos;innocent&apos;, 0.61851219917136446), (&apos;versions&apos;, 0.61730910416844087), (&apos;eddie&apos;, 0.61691981517206107), (&apos;criticism&apos;, 0.61651395453902935), (&apos;nature&apos;, 0.61594514653194088), (&apos;recognized&apos;, 0.61518563909023349), (&apos;sexuality&apos;, 0.61467556511845012), (&apos;contract&apos;, 0.61400986000122149), (&apos;brian&apos;, 0.61344043794920278), (&apos;remembered&apos;, 0.6131044728864089), (&apos;determined&apos;, 0.6123858239154869), (&apos;offers&apos;, 0.61207935747116349), (&apos;pleasure&apos;, 0.61195702582993206), (&apos;washington&apos;, 0.61180154110599294), (&apos;images&apos;, 0.61159731359583758), (&apos;games&apos;, 0.61067095873570676), (&apos;academy&apos;, 0.60872983874736208), (&apos;fashioned&apos;, 0.60798937221963845), (&apos;melodrama&apos;, 0.60749173598145145), (&apos;rough&apos;, 0.60613580357031549), (&apos;charismatic&apos;, 0.60613580357031549), (&apos;peoples&apos;, 0.60613580357031549), (&apos;dealing&apos;, 0.60517840761398811), (&apos;fine&apos;, 0.60496962268013299), (&apos;tap&apos;, 0.60391604683200273), (&apos;trio&apos;, 0.60157998703445481), (&apos;russell&apos;, 0.60120968523425966), (&apos;figures&apos;, 0.60077386042893011), (&apos;ward&apos;, 0.60005675749393339), (&apos;shine&apos;, 0.59911823091166894), (&apos;brady&apos;, 0.59911823091166894), (&apos;job&apos;, 0.59845562125168661), (&apos;satisfied&apos;, 0.59652034487087369), (&apos;river&apos;, 0.59637962862495086), (&apos;brown&apos;, 0.595773016534769), (&apos;believable&apos;, 0.59566072133302495), (&apos;always&apos;, 0.59470710774669278), (&apos;bound&apos;, 0.59470710774669278), (&apos;hall&apos;, 0.5933967777928858), (&apos;cook&apos;, 0.5916777203950857), (&apos;claire&apos;, 0.59136448625000293), (&apos;broadway&apos;, 0.59033768669372433), (&apos;anna&apos;, 0.58778666490211906), (&apos;peace&apos;, 0.58628403501758408), (&apos;visually&apos;, 0.58539431926349916), (&apos;morality&apos;, 0.58525821854876026), (&apos;falk&apos;, 0.58525821854876026), (&apos;growing&apos;, 0.58466653756587539), (&apos;experiences&apos;, 0.58314628534561685), (&apos;stood&apos;, 0.58314628534561685), (&apos;touch&apos;, 0.58122926435596001), (&apos;lives&apos;, 0.5810976767513224), (&apos;kubrick&apos;, 0.58066919713325493), (&apos;timing&apos;, 0.58047401805583243), (&apos;expressions&apos;, 0.57981849525294216), (&apos;struggles&apos;, 0.57981849525294216), (&apos;authentic&apos;, 0.57848427223980559), (&apos;helen&apos;, 0.57763429343810091), (&apos;pre&apos;, 0.57700753064729182), (&apos;quirky&apos;, 0.5753641449035618), (&apos;young&apos;, 0.57531672344534313), (&apos;inner&apos;, 0.57454143815209846), (&apos;mexico&apos;, 0.57443087372056334), (&apos;clint&apos;, 0.57380042292737909), (&apos;sisters&apos;, 0.57286101468544337), (&apos;realism&apos;, 0.57226528899949558), (&apos;french&apos;, 0.5720692490067093), (&apos;personalities&apos;, 0.5720692490067093), (&apos;surprises&apos;, 0.57113222999698177), (&apos;adventures&apos;, 0.57113222999698177), (&apos;overcome&apos;, 0.5697681593994407), (&apos;timothy&apos;, 0.56953322459276867), (&apos;tales&apos;, 0.56909453188996639), (&apos;war&apos;, 0.56843317302781682), (&apos;civil&apos;, 0.5679840376059393), (&apos;countries&apos;, 0.56737779327091187), (&apos;streep&apos;, 0.56710645966458029), (&apos;tradition&apos;, 0.56685345523565323), (&apos;oliver&apos;, 0.56673325570428668), (&apos;australia&apos;, 0.56580775818334383), (&apos;understanding&apos;, 0.56531380905006046), (&apos;players&apos;, 0.56509525370004821), (&apos;knowing&apos;, 0.56489284503626647), (&apos;rogers&apos;, 0.56421349718405212), (&apos;suspenseful&apos;, 0.56368911332305849), (&apos;variety&apos;, 0.56368911332305849), (&apos;true&apos;, 0.56281525180810066), (&apos;jr&apos;, 0.56220982311246936), (&apos;psychological&apos;, 0.56108745854687891), (&apos;sent&apos;, 0.55961578793542266), (&apos;grand&apos;, 0.55961578793542266), (&apos;branagh&apos;, 0.55961578793542266), (&apos;reminiscent&apos;, 0.55961578793542266), (&apos;performing&apos;, 0.55961578793542266), (&apos;wealth&apos;, 0.55961578793542266), (&apos;overwhelming&apos;, 0.55961578793542266), (&apos;odds&apos;, 0.55961578793542266), (&apos;brothers&apos;, 0.55891181043362848), (&apos;howard&apos;, 0.55811089675600245), (&apos;david&apos;, 0.55693122256475369), (&apos;generation&apos;, 0.55628799784274796), (&apos;grow&apos;, 0.55612538299565417), (&apos;survival&apos;, 0.55594605904646033), (&apos;mainstream&apos;, 0.55574731115750231), (&apos;dick&apos;, 0.55431073570572953), (&apos;charm&apos;, 0.55288175575407861), (&apos;kirk&apos;, 0.55278982286502287), (&apos;twists&apos;, 0.55244729845681018), (&apos;gangster&apos;, 0.55206858230003986), (&apos;jeff&apos;, 0.55179306225421365), (&apos;family&apos;, 0.55116244510065526), (&apos;tend&apos;, 0.55053307336110335), (&apos;thanks&apos;, 0.55049088015842218), (&apos;world&apos;, 0.54744234723432639), (&apos;sutherland&apos;, 0.54743536937855164), (&apos;life&apos;, 0.54695514434959924), (&apos;disc&apos;, 0.54654370636806993), (&apos;bug&apos;, 0.54654370636806993), (&apos;tribute&apos;, 0.5455111817538808), (&apos;europe&apos;, 0.54522705048332309), (&apos;sacrifice&apos;, 0.54430155296238014), (&apos;color&apos;, 0.54405127139431109), (&apos;superior&apos;, 0.54333490233128523), (&apos;york&apos;, 0.54318235866536513), (&apos;pulls&apos;, 0.54266622962164945), (&apos;jackson&apos;, 0.54232429082536171), (&apos;hearts&apos;, 0.54232429082536171), (&apos;enjoy&apos;, 0.54124285135906114), (&apos;redemption&apos;, 0.54056759296472823), (&apos;madness&apos;, 0.540384426007535), (&apos;stands&apos;, 0.5389965007326869), (&apos;trial&apos;, 0.5389965007326869), (&apos;greek&apos;, 0.5389965007326869), (&apos;hamilton&apos;, 0.5389965007326869), (&apos;each&apos;, 0.5388212312554177), (&apos;faithful&apos;, 0.53773307668591508), (&apos;received&apos;, 0.5372768098531604), (&apos;documentaries&apos;, 0.53714293208336406), (&apos;jealous&apos;, 0.53714293208336406), (&apos;different&apos;, 0.53709860682460819), (&apos;describes&apos;, 0.53680111016925136), (&apos;shorts&apos;, 0.53596159703753288), (&apos;brilliance&apos;, 0.53551823635636209), (&apos;mountains&apos;, 0.53492317534505118), (&apos;share&apos;, 0.53408248593025787), (&apos;dealt&apos;, 0.53408248593025787), (&apos;providing&apos;, 0.53329847961804933), (&apos;explore&apos;, 0.53329847961804933), (&apos;series&apos;, 0.5325809226575603), (&apos;fellow&apos;, 0.5323318289869543), (&apos;loves&apos;, 0.53062825106217038), (&apos;revolution&apos;, 0.53062825106217038), (&apos;olivier&apos;, 0.53062825106217038), (&apos;roman&apos;, 0.53062825106217038), (&apos;century&apos;, 0.53002783074992665), (&apos;musical&apos;, 0.52966871156747064), (&apos;heroic&apos;, 0.52925932545482868), (&apos;approach&apos;, 0.52806743020049673), (&apos;ironically&apos;, 0.52806743020049673), (&apos;temple&apos;, 0.52806743020049673), (&apos;moves&apos;, 0.5279372642387119), (&apos;gift&apos;, 0.52702030968597136), (&apos;julie&apos;, 0.52609309589677911), (&apos;tells&apos;, 0.52415107836314001), (&apos;radio&apos;, 0.52394671172868779), (&apos;uncle&apos;, 0.52354439617376536), (&apos;union&apos;, 0.52324814376454787), (&apos;deep&apos;, 0.52309571635780505), (&apos;reminds&apos;, 0.52157841554225237), (&apos;famous&apos;, 0.52118841080153722), (&apos;jazz&apos;, 0.52053443789295151), (&apos;dennis&apos;, 0.51987545928590861), (&apos;epic&apos;, 0.51919387343650736), (&apos;adult&apos;, 0.519167695083386), (&apos;shows&apos;, 0.51915322220375304), (&apos;performed&apos;, 0.5191244265806858), (&apos;demons&apos;, 0.5191244265806858), (&apos;discovered&apos;, 0.51879379341516751), (&apos;eric&apos;, 0.51879379341516751), (&apos;youth&apos;, 0.5185626062681431), (&apos;human&apos;, 0.51851411224987087), (&apos;tarzan&apos;, 0.51813827061227724), (&apos;ourselves&apos;, 0.51794309153485463), (&apos;wwii&apos;, 0.51758240622887042), (&apos;passion&apos;, 0.5162164724008671), (&apos;desire&apos;, 0.51607497965213445), (&apos;pays&apos;, 0.51581316527702981), (&apos;dirty&apos;, 0.51557622652458857), (&apos;fox&apos;, 0.51557622652458857), (&apos;sympathetic&apos;, 0.51546600332249293), (&apos;symbolism&apos;, 0.51546600332249293), (&apos;attitude&apos;, 0.51530993621331933), (&apos;appearances&apos;, 0.51466440007315639), (&apos;jeremy&apos;, 0.51466440007315639), (&apos;fun&apos;, 0.51439068993048687), (&apos;south&apos;, 0.51420972175023116), (&apos;arrives&apos;, 0.51409894911095988), (&apos;present&apos;, 0.51341965894303732), (&apos;com&apos;, 0.51326167856387173), (&apos;smile&apos;, 0.51265880484765169), (&apos;alan&apos;, 0.51082562376599072), (&apos;ring&apos;, 0.51082562376599072), (&apos;visit&apos;, 0.51082562376599072), (&apos;fits&apos;, 0.51082562376599072), (&apos;provided&apos;, 0.51082562376599072), (&apos;carter&apos;, 0.51082562376599072), (&apos;aging&apos;, 0.51082562376599072), (&apos;countryside&apos;, 0.51082562376599072), (&apos;begins&apos;, 0.51015650363396647), (&apos;success&apos;, 0.50900578704900468), (&apos;japan&apos;, 0.50900578704900468), (&apos;accurate&apos;, 0.50895471583017893), (&apos;proud&apos;, 0.50800474742434931), (&apos;daily&apos;, 0.5075946031845443), (&apos;karloff&apos;, 0.50724780241810674), (&apos;atmospheric&apos;, 0.50724780241810674), (&apos;recently&apos;, 0.50714914903668207), (&apos;fu&apos;, 0.50704490092608467), (&apos;horrors&apos;, 0.50656122497953315), (&apos;finding&apos;, 0.50637127341661037), (&apos;lust&apos;, 0.5059356384717989), (&apos;hitchcock&apos;, 0.50574947073413001), (&apos;among&apos;, 0.50334004951332734), (&apos;viewing&apos;, 0.50302139827440906), (&apos;investigation&apos;, 0.50262885656181222), (&apos;shining&apos;, 0.50262885656181222), (&apos;duo&apos;, 0.5020919437972361), (&apos;cameron&apos;, 0.5020919437972361), (&apos;finds&apos;, 0.50128303100539795), (&apos;contemporary&apos;, 0.50077528791248915), (&apos;genuine&apos;, 0.50046283673044401), (&apos;frightening&apos;, 0.49995595152908684), (&apos;plays&apos;, 0.49975983848890226), (&apos;age&apos;, 0.49941323171424595), (&apos;position&apos;, 0.49899116611898781), (&apos;continues&apos;, 0.49863035067217237), (&apos;roles&apos;, 0.49839716550752178), (&apos;james&apos;, 0.49837216269470402), (&apos;individuals&apos;, 0.49824684155913052), (&apos;brought&apos;, 0.49783842823917956), (&apos;hilarious&apos;, 0.49714551986191058), (&apos;brutal&apos;, 0.49681488669639234), (&apos;appropriate&apos;, 0.49643688631389105), (&apos;dance&apos;, 0.49581998314812048), (&apos;league&apos;, 0.49578774640145024), (&apos;helping&apos;, 0.49578774640145024), (&apos;answers&apos;, 0.49578774640145024), (&apos;stunts&apos;, 0.49561620510246196), (&apos;traveling&apos;, 0.49532143723002542), (&apos;thoroughly&apos;, 0.49414593456733524), (&apos;depicted&apos;, 0.49317068852726992), (&apos;combination&apos;, 0.49247648509779424), (&apos;honor&apos;, 0.49247648509779424), (&apos;differences&apos;, 0.49247648509779424), (&apos;fully&apos;, 0.49213349075383811), (&apos;tracy&apos;, 0.49159426183810306), (&apos;battles&apos;, 0.49140753790888908), (&apos;possibility&apos;, 0.49112055268665822), (&apos;romance&apos;, 0.4901589869574316), (&apos;initially&apos;, 0.49002249613622745), (&apos;happy&apos;, 0.4898997500608791), (&apos;crime&apos;, 0.48977221456815834), (&apos;singing&apos;, 0.4893852925281213), (&apos;especially&apos;, 0.48901267837860624), (&apos;shakespeare&apos;, 0.48754793889664511), (&apos;hugh&apos;, 0.48729512635579658), (&apos;detail&apos;, 0.48609484250827351), (&apos;julia&apos;, 0.48550781578170082), (&apos;san&apos;, 0.48550781578170082), (&apos;guide&apos;, 0.48550781578170082), (&apos;desperation&apos;, 0.48550781578170082), (&apos;companion&apos;, 0.48550781578170082), (&apos;strongly&apos;, 0.48460242866688824), (&apos;necessary&apos;, 0.48302334245403883), (&apos;humanity&apos;, 0.48265474679929443), (&apos;drama&apos;, 0.48221998493060503), (&apos;nonetheless&apos;, 0.48183808689273838), (&apos;intrigue&apos;, 0.48183808689273838), (&apos;warming&apos;, 0.48183808689273838), (&apos;cuba&apos;, 0.48183808689273838), (&apos;planned&apos;, 0.47957308026188628), (&apos;pictures&apos;, 0.47929937011921681), (&apos;broadcast&apos;, 0.47849024312305422), (&apos;nine&apos;, 0.47803580094299974), (&apos;settings&apos;, 0.47743860773325364), (&apos;history&apos;, 0.47732966933780852), (&apos;ordinary&apos;, 0.47725880012690741), (&apos;trade&apos;, 0.47692407209030935), (&apos;official&apos;, 0.47608267532211779), (&apos;primary&apos;, 0.47608267532211779), (&apos;episode&apos;, 0.47529620261150429), (&apos;role&apos;, 0.47520268270188676), (&apos;spirit&apos;, 0.47477690799839323), (&apos;grey&apos;, 0.47409361449726067), (&apos;ways&apos;, 0.47323464982718205), (&apos;cup&apos;, 0.47260441094579297), (&apos;piano&apos;, 0.47260441094579297), (&apos;familiar&apos;, 0.47241617565111949), (&apos;sinister&apos;, 0.47198579044972683), (&apos;reveal&apos;, 0.47171449364936496), (&apos;max&apos;, 0.47150852042515579), (&apos;dated&apos;, 0.47121648567094482), (&apos;losing&apos;, 0.47000362924573563), (&apos;discovery&apos;, 0.47000362924573563), (&apos;vicious&apos;, 0.47000362924573563), (&apos;genuinely&apos;, 0.46871413841586385), (&apos;hatred&apos;, 0.46734051182625186), (&apos;mistaken&apos;, 0.46702300110759781), (&apos;dream&apos;, 0.46608972992459924), (&apos;challenge&apos;, 0.46608972992459924), (&apos;crisis&apos;, 0.46575733836428446), (&apos;photographed&apos;, 0.46488852857896512), (&apos;critics&apos;, 0.46430560813109778), (&apos;bird&apos;, 0.46430560813109778), (&apos;machines&apos;, 0.46430560813109778), (&apos;born&apos;, 0.46411383518967209), (&apos;detective&apos;, 0.4636633473511525), (&apos;higher&apos;, 0.46328467899699055), (&apos;remains&apos;, 0.46262352194811296), (&apos;inevitable&apos;, 0.46262352194811296), (&apos;soviet&apos;, 0.4618180446592961), (&apos;ryan&apos;, 0.46134556650262099), (&apos;african&apos;, 0.46112595521371813), (&apos;smaller&apos;, 0.46081520319132935), (&apos;techniques&apos;, 0.46052488529119184), (&apos;information&apos;, 0.46034171833399862), (&apos;deserved&apos;, 0.45999798712841444), (&apos;lynch&apos;, 0.45953232937844013), (&apos;spielberg&apos;, 0.45953232937844013), (&apos;cynical&apos;, 0.45953232937844013), (&apos;tour&apos;, 0.45953232937844013), (&apos;francisco&apos;, 0.45953232937844013), (&apos;struggle&apos;, 0.45911782160048453), (&apos;language&apos;, 0.45902121257712653), (&apos;visual&apos;, 0.45823514408822852), (&apos;warner&apos;, 0.45724137763188427), (&apos;social&apos;, 0.45720078250735313), (&apos;reality&apos;, 0.45719346885019546), (&apos;hidden&apos;, 0.45675840249571492), (&apos;breaking&apos;, 0.45601738727099561), (&apos;sometimes&apos;, 0.45563021171182794), (&apos;modern&apos;, 0.45500247579345005), (&apos;surfing&apos;, 0.45425527227759638), (&apos;popular&apos;, 0.45410691533051023), (&apos;surprised&apos;, 0.4534409399850382), (&apos;follows&apos;, 0.45245361754408348), (&apos;keeps&apos;, 0.45234869400701483), (&apos;john&apos;, 0.4520909494482197), (&apos;mixed&apos;, 0.45198512374305722), (&apos;defeat&apos;, 0.45198512374305722), (&apos;justice&apos;, 0.45142724367280018), (&apos;treasure&apos;, 0.45083371313801535), (&apos;presents&apos;, 0.44973793178615257), (&apos;years&apos;, 0.44919197032104968), (&apos;chief&apos;, 0.44895022004790319), (&apos;shadows&apos;, 0.44802472252696035), (&apos;closely&apos;, 0.44701411102103689), (&apos;segments&apos;, 0.44701411102103689), (&apos;lose&apos;, 0.44658335503763702), (&apos;caine&apos;, 0.44628710262841953), (&apos;caught&apos;, 0.44610275383999071), (&apos;hamlet&apos;, 0.44558510189758965), (&apos;chinese&apos;, 0.44507424620321018), (&apos;welcome&apos;, 0.44438052435783792), (&apos;birth&apos;, 0.44368632092836219), (&apos;represents&apos;, 0.44320543609101143), (&apos;puts&apos;, 0.44279106572085081), (&apos;visuals&apos;, 0.44183275227903923), (&apos;fame&apos;, 0.44183275227903923), (&apos;closer&apos;, 0.44183275227903923), (&apos;web&apos;, 0.44183275227903923), (&apos;criminal&apos;, 0.4412745608048752), (&apos;minor&apos;, 0.4409224199448939), (&apos;jon&apos;, 0.44086703515908027), (&apos;liked&apos;, 0.44074991514020723), (&apos;restaurant&apos;, 0.44031183943833246), (&apos;de&apos;, 0.43983275161237217), (&apos;flaws&apos;, 0.43983275161237217), (&apos;searching&apos;, 0.4393666597838457), (&apos;rap&apos;, 0.43891304217570443), (&apos;light&apos;, 0.43884433018199892), (&apos;elizabeth&apos;, 0.43872232986464677), (&apos;marry&apos;, 0.43861731542506488), (&apos;learned&apos;, 0.43825493093115531), (&apos;controversial&apos;, 0.43825493093115531), (&apos;oz&apos;, 0.43825493093115531), (&apos;slowly&apos;, 0.43785660389939979), (&apos;comedic&apos;, 0.43721380642274466), (&apos;wayne&apos;, 0.43721380642274466), (&apos;thrilling&apos;, 0.43721380642274466), (&apos;bridge&apos;, 0.43721380642274466), (&apos;married&apos;, 0.43658501682196887), (&apos;nazi&apos;, 0.4361020775700542), (&apos;murder&apos;, 0.4353180712578455), (&apos;physical&apos;, 0.4353180712578455), (&apos;johnny&apos;, 0.43483971678806865), (&apos;michelle&apos;, 0.43445264498141672), (&apos;wallace&apos;, 0.43403848055222038), (&apos;comedies&apos;, 0.43395706390247063), (&apos;silent&apos;, 0.43395706390247063), (&apos;played&apos;, 0.43387244114515305), (&apos;international&apos;, 0.43363598507486073), (&apos;vision&apos;, 0.43286408229627887), (&apos;intelligent&apos;, 0.43196704885367099), (&apos;shop&apos;, 0.43078291609245434), (&apos;also&apos;, 0.43036720209769169), (&apos;levels&apos;, 0.4302451371066513), (&apos;miss&apos;, 0.43006426712153217), (&apos;movement&apos;, 0.4295626596872249), ...] 12# words most frequently seen in a review with a \"NEGATIVE\" labellist(reversed(pos_neg_ratios.most_common()))[0:30] [(&apos;boll&apos;, -4.0778152602708904), (&apos;uwe&apos;, -3.9218753018711578), (&apos;seagal&apos;, -3.3202501058581921), (&apos;unwatchable&apos;, -3.0269848170580955), (&apos;stinker&apos;, -2.9876839403711624), (&apos;mst&apos;, -2.7753833211707968), (&apos;incoherent&apos;, -2.7641396677532537), (&apos;unfunny&apos;, -2.5545257844967644), (&apos;waste&apos;, -2.4907515123361046), (&apos;blah&apos;, -2.4475792789485005), (&apos;horrid&apos;, -2.3715779644809971), (&apos;pointless&apos;, -2.3451073877136341), (&apos;atrocious&apos;, -2.3187369339642556), (&apos;redeeming&apos;, -2.2667790015910296), (&apos;prom&apos;, -2.2601040980178784), (&apos;drivel&apos;, -2.2476029585766928), (&apos;lousy&apos;, -2.2118080125207054), (&apos;worst&apos;, -2.1930856334332267), (&apos;laughable&apos;, -2.172468615469592), (&apos;awful&apos;, -2.1385076866397488), (&apos;poorly&apos;, -2.1326133844207011), (&apos;wasting&apos;, -2.1178155545614512), (&apos;remotely&apos;, -2.111046881095167), (&apos;existent&apos;, -2.0024805005437076), (&apos;boredom&apos;, -1.9241486572738005), (&apos;miserably&apos;, -1.9216610938019989), (&apos;sucks&apos;, -1.9166645809588516), (&apos;uninspired&apos;, -1.9131499212248517), (&apos;lame&apos;, -1.9117232884159072), (&apos;insult&apos;, -1.9085323769376259)] Transforming Text into Numbers12345from IPython.display import Imagereview = \"This was a horrible, terrible movie.\"Image(filename='sentiment_network.png') 123review = \"The movie was excellent\"Image(filename='sentiment_network_pos.png') Project 2: Creating the Input/Output Data123vocab = set(total_counts.keys())vocab_size = len(vocab)print(vocab_size) 74074 1list(vocab) [&apos;&apos;, &apos;werewoves&apos;, &apos;endowments&apos;, &apos;palace&apos;, &apos;persiflage&apos;, &apos;slasherville&apos;, &apos;locally&apos;, &apos;unrecycled&apos;, &apos;spearhead&apos;, &apos;allyson&apos;, &apos;manhating&apos;, &apos;bartok&apos;, &apos;gretorexes&apos;, &apos;soaks&apos;, &apos;protestations&apos;, &apos;superimposes&apos;, &apos;theirry&apos;, &apos;yaqui&apos;, &apos;contrives&apos;, &apos;accessorizing&apos;, &apos;arg&apos;, &apos;sanguine&apos;, &apos;batouch&apos;, &apos;asked&apos;, &apos;animals&apos;, &apos;cockpits&apos;, &apos;gorilla&apos;, &apos;diculous&apos;, &apos;establishing&apos;, &apos;kagemusha&apos;, &apos;sketches&apos;, &apos;rebuilt&apos;, &apos;perniciously&apos;, &apos;socioeconomic&apos;, &apos;ladylike&apos;, &apos;prognostication&apos;, &apos;blech&apos;, &apos;sugarbabe&apos;, &apos;desk&apos;, &apos;fez&apos;, &apos;accents&apos;, &apos;speach&apos;, &apos;rooster&apos;, &apos;effort&apos;, &apos;bodega&apos;, &apos;dong&apos;, &apos;preordained&apos;, &apos;dubliners&apos;, &apos;vili&apos;, &apos;imperatives&apos;, &apos;artifices&apos;, &apos;wieder&apos;, &apos;climate&apos;, &apos;whoopdedoodles&apos;, &apos;quatermass&apos;, &apos;inveterate&apos;, &apos;memorandum&apos;, &apos;crucially&apos;, &apos;bulimics&apos;, &apos;misdrawing&apos;, &apos;plympton&apos;, &apos;fireballs&apos;, &apos;verdant&apos;, &apos;testi&apos;, &apos;undeservingly&apos;, &apos;lusted&apos;, &apos;shylock&apos;, &apos;disinfecting&apos;, &apos;boxer&apos;, &apos;givney&apos;, &apos;hs&apos;, &apos;loser&apos;, &apos;civics&apos;, &apos;volcano&apos;, &apos;jur&apos;, &apos;mohnish&apos;, &apos;candidates&apos;, &apos;assemble&apos;, &apos;simi&apos;, &apos;resort&apos;, &apos;hessling&apos;, &apos;starbase&apos;, &apos;orgolini&apos;, &apos;starrett&apos;, &apos;weaker&apos;, &apos;transcending&apos;, &apos;levitate&apos;, &apos;spurns&apos;, &apos;contradictory&apos;, &apos;cambreau&apos;, &apos;latvia&apos;, &apos;kirkpatrick&apos;, &apos;betty&apos;, &apos;agnostic&apos;, &apos;sosa&apos;, &apos;kanji&apos;, &apos;swill&apos;, &apos;millinium&apos;, &apos;macgregor&apos;, &apos;brd&apos;, &apos;ariete&apos;, &apos;assassins&apos;, &apos;disscusion&apos;, &apos;legislative&apos;, &apos;dwars&apos;, &apos;controller&apos;, &apos;hadass&apos;, &apos;vega&apos;, &apos;bends&apos;, &apos;glock&apos;, &apos;spacewalk&apos;, &apos;va&apos;, &apos;offa&apos;, &apos;winfield&apos;, &apos;somewhat&apos;, &apos;yates&apos;, &apos;vinyl&apos;, &apos;complicity&apos;, &apos;bela&apos;, &apos;squishes&apos;, &apos;rippings&apos;, &apos;eyed&apos;, &apos;amatuerish&apos;, &apos;desilva&apos;, &apos;christmass&apos;, &apos;briley&apos;, &apos;bakhtyari&apos;, &apos;unmasked&apos;, &apos;huffman&apos;, &apos;fallacious&apos;, &apos;problem&apos;, &apos;sieger&apos;, &apos;koma&apos;, &apos;grovelling&apos;, &apos;incl&apos;, &apos;farlinger&apos;, &apos;teasers&apos;, &apos;huff&apos;, &apos;untried&apos;, &apos;crocker&apos;, &apos;dansu&apos;, &apos;scammers&apos;, &apos;popsicle&apos;, &apos;arthritic&apos;, &apos;grubs&apos;, &apos;exemplar&apos;, &apos;racial&apos;, &apos;verbiage&apos;, &apos;saloshin&apos;, &apos;painlessly&apos;, &apos;harewood&apos;, &apos;shart&apos;, &apos;keepers&apos;, &apos;archrivals&apos;, &apos;longish&apos;, &apos;batmobile&apos;, &apos;shakespearian&apos;, &apos;bestselling&apos;, &apos;spewing&apos;, &apos;midlands&apos;, &apos;trattoria&apos;, &apos;greenaway&apos;, &apos;gestapo&apos;, &apos;ed&apos;, &apos;huns&apos;, &apos;bloch&apos;, &apos;mashall&apos;, &apos;versy&apos;, &apos;david&apos;, &apos;sicilian&apos;, &apos;propositioned&apos;, &apos;eighty&apos;, &apos;carridine&apos;, &apos;delicates&apos;, &apos;veering&apos;, &apos;columbus&apos;, &apos;dunning&apos;, &apos;mercantile&apos;, &apos;rape&apos;, &apos;purely&apos;, &apos;rediscovered&apos;, &apos;abstinence&apos;, &apos;clunes&apos;, &apos;emerson&apos;, &apos;judgments&apos;, &apos;lawful&apos;, &apos;celebration&apos;, &apos;affirmative&apos;, &apos;sedately&apos;, &apos;sng&apos;, &apos;inuindo&apos;, &apos;mosely&apos;, &apos;bungalow&apos;, &apos;ninga&apos;, &apos;dripped&apos;, &apos;itallian&apos;, &apos;himalaya&apos;, &apos;shikoku&apos;, &apos;braik&apos;, &apos;grousing&apos;, &apos;nair&apos;, &apos;forrester&apos;, &apos;elemental&apos;, &apos;allegations&apos;, &apos;delilah&apos;, &apos;boneheaded&apos;, &apos;baltimoreans&apos;, &apos;dunebuggies&apos;, &apos;taguchi&apos;, &apos;coleseum&apos;, &apos;saratoga&apos;, &apos;ninotchka&apos;, &apos;afganistan&apos;, &apos;genorisity&apos;, &apos;haff&apos;, &apos;jennilee&apos;, &apos;jesues&apos;, &apos;dwarfs&apos;, &apos;enchilada&apos;, &apos;feminist&apos;, &apos;ghettoisation&apos;, &apos;handlebar&apos;, &apos;antagonistic&apos;, &apos;marian&apos;, &apos;crichton&apos;, &apos;ryo&apos;, &apos;mean&apos;, &apos;inheritance&apos;, &apos;presently&apos;, &apos;pear&apos;, &apos;inequality&apos;, &apos;stately&apos;, &apos;nooo&apos;, &apos;obscurities&apos;, &apos;determinedly&apos;, &apos;solemn&apos;, &apos;sullenly&apos;, &apos;machism&apos;, &apos;tingled&apos;, &apos;maschera&apos;, &apos;tristran&apos;, &apos;mendoza&apos;, &apos;baked&apos;, &apos;jonatha&apos;, &apos;lowly&apos;, &apos;halliwell&apos;, &apos;msted&apos;, &apos;rodann&apos;, &apos;hunkered&apos;, &apos;cashmere&apos;, &apos;chevalia&apos;, &apos;jakub&apos;, &apos;dobermann&apos;, &apos;overexxagerating&apos;, &apos;pfennig&apos;, &apos;fisted&apos;, &apos;mcelwee&apos;, &apos;chief&apos;, &apos;parlor&apos;, &apos;browbeating&apos;, &apos;parasol&apos;, &apos;negligible&apos;, &apos;kira&apos;, &apos;monceau&apos;, &apos;blew&apos;, &apos;odete&apos;, &apos;muco&apos;, &apos;predominantly&apos;, &apos;levon&apos;, &apos;discourage&apos;, &apos;fragmented&apos;, &apos;vandermey&apos;, &apos;etude&apos;, &apos;mitch&apos;, &apos;sandbag&apos;, &apos;bending&apos;, &apos;dizzying&apos;, &apos;mover&apos;, &apos;rewired&apos;, &apos;awww&apos;, &apos;di&apos;, &apos;bejesus&apos;, &apos;wallet&apos;, &apos;uprooting&apos;, &apos;atari&apos;, &apos;dreamlike&apos;, &apos;exacted&apos;, &apos;harbouring&apos;, &apos;indiscreet&apos;, &apos;turks&apos;, &apos;gems&apos;, &apos;hoboken&apos;, &apos;yalom&apos;, &apos;rooftop&apos;, &apos;howit&apos;, &apos;tolson&apos;, &apos;tulane&apos;, &apos;reductive&apos;, &apos;catharthic&apos;, &apos;famarialy&apos;, &apos;sista&apos;, &apos;ghidorah&apos;, &apos;ngoombujarra&apos;, &apos;intently&apos;, &apos;jlu&apos;, &apos;kyrptonite&apos;, &apos;hilda&apos;, &apos;census&apos;, &apos;baguette&apos;, &apos;mondrians&apos;, &apos;advisable&apos;, &apos;mcnee&apos;, &apos;candlelit&apos;, &apos;affability&apos;, &apos;intercut&apos;, &apos;installations&apos;, &apos;elliptical&apos;, &apos;washer&apos;, &apos;colt&apos;, &apos;pevensie&apos;, &apos;outshined&apos;, &apos;despotic&apos;, &apos;suares&apos;, &apos;privates&apos;, &apos;scrabble&apos;, &apos;milliardo&apos;, &apos;booting&apos;, &apos;rowan&apos;, &apos;golmaal&apos;, &apos;pueblos&apos;, &apos;msf&apos;, &apos;giulietta&apos;, &apos;phili&apos;, &apos;pleaseee&apos;, &apos;connecticute&apos;, &apos;rosnelski&apos;, &apos;tenebra&apos;, &apos;bako&apos;, &apos;blessings&apos;, &apos;smudge&apos;, &apos;cya&apos;, &apos;pummel&apos;, &apos;brocks&apos;, &apos;homere&apos;, &apos;propellant&apos;, &apos;deliveried&apos;, &apos;finisham&apos;, &apos;newsradio&apos;, &apos;bernie&apos;, &apos;gouden&apos;, &apos;enchant&apos;, &apos;bessie&apos;, &apos;semisubmerged&apos;, &apos;extraterrestrial&apos;, &apos;believably&apos;, &apos;accomplice&apos;, &apos;dooku&apos;, &apos;baja&apos;, &apos;met&apos;, &apos;circulate&apos;, &apos;disobeyed&apos;, &apos;quakerly&apos;, &apos;overstyling&apos;, &apos;softens&apos;, &apos;units&apos;, &apos;shaye&apos;, &apos;starters&apos;, &apos;gripes&apos;, &apos;nightmarish&apos;, &apos;patriotic&apos;, &apos;goodtimes&apos;, &apos;stroheim&apos;, &apos;debit&apos;, &apos;prissies&apos;, &apos;woebegone&apos;, &apos;deputies&apos;, &apos;awkwardness&apos;, &apos;obama&apos;, &apos;tarazu&apos;, &apos;kendra&apos;, &apos;patriots&apos;, &apos;helpfuls&apos;, &apos;mightily&apos;, &apos;polemical&apos;, &apos;unruly&apos;, &apos;planing&apos;, &apos;paperhouse&apos;, &apos;sororities&apos;, &apos;pym&apos;, &apos;therin&apos;, &apos;tarkovsky&apos;, &apos;rdiger&apos;, &apos;resembling&apos;, &apos;gimmicks&apos;, &apos;iler&apos;, &apos;lineal&apos;, &apos;taming&apos;, &apos;mortenson&apos;, &apos;waugh&apos;, &apos;furies&apos;, &apos;grufford&apos;, &apos;hammill&apos;, &apos;plunkett&apos;, &apos;paterson&apos;, &apos;konishita&apos;, &apos;immorality&apos;, &apos;angelos&apos;, &apos;kebbel&apos;, &apos;tamiroff&apos;, &apos;boen&apos;, &apos;rivalry&apos;, &apos;ethnic&apos;, &apos;funner&apos;, &apos;troops&apos;, &apos;deadeningly&apos;, &apos;watcha&apos;, &apos;dhiraj&apos;, &apos;haranguing&apos;, &apos;dejas&apos;, &apos;weasely&apos;, &apos;category&apos;, &apos;laughable&apos;, &apos;gramps&apos;, &apos;safdar&apos;, &apos;calorie&apos;, &apos;scandi&apos;, &apos;cannon&apos;, &apos;maliciously&apos;, &apos;bothered&apos;, &apos;troi&apos;, &apos;couleur&apos;, &apos;visionary&apos;, &apos;fizzles&apos;, &apos;evangalizing&apos;, &apos;reeves&apos;, &apos;bombadier&apos;, &apos;bowlegged&apos;, &apos;custody&apos;, &apos;weta&apos;, &apos;archambault&apos;, &apos;warlords&apos;, &apos;makeout&apos;, &apos;bonbons&apos;, &apos;importances&apos;, &apos;baruchel&apos;, &apos;floyd&apos;, &apos;infirm&apos;, &apos;bloodwaters&apos;, &apos;ashford&apos;, &apos;colleagues&apos;, &apos;discern&apos;, &apos;thunderjet&apos;, &apos;pullers&apos;, &apos;evos&apos;, &apos;celebrations&apos;, &apos;seely&apos;, &apos;nasty&apos;, &apos;keach&apos;, &apos;tonge&apos;, &apos;senki&apos;, &apos;approxiamtely&apos;, &apos;unable&apos;, &apos;rayburn&apos;, &apos;britons&apos;, &apos;christoph&apos;, &apos;proctor&apos;, &apos;tapped&apos;, &apos;lenz&apos;, &apos;vengeant&apos;, &apos;exaggerating&apos;, &apos;mle&apos;, &apos;declaims&apos;, &apos;hight&apos;, &apos;repetoir&apos;, &apos;yolu&apos;, &apos;smarty&apos;, &apos;steels&apos;, &apos;openness&apos;, &apos;coached&apos;, &apos;archiving&apos;, &apos;horrendous&apos;, &apos;engages&apos;, &apos;loosing&apos;, &apos;anchorpoint&apos;, &apos;fecal&apos;, &apos;gracefully&apos;, &apos;tapioca&apos;, &apos;bizniss&apos;, &apos;overhyped&apos;, &apos;shortland&apos;, &apos;cleansed&apos;, &apos;negativity&apos;, &apos;gushy&apos;, &apos;mortitz&apos;, &apos;stripper&apos;, &apos;woke&apos;, &apos;slayers&apos;, &apos;uncensored&apos;, &apos;textiles&apos;, &apos;louda&apos;, &apos;castrati&apos;, &apos;altmanesque&apos;, &apos;yes&apos;, &apos;huntress&apos;, &apos;urging&apos;, &apos;tua&apos;, &apos;sentient&apos;, &apos;kellogg&apos;, &apos;cheerful&apos;, &apos;swanks&apos;, &apos;shor&apos;, &apos;cheapo&apos;, &apos;flourishing&apos;, &apos;tap&apos;, &apos;kph&apos;, &apos;bobbidi&apos;, &apos;tangos&apos;, &apos;honey&apos;, &apos;oswald&apos;, &apos;philippians&apos;, &apos;payroll&apos;, &apos;chooses&apos;, &apos;archtypes&apos;, &apos;generators&apos;, &apos;grillo&apos;, &apos;horrorible&apos;, &apos;yellowing&apos;, &apos;vancouver&apos;, &apos;thet&apos;, &apos;babtise&apos;, &apos;participates&apos;, &apos;uriah&apos;, &apos;loust&apos;, &apos;ravishingly&apos;, &apos;punishing&apos;, &apos;jhoom&apos;, &apos;lulling&apos;, &apos;stetting&apos;, &apos;wierd&apos;, &apos;truce&apos;, &apos;peerce&apos;, &apos;transpose&apos;, &apos;unplanned&apos;, &apos;unmistakeably&apos;, &apos;approval&apos;, &apos;amontillado&apos;, &apos;een&apos;, &apos;lefties&apos;, &apos;tentatives&apos;, &apos;mysteriousness&apos;, &apos;mid&apos;, &apos;technicians&apos;, &apos;wich&apos;, &apos;englund&apos;, &apos;freespirited&apos;, &apos;kun&apos;, &apos;discourses&apos;, &apos;nyily&apos;, &apos;honorably&apos;, &apos;hankerchief&apos;, &apos;nugget&apos;, &apos;nationalism&apos;, &apos;reveals&apos;, &apos;lamppost&apos;, &apos;tempra&apos;, &apos;sanctimoniousness&apos;, &apos;wardrobes&apos;, &apos;visa&apos;, &apos;lenses&apos;, &apos;johars&apos;, &apos;prefers&apos;, &apos;webster&apos;, &apos;marcuzzo&apos;, &apos;licensable&apos;, &apos;brilliancy&apos;, &apos;gumbas&apos;, &apos;jacoby&apos;, &apos;twine&apos;, &apos;entices&apos;, &apos;unpremeditated&apos;, &apos;jin&apos;, &apos;affirmatively&apos;, &apos;joyful&apos;, &apos;plotkurt&apos;, &apos;danniele&apos;, &apos;rpond&apos;, &apos;flare&apos;, &apos;lester&apos;, &apos;toying&apos;, &apos;having&apos;, &apos;anorexia&apos;, &apos;hoof&apos;, &apos;stillman&apos;, &apos;hows&apos;, &apos;contrite&apos;, &apos;hersholt&apos;, &apos;utterance&apos;, &apos;superflous&apos;, &apos;orders&apos;, &apos;pamelyn&apos;, &apos;traumatized&apos;, &apos;poder&apos;, &apos;virtuality&apos;, &apos;reaper&apos;, &apos;trini&apos;, &apos;phantasm&apos;, &apos;fbp&apos;, &apos;nuked&apos;, &apos;siegfried&apos;, &apos;ralph&apos;, &apos;erwin&apos;, &apos;rhymer&apos;, &apos;christien&apos;, &apos;sidekick&apos;, &apos;grasshopper&apos;, &apos;steryotypes&apos;, &apos;donnagio&apos;, &apos;denny&apos;, &apos;fraudulent&apos;, &apos;weisse&apos;, &apos;yoji&apos;, &apos;adapters&apos;, &apos;andalthough&apos;, &apos;fee&apos;, &apos;attorney&apos;, &apos;holliday&apos;, &apos;prerequisite&apos;, &apos;ives&apos;, &apos;yvaine&apos;, &apos;smaller&apos;, &apos;satired&apos;, &apos;ghillie&apos;, &apos;hagelin&apos;, &apos;upsurge&apos;, &apos;empirical&apos;, &apos;smap&apos;, &apos;kirk&apos;, &apos;conservatism&apos;, &apos;wesley&apos;, &apos;becuz&apos;, &apos;fantasia&apos;, &apos;treadstone&apos;, &apos;berdalh&apos;, &apos;reaganomics&apos;, &apos;schwarzenberg&apos;, &apos;housemann&apos;, &apos;jumpstart&apos;, &apos;glamorise&apos;, &apos;braves&apos;, &apos;simply&apos;, &apos;which&apos;, &apos;knifes&apos;, &apos;ramblings&apos;, &apos;bused&apos;, &apos;lombardo&apos;, &apos;refresher&apos;, &apos;evenings&apos;, &apos;openings&apos;, &apos;rings&apos;, &apos;reverend&apos;, &apos;blurry&apos;, &apos;baldy&apos;, &apos;acing&apos;, &apos;mollys&apos;, &apos;meditteranean&apos;, &apos;workday&apos;, &apos;apologies&apos;, &apos;empathise&apos;, &apos;outs&apos;, &apos;hmmmmmmmm&apos;, &apos;enquiry&apos;, &apos;detector&apos;, &apos;copying&apos;, &apos;outlive&apos;, &apos;gangsta&apos;, &apos;koyaanisqatsi&apos;, &apos;entrenches&apos;, &apos;author&apos;, &apos;undistinguished&apos;, &apos;izzard&apos;, &apos;orgue&apos;, &apos;negotiator&apos;, &apos;behaviorally&apos;, &apos;eyebrowed&apos;, &apos;maximizes&apos;, &apos;pilippinos&apos;, &apos;recurred&apos;, &apos;bullt&apos;, &apos;infinnerty&apos;, &apos;suspicious&apos;, &apos;uncooked&apos;, &apos;these&apos;, &apos;ozaki&apos;, &apos;sweden&apos;, &apos;petition&apos;, &apos;opium&apos;, &apos;complacency&apos;, &apos;deux&apos;, &apos;kramer&apos;, &apos;opt&apos;, &apos;auras&apos;, &apos;shyamalan&apos;, &apos;lamore&apos;, &apos;sunbathing&apos;, &apos;toxins&apos;, &apos;limned&apos;, &apos;khali&apos;, &apos;jefferey&apos;, &apos;interviewee&apos;, &apos;righted&apos;, &apos;grandmammy&apos;, &apos;wol&apos;, &apos;verica&apos;, &apos;footwork&apos;, &apos;doug&apos;, &apos;euthanasiarist&apos;, &apos;repeating&apos;, &apos;debutante&apos;, &apos;trusts&apos;, &apos;righto&apos;, &apos;phyllida&apos;, &apos;upa&apos;, &apos;doogie&apos;, &apos;gig&apos;, &apos;violins&apos;, &apos;ardor&apos;, &apos;ould&apos;, &apos;stymieing&apos;, &apos;libs&apos;, &apos;alejo&apos;, &apos;sick&apos;, &apos;propensities&apos;, &apos;occasions&apos;, &apos;spiderman&apos;, &apos;limousines&apos;, &apos;hearkening&apos;, &apos;reinstated&apos;, &apos;concede&apos;, &apos;vineyard&apos;, &apos;image&apos;, &apos;waxed&apos;, &apos;inuyasha&apos;, &apos;paralyzed&apos;, &apos;notches&apos;, &apos;latifah&apos;, &apos;mediation&apos;, &apos;cozies&apos;, &apos;spirit&apos;, &apos;fathoms&apos;, &apos;uecker&apos;, &apos;hoochie&apos;, &apos;akria&apos;, &apos;praises&apos;, &apos;wiring&apos;, &apos;pastparticularly&apos;, &apos;ghastliness&apos;, &apos;artiness&apos;, &apos;gruner&apos;, &apos;admirals&apos;, &apos;egger&apos;, &apos;extract&apos;, &apos;guiltlessly&apos;, &apos;pie&apos;, &apos;audaciousness&apos;, &apos;stallonethat&apos;, &apos;balconys&apos;, &apos;cassi&apos;, &apos;definable&apos;, &apos;rote&apos;, &apos;assaulted&apos;, &apos;schmoeller&apos;, &apos;cancer&apos;, &apos;equality&apos;, &apos;kruk&apos;, &apos;whoah&apos;, &apos;dalai&apos;, &apos;tuareg&apos;, &apos;split&apos;, &apos;bollywood&apos;, &apos;mates&apos;, &apos;supports&apos;, &apos;whiskers&apos;, &apos;meres&apos;, &apos;plasticine&apos;, &apos;bartel&apos;, &apos;phrase&apos;, &apos;poldark&apos;, &apos;pylon&apos;, &apos;undefined&apos;, &apos;videographer&apos;, &apos;blithesome&apos;, &apos;prendergast&apos;, &apos;goddard&apos;, &apos;spectular&apos;, &apos;fof&apos;, &apos;kiddie&apos;, &apos;accelerating&apos;, &apos;secreted&apos;, &apos;manslaughter&apos;, &apos;akimbo&apos;, &apos;privacy&apos;, &apos;michigan&apos;, &apos;ambiguities&apos;, &apos;belabors&apos;, &apos;mol&apos;, &apos;disemboweled&apos;, &apos;creely&apos;, &apos;nosebleed&apos;, &apos;autobiography&apos;, &apos;dispelled&apos;, &apos;lancie&apos;, &apos;revolutionaries&apos;, &apos;allende&apos;, &apos;jacy&apos;, &apos;kostic&apos;, &apos;tormei&apos;, &apos;chiefly&apos;, &apos;atmospheric&apos;, &apos;europa&apos;, &apos;judmila&apos;, &apos;extremal&apos;, &apos;decaprio&apos;, &apos;amore&apos;, &apos;cockneys&apos;, &apos;chong&apos;, &apos;coordinates&apos;, &apos;ctomvelu&apos;, &apos;scums&apos;, &apos;valleyspeak&apos;, &apos;minstrel&apos;, &apos;shoddier&apos;, &apos;combusted&apos;, &apos;tirade&apos;, &apos;marketplaces&apos;, &apos;reflex&apos;, &apos;rjt&apos;, &apos;deckard&apos;, &apos;godfathers&apos;, &apos;sibling&apos;, &apos;erupted&apos;, &apos;wasnt&apos;, &apos;lollipop&apos;, &apos;narcotics&apos;, &apos;showdowns&apos;, &apos;excess&apos;, &apos;taught&apos;, &apos;persuade&apos;, &apos;homer&apos;, &apos;binysh&apos;, &apos;ravaging&apos;, &apos;minutest&apos;, &apos;yomada&apos;, &apos;leckie&apos;, &apos;snazzy&apos;, &apos;rafting&apos;, &apos;grendelif&apos;, &apos;nemeses&apos;, &apos;westmore&apos;, &apos;sty&apos;, &apos;puertorricans&apos;, &apos;zaara&apos;, &apos;timemachine&apos;, &apos;similarities&apos;, &apos;colera&apos;, &apos;firefall&apos;, &apos;winked&apos;, &apos;painkiller&apos;, &apos;leaflets&apos;, &apos;tehran&apos;, &apos;hooker&apos;, &apos;appalingly&apos;, &apos;humility&apos;, &apos;illegitimate&apos;, &apos;coer&apos;, &apos;responisible&apos;, &apos;conceded&apos;, &apos;scarves&apos;, &apos;dawid&apos;, &apos;overflows&apos;, &apos;annuder&apos;, &apos;nickelodean&apos;, &apos;comanche&apos;, &apos;betrail&apos;, &apos;pillage&apos;, &apos;daffy&apos;, &apos;dobson&apos;, &apos;tessier&apos;, &apos;egoism&apos;, &apos;meanie&apos;, &apos;trancers&apos;, &apos;sequences&apos;, &apos;viciente&apos;, &apos;redlich&apos;, &apos;filmfrderung&apos;, &apos;leveled&apos;, &apos;performer&apos;, &apos;opponent&apos;, &apos;appears&apos;, &apos;squeaks&apos;, &apos;peripheral&apos;, &apos;blimey&apos;, &apos;glass&apos;, &apos;captors&apos;, &apos;strains&apos;, &apos;codenamealexa&apos;, &apos;tooo&apos;, &apos;aiello&apos;, &apos;matines&apos;, &apos;calibre&apos;, &apos;tighten&apos;, &apos;papercuts&apos;, &apos;necrotic&apos;, &apos;hums&apos;, &apos;kavner&apos;, &apos;employers&apos;, &apos;troy&apos;, &apos;almerayeda&apos;, &apos;barnet&apos;, &apos;nicotero&apos;, &apos;rush&apos;, &apos;ahehehe&apos;, &apos;dui&apos;, &apos;bleeps&apos;, &apos;heroe&apos;, &apos;gangreen&apos;, &apos;paintbrush&apos;, &apos;dowager&apos;, &apos;khakkee&apos;, &apos;chariots&apos;, &apos;benfer&apos;, &apos;mcneely&apos;, &apos;quelled&apos;, &apos;blockheads&apos;, &apos;dufy&apos;, &apos;badmen&apos;, &apos;dondaro&apos;, &apos;nachoo&apos;, &apos;intercedes&apos;, &apos;looksand&apos;, &apos;hasidic&apos;, &apos;will&apos;, &apos;practicable&apos;, &apos;reading&apos;, &apos;manufacture&apos;, &apos;bao&apos;, &apos;cigarette&apos;, &apos;chomps&apos;, &apos;subverting&apos;, &apos;reichdeutch&apos;, &apos;dexter&apos;, &apos;hrishitta&apos;, &apos;splitting&apos;, &apos;uproarious&apos;, &apos;ametuer&apos;, &apos;speedway&apos;, &apos;worser&apos;, &apos;brisco&apos;, &apos;stream&apos;, &apos;etre&apos;, &apos;lengths&apos;, &apos;chimpnaut&apos;, &apos;corny&apos;, &apos;stirring&apos;, &apos;tremendous&apos;, &apos;tually&apos;, &apos;mnage&apos;, &apos;ashitaka&apos;, &apos;crossbows&apos;, &apos;hackery&apos;, &apos;riker&apos;, &apos;twelve&apos;, &apos;freshner&apos;, &apos;bobbie&apos;, &apos;percussion&apos;, &apos;overpopulation&apos;, &apos;eeeekkk&apos;, &apos;centaury&apos;, &apos;summitting&apos;, &apos;andbest&apos;, &apos;pumping&apos;, &apos;somnolent&apos;, &apos;infatuation&apos;, &apos;shakesphere&apos;, &apos;ingred&apos;, &apos;moon&apos;, &apos;keven&apos;, &apos;sanguisuga&apos;, &apos;quivers&apos;, &apos;equalling&apos;, &apos;vaugely&apos;, &apos;supervising&apos;, &apos;dissolved&apos;, &apos;cheshire&apos;, &apos;retribution&apos;, &apos;cartoons&apos;, &apos;maisie&apos;, &apos;reptiles&apos;, &apos;rsther&apos;, &apos;erratically&apos;, &apos;hoyt&apos;, ...] 1234import numpy as nplayer_0 = np.zeros((1,vocab_size))layer_0 array([[ 0., 0., 0., ..., 0., 0., 0.]]) 12from IPython.display import ImageImage(filename='sentiment_network.png') 12345word2index = &#123;&#125;for i,word in enumerate(vocab): word2index[word] = iword2index {&apos;&apos;: 0, &apos;werewoves&apos;: 1, &apos;endowments&apos;: 2, &apos;palace&apos;: 3, &apos;persiflage&apos;: 4, &apos;slasherville&apos;: 5, &apos;locally&apos;: 6, &apos;unrecycled&apos;: 7, &apos;spearhead&apos;: 8, &apos;allyson&apos;: 9, &apos;manhating&apos;: 10, &apos;bartok&apos;: 11, &apos;gretorexes&apos;: 12, &apos;soaks&apos;: 13, &apos;protestations&apos;: 14, &apos;superimposes&apos;: 15, &apos;theirry&apos;: 16, &apos;yaqui&apos;: 17, &apos;contrives&apos;: 18, &apos;accessorizing&apos;: 19, &apos;arg&apos;: 20, &apos;sanguine&apos;: 21, &apos;batouch&apos;: 22, &apos;asked&apos;: 23, &apos;animals&apos;: 24, &apos;cockpits&apos;: 25, &apos;gorilla&apos;: 26, &apos;diculous&apos;: 27, &apos;establishing&apos;: 28, &apos;kagemusha&apos;: 29, &apos;sketches&apos;: 30, &apos;rebuilt&apos;: 31, &apos;perniciously&apos;: 32, &apos;socioeconomic&apos;: 33, &apos;ladylike&apos;: 34, &apos;prognostication&apos;: 35, &apos;blech&apos;: 36, &apos;sugarbabe&apos;: 37, &apos;desk&apos;: 38, &apos;fez&apos;: 39, &apos;accents&apos;: 40, &apos;speach&apos;: 41, &apos;rooster&apos;: 42, &apos;effort&apos;: 43, &apos;bodega&apos;: 44, &apos;dong&apos;: 45, &apos;preordained&apos;: 46, &apos;dubliners&apos;: 47, &apos;vili&apos;: 48, &apos;imperatives&apos;: 49, &apos;artifices&apos;: 50, &apos;wieder&apos;: 51, &apos;climate&apos;: 52, &apos;whoopdedoodles&apos;: 53, &apos;quatermass&apos;: 54, &apos;inveterate&apos;: 55, &apos;memorandum&apos;: 56, &apos;crucially&apos;: 57, &apos;bulimics&apos;: 58, &apos;misdrawing&apos;: 59, &apos;plympton&apos;: 60, &apos;fireballs&apos;: 61, &apos;verdant&apos;: 62, &apos;testi&apos;: 63, &apos;undeservingly&apos;: 64, &apos;lusted&apos;: 65, &apos;shylock&apos;: 66, &apos;disinfecting&apos;: 67, &apos;boxer&apos;: 68, &apos;givney&apos;: 69, &apos;hs&apos;: 70, &apos;loser&apos;: 71, &apos;civics&apos;: 72, &apos;volcano&apos;: 73, &apos;jur&apos;: 74, &apos;mohnish&apos;: 75, &apos;candidates&apos;: 76, &apos;assemble&apos;: 77, &apos;simi&apos;: 78, &apos;resort&apos;: 79, &apos;hessling&apos;: 80, &apos;starbase&apos;: 81, &apos;orgolini&apos;: 82, &apos;starrett&apos;: 83, &apos;weaker&apos;: 84, &apos;transcending&apos;: 85, &apos;levitate&apos;: 86, &apos;spurns&apos;: 87, &apos;contradictory&apos;: 88, &apos;cambreau&apos;: 89, &apos;latvia&apos;: 90, &apos;kirkpatrick&apos;: 91, &apos;betty&apos;: 92, &apos;agnostic&apos;: 93, &apos;sosa&apos;: 94, &apos;kanji&apos;: 95, &apos;swill&apos;: 96, &apos;millinium&apos;: 97, &apos;macgregor&apos;: 98, &apos;brd&apos;: 99, &apos;ariete&apos;: 100, &apos;assassins&apos;: 101, &apos;disscusion&apos;: 102, &apos;legislative&apos;: 103, &apos;dwars&apos;: 104, &apos;controller&apos;: 105, &apos;hadass&apos;: 106, &apos;vega&apos;: 107, &apos;bends&apos;: 108, &apos;glock&apos;: 109, &apos;spacewalk&apos;: 110, &apos;va&apos;: 111, &apos;offa&apos;: 112, &apos;winfield&apos;: 113, &apos;somewhat&apos;: 114, &apos;yates&apos;: 115, &apos;vinyl&apos;: 116, &apos;complicity&apos;: 117, &apos;bela&apos;: 118, &apos;squishes&apos;: 119, &apos;rippings&apos;: 120, &apos;eyed&apos;: 121, &apos;amatuerish&apos;: 122, &apos;desilva&apos;: 123, &apos;christmass&apos;: 124, &apos;briley&apos;: 125, &apos;bakhtyari&apos;: 126, &apos;unmasked&apos;: 127, &apos;huffman&apos;: 128, &apos;fallacious&apos;: 129, &apos;problem&apos;: 130, &apos;sieger&apos;: 131, &apos;koma&apos;: 132, &apos;grovelling&apos;: 133, &apos;incl&apos;: 134, &apos;farlinger&apos;: 135, &apos;teasers&apos;: 136, &apos;huff&apos;: 137, &apos;untried&apos;: 138, &apos;crocker&apos;: 139, &apos;dansu&apos;: 140, &apos;scammers&apos;: 141, &apos;popsicle&apos;: 142, &apos;arthritic&apos;: 143, &apos;grubs&apos;: 144, &apos;exemplar&apos;: 145, &apos;racial&apos;: 146, &apos;verbiage&apos;: 147, &apos;saloshin&apos;: 148, &apos;painlessly&apos;: 149, &apos;harewood&apos;: 150, &apos;shart&apos;: 151, &apos;keepers&apos;: 152, &apos;archrivals&apos;: 153, &apos;longish&apos;: 154, &apos;batmobile&apos;: 155, &apos;shakespearian&apos;: 156, &apos;bestselling&apos;: 157, &apos;spewing&apos;: 158, &apos;midlands&apos;: 159, &apos;trattoria&apos;: 160, &apos;greenaway&apos;: 161, &apos;gestapo&apos;: 162, &apos;ed&apos;: 163, &apos;huns&apos;: 164, &apos;bloch&apos;: 165, &apos;mashall&apos;: 166, &apos;versy&apos;: 167, &apos;david&apos;: 168, &apos;sicilian&apos;: 169, &apos;propositioned&apos;: 170, &apos;eighty&apos;: 171, &apos;carridine&apos;: 172, &apos;delicates&apos;: 173, &apos;veering&apos;: 174, &apos;columbus&apos;: 175, &apos;dunning&apos;: 176, &apos;mercantile&apos;: 177, &apos;rape&apos;: 178, &apos;purely&apos;: 179, &apos;rediscovered&apos;: 180, &apos;abstinence&apos;: 181, &apos;clunes&apos;: 182, &apos;emerson&apos;: 183, &apos;judgments&apos;: 184, &apos;lawful&apos;: 185, &apos;celebration&apos;: 186, &apos;affirmative&apos;: 187, &apos;sedately&apos;: 188, &apos;sng&apos;: 189, &apos;inuindo&apos;: 190, &apos;mosely&apos;: 191, &apos;bungalow&apos;: 192, &apos;ninga&apos;: 193, &apos;dripped&apos;: 194, &apos;itallian&apos;: 195, &apos;himalaya&apos;: 196, &apos;shikoku&apos;: 197, &apos;braik&apos;: 198, &apos;grousing&apos;: 199, &apos;nair&apos;: 200, &apos;forrester&apos;: 201, &apos;elemental&apos;: 202, &apos;allegations&apos;: 203, &apos;delilah&apos;: 204, &apos;boneheaded&apos;: 205, &apos;baltimoreans&apos;: 206, &apos;dunebuggies&apos;: 207, &apos;taguchi&apos;: 208, &apos;coleseum&apos;: 209, &apos;saratoga&apos;: 210, &apos;ninotchka&apos;: 211, &apos;afganistan&apos;: 212, &apos;genorisity&apos;: 213, &apos;haff&apos;: 214, &apos;jennilee&apos;: 215, &apos;jesues&apos;: 216, &apos;dwarfs&apos;: 217, &apos;enchilada&apos;: 218, &apos;feminist&apos;: 219, &apos;ghettoisation&apos;: 220, &apos;handlebar&apos;: 221, &apos;antagonistic&apos;: 222, &apos;marian&apos;: 223, &apos;crichton&apos;: 224, &apos;ryo&apos;: 225, &apos;mean&apos;: 226, &apos;inheritance&apos;: 227, &apos;presently&apos;: 228, &apos;pear&apos;: 229, &apos;inequality&apos;: 230, &apos;stately&apos;: 231, &apos;nooo&apos;: 232, &apos;obscurities&apos;: 233, &apos;determinedly&apos;: 234, &apos;solemn&apos;: 235, &apos;sullenly&apos;: 236, &apos;machism&apos;: 237, &apos;tingled&apos;: 238, &apos;maschera&apos;: 239, &apos;tristran&apos;: 240, &apos;mendoza&apos;: 241, &apos;baked&apos;: 242, &apos;jonatha&apos;: 243, &apos;lowly&apos;: 244, &apos;halliwell&apos;: 245, &apos;msted&apos;: 246, &apos;rodann&apos;: 247, &apos;hunkered&apos;: 248, &apos;cashmere&apos;: 249, &apos;chevalia&apos;: 250, &apos;jakub&apos;: 251, &apos;dobermann&apos;: 252, &apos;overexxagerating&apos;: 253, &apos;pfennig&apos;: 254, &apos;fisted&apos;: 255, &apos;mcelwee&apos;: 256, &apos;chief&apos;: 257, &apos;parlor&apos;: 258, &apos;browbeating&apos;: 259, &apos;parasol&apos;: 260, &apos;negligible&apos;: 261, &apos;kira&apos;: 262, &apos;monceau&apos;: 263, &apos;blew&apos;: 264, &apos;odete&apos;: 265, &apos;muco&apos;: 266, &apos;predominantly&apos;: 267, &apos;levon&apos;: 268, &apos;discourage&apos;: 269, &apos;fragmented&apos;: 270, &apos;vandermey&apos;: 271, &apos;etude&apos;: 272, &apos;mitch&apos;: 273, &apos;sandbag&apos;: 274, &apos;bending&apos;: 275, &apos;dizzying&apos;: 276, &apos;mover&apos;: 277, &apos;rewired&apos;: 278, &apos;awww&apos;: 279, &apos;di&apos;: 280, &apos;bejesus&apos;: 281, &apos;wallet&apos;: 282, &apos;uprooting&apos;: 283, &apos;atari&apos;: 284, &apos;dreamlike&apos;: 285, &apos;exacted&apos;: 286, &apos;harbouring&apos;: 287, &apos;indiscreet&apos;: 288, &apos;turks&apos;: 289, &apos;gems&apos;: 290, &apos;hoboken&apos;: 291, &apos;yalom&apos;: 292, &apos;rooftop&apos;: 293, &apos;howit&apos;: 294, &apos;tolson&apos;: 295, &apos;tulane&apos;: 296, &apos;reductive&apos;: 297, &apos;catharthic&apos;: 298, &apos;famarialy&apos;: 299, &apos;sista&apos;: 300, &apos;ghidorah&apos;: 301, &apos;ngoombujarra&apos;: 302, &apos;intently&apos;: 303, &apos;jlu&apos;: 304, &apos;kyrptonite&apos;: 305, &apos;hilda&apos;: 306, &apos;census&apos;: 307, &apos;baguette&apos;: 308, &apos;mondrians&apos;: 309, &apos;advisable&apos;: 310, &apos;mcnee&apos;: 311, &apos;candlelit&apos;: 312, &apos;affability&apos;: 313, &apos;intercut&apos;: 314, &apos;installations&apos;: 315, &apos;elliptical&apos;: 316, &apos;washer&apos;: 317, &apos;colt&apos;: 318, &apos;pevensie&apos;: 319, &apos;outshined&apos;: 320, &apos;despotic&apos;: 321, &apos;suares&apos;: 322, &apos;privates&apos;: 323, &apos;scrabble&apos;: 324, &apos;milliardo&apos;: 325, &apos;booting&apos;: 326, &apos;rowan&apos;: 327, &apos;golmaal&apos;: 328, &apos;pueblos&apos;: 329, &apos;msf&apos;: 330, &apos;giulietta&apos;: 331, &apos;phili&apos;: 332, &apos;pleaseee&apos;: 333, &apos;connecticute&apos;: 334, &apos;rosnelski&apos;: 335, &apos;tenebra&apos;: 336, &apos;bako&apos;: 337, &apos;blessings&apos;: 338, &apos;smudge&apos;: 339, &apos;cya&apos;: 340, &apos;pummel&apos;: 341, &apos;brocks&apos;: 342, &apos;homere&apos;: 343, &apos;propellant&apos;: 344, &apos;deliveried&apos;: 345, &apos;finisham&apos;: 346, &apos;newsradio&apos;: 347, &apos;bernie&apos;: 348, &apos;gouden&apos;: 349, &apos;enchant&apos;: 350, &apos;bessie&apos;: 351, &apos;semisubmerged&apos;: 352, &apos;extraterrestrial&apos;: 353, &apos;believably&apos;: 354, &apos;accomplice&apos;: 355, &apos;dooku&apos;: 356, &apos;baja&apos;: 357, &apos;met&apos;: 358, &apos;circulate&apos;: 359, &apos;disobeyed&apos;: 360, &apos;quakerly&apos;: 361, &apos;overstyling&apos;: 362, &apos;softens&apos;: 363, &apos;units&apos;: 364, &apos;shaye&apos;: 365, &apos;starters&apos;: 366, &apos;gripes&apos;: 367, &apos;nightmarish&apos;: 368, &apos;patriotic&apos;: 369, &apos;goodtimes&apos;: 370, &apos;stroheim&apos;: 371, &apos;debit&apos;: 372, &apos;prissies&apos;: 373, &apos;woebegone&apos;: 374, &apos;deputies&apos;: 375, &apos;awkwardness&apos;: 376, &apos;obama&apos;: 377, &apos;tarazu&apos;: 378, &apos;kendra&apos;: 379, &apos;patriots&apos;: 380, &apos;helpfuls&apos;: 381, &apos;mightily&apos;: 382, &apos;polemical&apos;: 383, &apos;unruly&apos;: 384, &apos;planing&apos;: 385, &apos;paperhouse&apos;: 386, &apos;sororities&apos;: 387, &apos;pym&apos;: 388, &apos;therin&apos;: 389, &apos;tarkovsky&apos;: 390, &apos;rdiger&apos;: 391, &apos;resembling&apos;: 392, &apos;gimmicks&apos;: 393, &apos;iler&apos;: 394, &apos;lineal&apos;: 395, &apos;taming&apos;: 396, &apos;mortenson&apos;: 397, &apos;waugh&apos;: 398, &apos;furies&apos;: 399, &apos;grufford&apos;: 400, &apos;hammill&apos;: 401, &apos;plunkett&apos;: 402, &apos;paterson&apos;: 403, &apos;konishita&apos;: 404, &apos;immorality&apos;: 405, &apos;angelos&apos;: 406, &apos;kebbel&apos;: 407, &apos;tamiroff&apos;: 408, &apos;boen&apos;: 409, &apos;rivalry&apos;: 410, &apos;ethnic&apos;: 411, &apos;funner&apos;: 412, &apos;troops&apos;: 413, &apos;deadeningly&apos;: 414, &apos;watcha&apos;: 415, &apos;dhiraj&apos;: 416, &apos;haranguing&apos;: 417, &apos;dejas&apos;: 418, &apos;weasely&apos;: 419, &apos;category&apos;: 420, &apos;laughable&apos;: 421, &apos;gramps&apos;: 422, &apos;safdar&apos;: 423, &apos;calorie&apos;: 424, &apos;scandi&apos;: 425, &apos;cannon&apos;: 426, &apos;maliciously&apos;: 427, &apos;bothered&apos;: 428, &apos;troi&apos;: 429, &apos;couleur&apos;: 430, &apos;visionary&apos;: 431, &apos;fizzles&apos;: 432, &apos;evangalizing&apos;: 433, &apos;reeves&apos;: 434, &apos;bombadier&apos;: 435, &apos;bowlegged&apos;: 436, &apos;custody&apos;: 437, &apos;weta&apos;: 438, &apos;archambault&apos;: 439, &apos;warlords&apos;: 440, &apos;makeout&apos;: 441, &apos;bonbons&apos;: 442, &apos;importances&apos;: 443, &apos;baruchel&apos;: 444, &apos;floyd&apos;: 445, &apos;infirm&apos;: 446, &apos;bloodwaters&apos;: 447, &apos;ashford&apos;: 448, &apos;colleagues&apos;: 449, &apos;discern&apos;: 450, &apos;thunderjet&apos;: 451, &apos;pullers&apos;: 452, &apos;evos&apos;: 453, &apos;celebrations&apos;: 454, &apos;seely&apos;: 455, &apos;nasty&apos;: 456, &apos;keach&apos;: 457, &apos;tonge&apos;: 458, &apos;senki&apos;: 459, &apos;approxiamtely&apos;: 460, &apos;unable&apos;: 461, &apos;rayburn&apos;: 462, &apos;britons&apos;: 463, &apos;christoph&apos;: 464, &apos;proctor&apos;: 465, &apos;tapped&apos;: 466, &apos;lenz&apos;: 467, &apos;vengeant&apos;: 468, &apos;exaggerating&apos;: 469, &apos;mle&apos;: 470, &apos;declaims&apos;: 471, &apos;hight&apos;: 472, &apos;repetoir&apos;: 473, &apos;yolu&apos;: 474, &apos;smarty&apos;: 475, &apos;steels&apos;: 476, &apos;openness&apos;: 477, &apos;coached&apos;: 478, &apos;archiving&apos;: 479, &apos;horrendous&apos;: 480, &apos;engages&apos;: 481, &apos;loosing&apos;: 482, &apos;anchorpoint&apos;: 483, &apos;fecal&apos;: 484, &apos;gracefully&apos;: 485, &apos;tapioca&apos;: 486, &apos;bizniss&apos;: 487, &apos;overhyped&apos;: 488, &apos;shortland&apos;: 489, &apos;cleansed&apos;: 490, &apos;negativity&apos;: 491, &apos;gushy&apos;: 492, &apos;mortitz&apos;: 493, &apos;stripper&apos;: 494, &apos;woke&apos;: 495, &apos;slayers&apos;: 496, &apos;uncensored&apos;: 497, &apos;textiles&apos;: 498, &apos;louda&apos;: 499, &apos;castrati&apos;: 500, &apos;altmanesque&apos;: 501, &apos;yes&apos;: 502, &apos;huntress&apos;: 503, &apos;urging&apos;: 504, &apos;tua&apos;: 505, &apos;sentient&apos;: 506, &apos;kellogg&apos;: 507, &apos;cheerful&apos;: 508, &apos;swanks&apos;: 509, &apos;shor&apos;: 510, &apos;cheapo&apos;: 511, &apos;flourishing&apos;: 512, &apos;tap&apos;: 513, &apos;kph&apos;: 514, &apos;bobbidi&apos;: 515, &apos;tangos&apos;: 516, &apos;honey&apos;: 517, &apos;oswald&apos;: 518, &apos;philippians&apos;: 519, &apos;payroll&apos;: 520, &apos;chooses&apos;: 521, &apos;archtypes&apos;: 522, &apos;generators&apos;: 523, &apos;grillo&apos;: 524, &apos;horrorible&apos;: 525, &apos;yellowing&apos;: 526, &apos;vancouver&apos;: 527, &apos;thet&apos;: 528, &apos;babtise&apos;: 529, &apos;participates&apos;: 530, &apos;uriah&apos;: 531, &apos;loust&apos;: 532, &apos;ravishingly&apos;: 533, &apos;punishing&apos;: 534, &apos;jhoom&apos;: 535, &apos;lulling&apos;: 536, &apos;stetting&apos;: 537, &apos;wierd&apos;: 538, &apos;truce&apos;: 539, &apos;peerce&apos;: 540, &apos;transpose&apos;: 541, &apos;unplanned&apos;: 542, &apos;unmistakeably&apos;: 543, &apos;approval&apos;: 544, &apos;amontillado&apos;: 545, &apos;een&apos;: 546, &apos;lefties&apos;: 547, &apos;tentatives&apos;: 548, &apos;mysteriousness&apos;: 549, &apos;mid&apos;: 550, &apos;technicians&apos;: 551, &apos;wich&apos;: 552, &apos;englund&apos;: 553, &apos;freespirited&apos;: 554, &apos;kun&apos;: 555, &apos;discourses&apos;: 556, &apos;nyily&apos;: 557, &apos;honorably&apos;: 558, &apos;hankerchief&apos;: 559, &apos;nugget&apos;: 560, &apos;nationalism&apos;: 561, &apos;reveals&apos;: 562, &apos;lamppost&apos;: 563, &apos;tempra&apos;: 564, &apos;sanctimoniousness&apos;: 565, &apos;wardrobes&apos;: 566, &apos;visa&apos;: 567, &apos;lenses&apos;: 568, &apos;johars&apos;: 569, &apos;prefers&apos;: 570, &apos;webster&apos;: 571, &apos;marcuzzo&apos;: 572, &apos;licensable&apos;: 573, &apos;brilliancy&apos;: 574, &apos;gumbas&apos;: 575, &apos;jacoby&apos;: 576, &apos;twine&apos;: 577, &apos;entices&apos;: 578, &apos;unpremeditated&apos;: 579, &apos;jin&apos;: 580, &apos;affirmatively&apos;: 581, &apos;joyful&apos;: 582, &apos;plotkurt&apos;: 583, &apos;danniele&apos;: 584, &apos;rpond&apos;: 585, &apos;flare&apos;: 586, &apos;lester&apos;: 587, &apos;toying&apos;: 588, &apos;having&apos;: 589, &apos;anorexia&apos;: 590, &apos;hoof&apos;: 591, &apos;stillman&apos;: 592, &apos;hows&apos;: 593, &apos;contrite&apos;: 594, &apos;hersholt&apos;: 595, &apos;utterance&apos;: 596, &apos;superflous&apos;: 597, &apos;orders&apos;: 598, &apos;pamelyn&apos;: 599, &apos;traumatized&apos;: 600, &apos;poder&apos;: 601, &apos;virtuality&apos;: 602, &apos;reaper&apos;: 603, &apos;trini&apos;: 604, &apos;phantasm&apos;: 605, &apos;fbp&apos;: 606, &apos;nuked&apos;: 607, &apos;siegfried&apos;: 608, &apos;ralph&apos;: 609, &apos;erwin&apos;: 610, &apos;rhymer&apos;: 611, &apos;christien&apos;: 612, &apos;sidekick&apos;: 613, &apos;grasshopper&apos;: 614, &apos;steryotypes&apos;: 615, &apos;donnagio&apos;: 616, &apos;denny&apos;: 617, &apos;fraudulent&apos;: 618, &apos;weisse&apos;: 619, &apos;yoji&apos;: 620, &apos;adapters&apos;: 621, &apos;andalthough&apos;: 622, &apos;fee&apos;: 623, &apos;attorney&apos;: 624, &apos;holliday&apos;: 625, &apos;prerequisite&apos;: 626, &apos;ives&apos;: 627, &apos;yvaine&apos;: 628, &apos;smaller&apos;: 629, &apos;satired&apos;: 630, &apos;ghillie&apos;: 631, &apos;hagelin&apos;: 632, &apos;upsurge&apos;: 633, &apos;empirical&apos;: 634, &apos;smap&apos;: 635, &apos;kirk&apos;: 636, &apos;conservatism&apos;: 637, &apos;wesley&apos;: 638, &apos;becuz&apos;: 639, &apos;fantasia&apos;: 640, &apos;treadstone&apos;: 641, &apos;berdalh&apos;: 642, &apos;reaganomics&apos;: 643, &apos;schwarzenberg&apos;: 644, &apos;housemann&apos;: 645, &apos;jumpstart&apos;: 646, &apos;glamorise&apos;: 647, &apos;braves&apos;: 648, &apos;simply&apos;: 649, &apos;which&apos;: 650, &apos;knifes&apos;: 651, &apos;ramblings&apos;: 652, &apos;bused&apos;: 653, &apos;lombardo&apos;: 654, &apos;refresher&apos;: 655, &apos;evenings&apos;: 656, &apos;openings&apos;: 657, &apos;rings&apos;: 658, &apos;reverend&apos;: 659, &apos;blurry&apos;: 660, &apos;baldy&apos;: 661, &apos;acing&apos;: 662, &apos;mollys&apos;: 663, &apos;meditteranean&apos;: 664, &apos;workday&apos;: 665, &apos;apologies&apos;: 666, &apos;empathise&apos;: 667, &apos;outs&apos;: 668, &apos;hmmmmmmmm&apos;: 669, &apos;enquiry&apos;: 670, &apos;detector&apos;: 671, &apos;copying&apos;: 672, &apos;outlive&apos;: 673, &apos;gangsta&apos;: 674, &apos;koyaanisqatsi&apos;: 675, &apos;entrenches&apos;: 676, &apos;author&apos;: 677, &apos;undistinguished&apos;: 678, &apos;izzard&apos;: 679, &apos;orgue&apos;: 680, &apos;negotiator&apos;: 681, &apos;behaviorally&apos;: 682, &apos;eyebrowed&apos;: 683, &apos;maximizes&apos;: 684, &apos;pilippinos&apos;: 685, &apos;recurred&apos;: 686, &apos;bullt&apos;: 687, &apos;infinnerty&apos;: 688, &apos;suspicious&apos;: 689, &apos;uncooked&apos;: 690, &apos;these&apos;: 691, &apos;ozaki&apos;: 692, &apos;sweden&apos;: 693, &apos;petition&apos;: 694, &apos;opium&apos;: 695, &apos;complacency&apos;: 696, &apos;deux&apos;: 697, &apos;kramer&apos;: 698, &apos;opt&apos;: 699, &apos;auras&apos;: 700, &apos;shyamalan&apos;: 701, &apos;lamore&apos;: 702, &apos;sunbathing&apos;: 703, &apos;toxins&apos;: 704, &apos;limned&apos;: 705, &apos;khali&apos;: 706, &apos;jefferey&apos;: 707, &apos;interviewee&apos;: 708, &apos;righted&apos;: 709, &apos;grandmammy&apos;: 710, &apos;wol&apos;: 711, &apos;verica&apos;: 712, &apos;footwork&apos;: 713, &apos;doug&apos;: 714, &apos;euthanasiarist&apos;: 715, &apos;repeating&apos;: 716, &apos;debutante&apos;: 717, &apos;trusts&apos;: 718, &apos;righto&apos;: 719, &apos;phyllida&apos;: 720, &apos;upa&apos;: 721, &apos;doogie&apos;: 722, &apos;gig&apos;: 723, &apos;violins&apos;: 724, &apos;ardor&apos;: 725, &apos;ould&apos;: 726, &apos;stymieing&apos;: 727, &apos;libs&apos;: 728, &apos;alejo&apos;: 729, &apos;sick&apos;: 730, &apos;propensities&apos;: 731, &apos;occasions&apos;: 732, &apos;spiderman&apos;: 733, &apos;limousines&apos;: 734, &apos;hearkening&apos;: 735, &apos;reinstated&apos;: 736, &apos;concede&apos;: 737, &apos;vineyard&apos;: 738, &apos;image&apos;: 739, &apos;waxed&apos;: 740, &apos;inuyasha&apos;: 741, &apos;paralyzed&apos;: 742, &apos;notches&apos;: 743, &apos;latifah&apos;: 744, &apos;mediation&apos;: 745, &apos;cozies&apos;: 746, &apos;spirit&apos;: 747, &apos;fathoms&apos;: 748, &apos;uecker&apos;: 749, &apos;hoochie&apos;: 750, &apos;akria&apos;: 751, &apos;praises&apos;: 752, &apos;wiring&apos;: 753, &apos;pastparticularly&apos;: 754, &apos;ghastliness&apos;: 755, &apos;artiness&apos;: 756, &apos;gruner&apos;: 757, &apos;admirals&apos;: 758, &apos;egger&apos;: 759, &apos;extract&apos;: 760, &apos;guiltlessly&apos;: 761, &apos;pie&apos;: 762, &apos;audaciousness&apos;: 763, &apos;stallonethat&apos;: 764, &apos;balconys&apos;: 765, &apos;cassi&apos;: 766, &apos;definable&apos;: 767, &apos;rote&apos;: 768, &apos;assaulted&apos;: 769, &apos;schmoeller&apos;: 770, &apos;cancer&apos;: 771, &apos;equality&apos;: 772, &apos;kruk&apos;: 773, &apos;whoah&apos;: 774, &apos;dalai&apos;: 775, &apos;tuareg&apos;: 776, &apos;split&apos;: 777, &apos;bollywood&apos;: 778, &apos;mates&apos;: 779, &apos;supports&apos;: 780, &apos;whiskers&apos;: 781, &apos;meres&apos;: 782, &apos;plasticine&apos;: 783, &apos;bartel&apos;: 784, &apos;phrase&apos;: 785, &apos;poldark&apos;: 786, &apos;pylon&apos;: 787, &apos;undefined&apos;: 788, &apos;videographer&apos;: 789, &apos;blithesome&apos;: 790, &apos;prendergast&apos;: 791, &apos;goddard&apos;: 792, &apos;spectular&apos;: 793, &apos;fof&apos;: 794, &apos;kiddie&apos;: 795, &apos;accelerating&apos;: 796, &apos;secreted&apos;: 797, &apos;manslaughter&apos;: 798, &apos;akimbo&apos;: 799, &apos;privacy&apos;: 800, &apos;michigan&apos;: 801, &apos;ambiguities&apos;: 802, &apos;belabors&apos;: 803, &apos;mol&apos;: 804, &apos;disemboweled&apos;: 805, &apos;creely&apos;: 806, &apos;nosebleed&apos;: 807, &apos;autobiography&apos;: 808, &apos;dispelled&apos;: 809, &apos;lancie&apos;: 810, &apos;revolutionaries&apos;: 811, &apos;allende&apos;: 812, &apos;jacy&apos;: 813, &apos;kostic&apos;: 814, &apos;tormei&apos;: 815, &apos;chiefly&apos;: 816, &apos;atmospheric&apos;: 817, &apos;europa&apos;: 818, &apos;judmila&apos;: 819, &apos;extremal&apos;: 820, &apos;decaprio&apos;: 821, &apos;amore&apos;: 822, &apos;cockneys&apos;: 823, &apos;chong&apos;: 824, &apos;coordinates&apos;: 825, &apos;ctomvelu&apos;: 826, &apos;scums&apos;: 827, &apos;valleyspeak&apos;: 828, &apos;minstrel&apos;: 829, &apos;shoddier&apos;: 830, &apos;combusted&apos;: 831, &apos;tirade&apos;: 832, &apos;marketplaces&apos;: 833, &apos;reflex&apos;: 834, &apos;rjt&apos;: 835, &apos;deckard&apos;: 836, &apos;godfathers&apos;: 837, &apos;sibling&apos;: 838, &apos;erupted&apos;: 839, &apos;wasnt&apos;: 840, &apos;lollipop&apos;: 841, &apos;narcotics&apos;: 842, &apos;showdowns&apos;: 843, &apos;excess&apos;: 844, &apos;taught&apos;: 845, &apos;persuade&apos;: 846, &apos;homer&apos;: 847, &apos;binysh&apos;: 848, &apos;ravaging&apos;: 849, &apos;minutest&apos;: 850, &apos;yomada&apos;: 851, &apos;leckie&apos;: 852, &apos;snazzy&apos;: 853, &apos;rafting&apos;: 854, &apos;grendelif&apos;: 855, &apos;nemeses&apos;: 856, &apos;westmore&apos;: 857, &apos;sty&apos;: 858, &apos;puertorricans&apos;: 859, &apos;zaara&apos;: 860, &apos;timemachine&apos;: 861, &apos;similarities&apos;: 862, &apos;colera&apos;: 863, &apos;firefall&apos;: 864, &apos;winked&apos;: 865, &apos;painkiller&apos;: 866, &apos;leaflets&apos;: 867, &apos;tehran&apos;: 868, &apos;hooker&apos;: 869, &apos;appalingly&apos;: 870, &apos;humility&apos;: 871, &apos;illegitimate&apos;: 872, &apos;coer&apos;: 873, &apos;responisible&apos;: 874, &apos;conceded&apos;: 875, &apos;scarves&apos;: 876, &apos;dawid&apos;: 877, &apos;overflows&apos;: 878, &apos;annuder&apos;: 879, &apos;nickelodean&apos;: 880, &apos;comanche&apos;: 881, &apos;betrail&apos;: 882, &apos;pillage&apos;: 883, &apos;daffy&apos;: 884, &apos;dobson&apos;: 885, &apos;tessier&apos;: 886, &apos;egoism&apos;: 887, &apos;meanie&apos;: 888, &apos;trancers&apos;: 889, &apos;sequences&apos;: 890, &apos;viciente&apos;: 891, &apos;redlich&apos;: 892, &apos;filmfrderung&apos;: 893, &apos;leveled&apos;: 894, &apos;performer&apos;: 895, &apos;opponent&apos;: 896, &apos;appears&apos;: 897, &apos;squeaks&apos;: 898, &apos;peripheral&apos;: 899, &apos;blimey&apos;: 900, &apos;glass&apos;: 901, &apos;captors&apos;: 902, &apos;strains&apos;: 903, &apos;codenamealexa&apos;: 904, &apos;tooo&apos;: 905, &apos;aiello&apos;: 906, &apos;matines&apos;: 907, &apos;calibre&apos;: 908, &apos;tighten&apos;: 909, &apos;papercuts&apos;: 910, &apos;necrotic&apos;: 911, &apos;hums&apos;: 912, &apos;kavner&apos;: 913, &apos;employers&apos;: 914, &apos;troy&apos;: 915, &apos;almerayeda&apos;: 916, &apos;barnet&apos;: 917, &apos;nicotero&apos;: 918, &apos;rush&apos;: 919, &apos;ahehehe&apos;: 920, &apos;dui&apos;: 921, &apos;bleeps&apos;: 922, &apos;heroe&apos;: 923, &apos;gangreen&apos;: 924, &apos;paintbrush&apos;: 925, &apos;dowager&apos;: 926, &apos;khakkee&apos;: 927, &apos;chariots&apos;: 928, &apos;benfer&apos;: 929, &apos;mcneely&apos;: 930, &apos;quelled&apos;: 931, &apos;blockheads&apos;: 932, &apos;dufy&apos;: 933, &apos;badmen&apos;: 934, &apos;dondaro&apos;: 935, &apos;nachoo&apos;: 936, &apos;intercedes&apos;: 937, &apos;looksand&apos;: 938, &apos;hasidic&apos;: 939, &apos;will&apos;: 940, &apos;practicable&apos;: 941, &apos;reading&apos;: 942, &apos;manufacture&apos;: 943, &apos;bao&apos;: 944, &apos;cigarette&apos;: 945, &apos;chomps&apos;: 946, &apos;subverting&apos;: 947, &apos;reichdeutch&apos;: 948, &apos;dexter&apos;: 949, &apos;hrishitta&apos;: 950, &apos;splitting&apos;: 951, &apos;uproarious&apos;: 952, &apos;ametuer&apos;: 953, &apos;speedway&apos;: 954, &apos;worser&apos;: 955, &apos;brisco&apos;: 956, &apos;stream&apos;: 957, &apos;etre&apos;: 958, &apos;lengths&apos;: 959, &apos;chimpnaut&apos;: 960, &apos;corny&apos;: 961, &apos;stirring&apos;: 962, &apos;tremendous&apos;: 963, &apos;tually&apos;: 964, &apos;mnage&apos;: 965, &apos;ashitaka&apos;: 966, &apos;crossbows&apos;: 967, &apos;hackery&apos;: 968, &apos;riker&apos;: 969, &apos;twelve&apos;: 970, &apos;freshner&apos;: 971, &apos;bobbie&apos;: 972, &apos;percussion&apos;: 973, &apos;overpopulation&apos;: 974, &apos;eeeekkk&apos;: 975, &apos;centaury&apos;: 976, &apos;summitting&apos;: 977, &apos;andbest&apos;: 978, &apos;pumping&apos;: 979, &apos;somnolent&apos;: 980, &apos;infatuation&apos;: 981, &apos;shakesphere&apos;: 982, &apos;ingred&apos;: 983, &apos;moon&apos;: 984, &apos;keven&apos;: 985, &apos;sanguisuga&apos;: 986, &apos;quivers&apos;: 987, &apos;equalling&apos;: 988, &apos;vaugely&apos;: 989, &apos;supervising&apos;: 990, &apos;dissolved&apos;: 991, &apos;cheshire&apos;: 992, &apos;retribution&apos;: 993, &apos;cartoons&apos;: 994, &apos;maisie&apos;: 995, &apos;reptiles&apos;: 996, &apos;rsther&apos;: 997, &apos;erratically&apos;: 998, &apos;hoyt&apos;: 999, ...} 12345678910def update_input_layer(review): global layer_0 # clear out previous state, reset the layer to be all 0s layer_0 *= 0 for word in review.split(\" \"): layer_0[0][word2index[word]] += 1update_input_layer(reviews[0]) 1layer_0 array([[ 18., 0., 0., ..., 0., 0., 0.]]) 12345def get_target_for_label(label): if(label == 'POSITIVE'): return 1 else: return 0 1labels[0] &apos;POSITIVE&apos; 1get_target_for_label(labels[0]) 1 1labels[1] &apos;NEGATIVE&apos; 1get_target_for_label(labels[1]) 0 Project 3: Building a Neural Network Start with your neural network from the last chapter 3 layer neural network no non-linearity in hidden layer use our functions to create the training data create a “pre_process_data” function to create vocabulary for our training data generating functions modify “train” to train over the entire corpus Where to Get Help if You Need it Re-watch previous week’s Udacity Lectures Chapters 3-5 - Grokking Deep Learning - (40% Off: traskud17) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161import timeimport sysimport numpy as np# Let's tweak our network from before to model these phenomenaclass SentimentNetwork: def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1): # set our random number generator np.random.seed(1) self.pre_process_data(reviews, labels) self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate) def pre_process_data(self, reviews, labels): review_vocab = set() for review in reviews: for word in review.split(\" \"): review_vocab.add(word) self.review_vocab = list(review_vocab) label_vocab = set() for label in labels: label_vocab.add(label) self.label_vocab = list(label_vocab) self.review_vocab_size = len(self.review_vocab) self.label_vocab_size = len(self.label_vocab) self.word2index = &#123;&#125; for i, word in enumerate(self.review_vocab): self.word2index[word] = i self.label2index = &#123;&#125; for i, label in enumerate(self.label_vocab): self.label2index[label] = i def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate): # Set number of nodes in input, hidden and output layers. self.input_nodes = input_nodes self.hidden_nodes = hidden_nodes self.output_nodes = output_nodes # Initialize weights self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes)) self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes)) self.learning_rate = learning_rate self.layer_0 = np.zeros((1,input_nodes)) def update_input_layer(self,review): # clear out previous state, reset the layer to be all 0s self.layer_0 *= 0 for word in review.split(\" \"): if(word in self.word2index.keys()): self.layer_0[0][self.word2index[word]] += 1 def get_target_for_label(self,label): if(label == 'POSITIVE'): return 1 else: return 0 def sigmoid(self,x): return 1 / (1 + np.exp(-x)) def sigmoid_output_2_derivative(self,output): return output * (1 - output) def train(self, training_reviews, training_labels): assert(len(training_reviews) == len(training_labels)) correct_so_far = 0 start = time.time() for i in range(len(training_reviews)): review = training_reviews[i] label = training_labels[i] #### Implement the forward pass here #### ### Forward pass ### # Input Layer self.update_input_layer(review) # Hidden layer layer_1 = self.layer_0.dot(self.weights_0_1) # Output layer layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2)) #### Implement the backward pass here #### ### Backward pass ### # TODO: Output error layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output. layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2) # TODO: Backpropagated error layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error # TODO: Update the weights self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step if(np.abs(layer_2_error) &lt; 0.5): correct_so_far += 1 reviews_per_second = i / float(time.time() - start) sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\") if(i % 2500 == 0): print(\"\") def test(self, testing_reviews, testing_labels): correct = 0 start = time.time() for i in range(len(testing_reviews)): pred = self.run(testing_reviews[i]) if(pred == testing_labels[i]): correct += 1 reviews_per_second = i / float(time.time() - start) sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\ + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\ + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\") def run(self, review): # Input Layer self.update_input_layer(review.lower()) # Hidden layer layer_1 = self.layer_0.dot(self.weights_0_1) # Output layer layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2)) if(layer_2[0] &gt; 0.5): return \"POSITIVE\" else: return \"NEGATIVE\" 1mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1) 12# evaluate our model before training (just to show how horrible it is)mlp.test(reviews[-1000:],labels[-1000:]) Progress:99.9% Speed(reviews/sec):1242.% #Correct:500 #Tested:1000 Testing Accuracy:50.0% 12# train the networkmlp.train(reviews[:-1000],labels[:-1000]) Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0% Progress:10.4% Speed(reviews/sec):167.7 #Correct:1250 #Trained:2501 Training Accuracy:49.9% Progress:20.8% Speed(reviews/sec):170.2 #Correct:2500 #Trained:5001 Training Accuracy:49.9% Progress:31.2% Speed(reviews/sec):169.9 #Correct:3750 #Trained:7501 Training Accuracy:49.9% Progress:41.6% Speed(reviews/sec):171.3 #Correct:5000 #Trained:10001 Training Accuracy:49.9% Progress:52.0% Speed(reviews/sec):170.0 #Correct:6250 #Trained:12501 Training Accuracy:49.9% Progress:62.5% Speed(reviews/sec):170.8 #Correct:7500 #Trained:15001 Training Accuracy:49.9% Progress:72.9% Speed(reviews/sec):171.4 #Correct:8750 #Trained:17501 Training Accuracy:49.9% Progress:83.3% Speed(reviews/sec):171.7 #Correct:10000 #Trained:20001 Training Accuracy:49.9% Progress:93.7% Speed(reviews/sec):172.6 #Correct:11250 #Trained:22501 Training Accuracy:49.9% Progress:99.9% Speed(reviews/sec):172.5 #Correct:11999 #Trained:24000 Training Accuracy:49.9% 1mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.01) 12# train the networkmlp.train(reviews[:-1000],labels[:-1000]) Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0% Progress:10.4% Speed(reviews/sec):149.0 #Correct:1247 #Trained:2501 Training Accuracy:49.8% Progress:20.8% Speed(reviews/sec):145.3 #Correct:2497 #Trained:5001 Training Accuracy:49.9% Progress:31.2% Speed(reviews/sec):144.0 #Correct:3747 #Trained:7501 Training Accuracy:49.9% Progress:41.6% Speed(reviews/sec):141.8 #Correct:4997 #Trained:10001 Training Accuracy:49.9% Progress:52.0% Speed(reviews/sec):137.0 #Correct:6247 #Trained:12501 Training Accuracy:49.9% Progress:62.5% Speed(reviews/sec):137.7 #Correct:7489 #Trained:15001 Training Accuracy:49.9% Progress:72.9% Speed(reviews/sec):137.1 #Correct:8740 #Trained:17501 Training Accuracy:49.9% Progress:83.3% Speed(reviews/sec):138.1 #Correct:9990 #Trained:20001 Training Accuracy:49.9% Progress:93.7% Speed(reviews/sec):138.9 #Correct:11240 #Trained:22501 Training Accuracy:49.9% Progress:99.9% Speed(reviews/sec):139.4 #Correct:11989 #Trained:24000 Training Accuracy:49.9% 1mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.001) 12# train the networkmlp.train(reviews[:-1000],labels[:-1000]) Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0% Progress:10.4% Speed(reviews/sec):147.5 #Correct:1267 #Trained:2501 Training Accuracy:50.6% Progress:20.8% Speed(reviews/sec):147.3 #Correct:2608 #Trained:5001 Training Accuracy:52.1% Progress:31.2% Speed(reviews/sec):147.3 #Correct:4021 #Trained:7501 Training Accuracy:53.6% Progress:41.6% Speed(reviews/sec):147.3 #Correct:5497 #Trained:10001 Training Accuracy:54.9% Progress:52.0% Speed(reviews/sec):147.3 #Correct:7071 #Trained:12501 Training Accuracy:56.5% Progress:62.5% Speed(reviews/sec):146.9 #Correct:8632 #Trained:15001 Training Accuracy:57.5% Progress:72.9% Speed(reviews/sec):146.9 #Correct:10228 #Trained:17501 Training Accuracy:58.4% Progress:83.3% Speed(reviews/sec):146.9 #Correct:11880 #Trained:20001 Training Accuracy:59.3% Progress:93.7% Speed(reviews/sec):147.0 #Correct:13580 #Trained:22501 Training Accuracy:60.3% Progress:99.9% Speed(reviews/sec):146.9 #Correct:14658 #Trained:24000 Training Accuracy:61.0% Understanding Neural Noise12from IPython.display import ImageImage(filename='sentiment_network.png') 12345678910def update_input_layer(review): global layer_0 # clear out previous state, reset the layer to be all 0s layer_0 *= 0 for word in review.split(\" \"): layer_0[0][word2index[word]] += 1update_input_layer(reviews[0]) 1layer_0 array([[ 18., 0., 0., ..., 0., 0., 0.]]) 1review_counter = Counter() 12for word in reviews[0].split(\" \"): review_counter[word] += 1 1review_counter.most_common() [(&apos;.&apos;, 27), (&apos;&apos;, 18), (&apos;the&apos;, 9), (&apos;to&apos;, 6), (&apos;high&apos;, 5), (&apos;i&apos;, 5), (&apos;bromwell&apos;, 4), (&apos;is&apos;, 4), (&apos;a&apos;, 4), (&apos;teachers&apos;, 4), (&apos;that&apos;, 4), (&apos;of&apos;, 4), (&apos;it&apos;, 2), (&apos;at&apos;, 2), (&apos;as&apos;, 2), (&apos;school&apos;, 2), (&apos;my&apos;, 2), (&apos;in&apos;, 2), (&apos;me&apos;, 2), (&apos;students&apos;, 2), (&apos;their&apos;, 2), (&apos;student&apos;, 2), (&apos;cartoon&apos;, 1), (&apos;comedy&apos;, 1), (&apos;ran&apos;, 1), (&apos;same&apos;, 1), (&apos;time&apos;, 1), (&apos;some&apos;, 1), (&apos;other&apos;, 1), (&apos;programs&apos;, 1), (&apos;about&apos;, 1), (&apos;life&apos;, 1), (&apos;such&apos;, 1), (&apos;years&apos;, 1), (&apos;teaching&apos;, 1), (&apos;profession&apos;, 1), (&apos;lead&apos;, 1), (&apos;believe&apos;, 1), (&apos;s&apos;, 1), (&apos;satire&apos;, 1), (&apos;much&apos;, 1), (&apos;closer&apos;, 1), (&apos;reality&apos;, 1), (&apos;than&apos;, 1), (&apos;scramble&apos;, 1), (&apos;survive&apos;, 1), (&apos;financially&apos;, 1), (&apos;insightful&apos;, 1), (&apos;who&apos;, 1), (&apos;can&apos;, 1), (&apos;see&apos;, 1), (&apos;right&apos;, 1), (&apos;through&apos;, 1), (&apos;pathetic&apos;, 1), (&apos;pomp&apos;, 1), (&apos;pettiness&apos;, 1), (&apos;whole&apos;, 1), (&apos;situation&apos;, 1), (&apos;all&apos;, 1), (&apos;remind&apos;, 1), (&apos;schools&apos;, 1), (&apos;knew&apos;, 1), (&apos;and&apos;, 1), (&apos;when&apos;, 1), (&apos;saw&apos;, 1), (&apos;episode&apos;, 1), (&apos;which&apos;, 1), (&apos;repeatedly&apos;, 1), (&apos;tried&apos;, 1), (&apos;burn&apos;, 1), (&apos;down&apos;, 1), (&apos;immediately&apos;, 1), (&apos;recalled&apos;, 1), (&apos;classic&apos;, 1), (&apos;line&apos;, 1), (&apos;inspector&apos;, 1), (&apos;m&apos;, 1), (&apos;here&apos;, 1), (&apos;sack&apos;, 1), (&apos;one&apos;, 1), (&apos;your&apos;, 1), (&apos;welcome&apos;, 1), (&apos;expect&apos;, 1), (&apos;many&apos;, 1), (&apos;adults&apos;, 1), (&apos;age&apos;, 1), (&apos;think&apos;, 1), (&apos;far&apos;, 1), (&apos;fetched&apos;, 1), (&apos;what&apos;, 1), (&apos;pity&apos;, 1), (&apos;isn&apos;, 1), (&apos;t&apos;, 1)] Project 4: Reducing Noise in our Input Data123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161import timeimport sysimport numpy as np# Let's tweak our network from before to model these phenomenaclass SentimentNetwork: def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1): # set our random number generator np.random.seed(1) self.pre_process_data(reviews, labels) self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate) def pre_process_data(self, reviews, labels): review_vocab = set() for review in reviews: for word in review.split(\" \"): review_vocab.add(word) self.review_vocab = list(review_vocab) label_vocab = set() for label in labels: label_vocab.add(label) self.label_vocab = list(label_vocab) self.review_vocab_size = len(self.review_vocab) self.label_vocab_size = len(self.label_vocab) self.word2index = &#123;&#125; for i, word in enumerate(self.review_vocab): self.word2index[word] = i self.label2index = &#123;&#125; for i, label in enumerate(self.label_vocab): self.label2index[label] = i def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate): # Set number of nodes in input, hidden and output layers. self.input_nodes = input_nodes self.hidden_nodes = hidden_nodes self.output_nodes = output_nodes # Initialize weights self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes)) self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes)) self.learning_rate = learning_rate self.layer_0 = np.zeros((1,input_nodes)) def update_input_layer(self,review): # clear out previous state, reset the layer to be all 0s self.layer_0 *= 0 for word in review.split(\" \"): if(word in self.word2index.keys()): self.layer_0[0][self.word2index[word]] = 1 def get_target_for_label(self,label): if(label == 'POSITIVE'): return 1 else: return 0 def sigmoid(self,x): return 1 / (1 + np.exp(-x)) def sigmoid_output_2_derivative(self,output): return output * (1 - output) def train(self, training_reviews, training_labels): assert(len(training_reviews) == len(training_labels)) correct_so_far = 0 start = time.time() for i in range(len(training_reviews)): review = training_reviews[i] label = training_labels[i] #### Implement the forward pass here #### ### Forward pass ### # Input Layer self.update_input_layer(review) # Hidden layer layer_1 = self.layer_0.dot(self.weights_0_1) # Output layer layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2)) #### Implement the backward pass here #### ### Backward pass ### # TODO: Output error layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output. layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2) # TODO: Backpropagated error layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error # TODO: Update the weights self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step if(np.abs(layer_2_error) &lt; 0.5): correct_so_far += 1 reviews_per_second = i / float(time.time() - start) sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\") if(i % 2500 == 0): print(\"\") def test(self, testing_reviews, testing_labels): correct = 0 start = time.time() for i in range(len(testing_reviews)): pred = self.run(testing_reviews[i]) if(pred == testing_labels[i]): correct += 1 reviews_per_second = i / float(time.time() - start) sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\ + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\ + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\") def run(self, review): # Input Layer self.update_input_layer(review.lower()) # Hidden layer layer_1 = self.layer_0.dot(self.weights_0_1) # Output layer layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2)) if(layer_2[0] &gt; 0.5): return \"POSITIVE\" else: return \"NEGATIVE\" 1mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1) 1mlp.train(reviews[:-1000],labels[:-1000]) Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0% Progress:10.4% Speed(reviews/sec):150.5 #Correct:1770 #Trained:2501 Training Accuracy:70.7% Progress:20.8% Speed(reviews/sec):150.5 #Correct:3719 #Trained:5001 Training Accuracy:74.3% Progress:31.2% Speed(reviews/sec):150.2 #Correct:5812 #Trained:7501 Training Accuracy:77.4% Progress:41.6% Speed(reviews/sec):150.3 #Correct:7932 #Trained:10001 Training Accuracy:79.3% Progress:52.0% Speed(reviews/sec):150.3 #Correct:10058 #Trained:12501 Training Accuracy:80.4% Progress:62.5% Speed(reviews/sec):150.2 #Correct:12192 #Trained:15001 Training Accuracy:81.2% Progress:72.9% Speed(reviews/sec):149.9 #Correct:14313 #Trained:17501 Training Accuracy:81.7% Progress:83.3% Speed(reviews/sec):149.9 #Correct:16486 #Trained:20001 Training Accuracy:82.4% Progress:93.7% Speed(reviews/sec):150.0 #Correct:18672 #Trained:22501 Training Accuracy:82.9% Progress:99.9% Speed(reviews/sec):150.1 #Correct:19999 #Trained:24000 Training Accuracy:83.3% 12# evaluate our model before training (just to show how horrible it is)mlp.test(reviews[-1000:],labels[-1000:]) Progress:99.9% Speed(reviews/sec):1621.% #Correct:858 #Tested:1000 Testing Accuracy:85.8% Analyzing Inefficiencies in our Network1Image(filename='sentiment_network_sparse.png') 1layer_0 = np.zeros(10) 1layer_0 array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 12layer_0[4] = 1layer_0[9] = 1 1layer_0 array([ 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.]) 1weights_0_1 = np.random.randn(10,5) 1layer_0.dot(weights_0_1) array([-0.10503756, 0.44222989, 0.24392938, -0.55961832, 0.21389503]) 1indices = [4,9] 1layer_1 = np.zeros(5) 12for index in indices: layer_1 += (weights_0_1[index]) 1layer_1 array([-0.10503756, 0.44222989, 0.24392938, -0.55961832, 0.21389503]) 1Image(filename='sentiment_network_sparse_2.png') Project 5: Making our Network More Efficient123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175import timeimport sys# Let's tweak our network from before to model these phenomenaclass SentimentNetwork: def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1): np.random.seed(1) self.pre_process_data(reviews) self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate) def pre_process_data(self,reviews): review_vocab = set() for review in reviews: for word in review.split(\" \"): review_vocab.add(word) self.review_vocab = list(review_vocab) label_vocab = set() for label in labels: label_vocab.add(label) self.label_vocab = list(label_vocab) self.review_vocab_size = len(self.review_vocab) self.label_vocab_size = len(self.label_vocab) self.word2index = &#123;&#125; for i, word in enumerate(self.review_vocab): self.word2index[word] = i self.label2index = &#123;&#125; for i, label in enumerate(self.label_vocab): self.label2index[label] = i def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate): # Set number of nodes in input, hidden and output layers. self.input_nodes = input_nodes self.hidden_nodes = hidden_nodes self.output_nodes = output_nodes # Initialize weights self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes)) self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes)) self.learning_rate = learning_rate self.layer_0 = np.zeros((1,input_nodes)) self.layer_1 = np.zeros((1,hidden_nodes)) def sigmoid(self,x): return 1 / (1 + np.exp(-x)) def sigmoid_output_2_derivative(self,output): return output * (1 - output) def update_input_layer(self,review): # clear out previous state, reset the layer to be all 0s self.layer_0 *= 0 for word in review.split(\" \"): self.layer_0[0][self.word2index[word]] = 1 def get_target_for_label(self,label): if(label == 'POSITIVE'): return 1 else: return 0 def train(self, training_reviews_raw, training_labels): training_reviews = list() for review in training_reviews_raw: indices = set() for word in review.split(\" \"): if(word in self.word2index.keys()): indices.add(self.word2index[word]) training_reviews.append(list(indices)) assert(len(training_reviews) == len(training_labels)) correct_so_far = 0 start = time.time() for i in range(len(training_reviews)): review = training_reviews[i] label = training_labels[i] #### Implement the forward pass here #### ### Forward pass ### # Input Layer # Hidden layer# layer_1 = self.layer_0.dot(self.weights_0_1) self.layer_1 *= 0 for index in review: self.layer_1 += self.weights_0_1[index] # Output layer layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2)) #### Implement the backward pass here #### ### Backward pass ### # Output error layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output. layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2) # Backpropagated error layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error # Update the weights self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step for index in review: self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate # update input-to-hidden weights with gradient descent step if(np.abs(layer_2_error) &lt; 0.5): correct_so_far += 1 reviews_per_second = i / float(time.time() - start) sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\") def test(self, testing_reviews, testing_labels): correct = 0 start = time.time() for i in range(len(testing_reviews)): pred = self.run(testing_reviews[i]) if(pred == testing_labels[i]): correct += 1 reviews_per_second = i / float(time.time() - start) sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\ + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\ + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\") def run(self, review): # Input Layer # Hidden layer self.layer_1 *= 0 unique_indices = set() for word in review.lower().split(\" \"): if word in self.word2index.keys(): unique_indices.add(self.word2index[word]) for index in unique_indices: self.layer_1 += self.weights_0_1[index] # Output layer layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2)) if(layer_2[0] &gt; 0.5): return \"POSITIVE\" else: return \"NEGATIVE\" 1mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1) 1mlp.train(reviews[:-1000],labels[:-1000]) Progress:99.9% Speed(reviews/sec):2564. #Correct:20113 #Trained:24000 Training Accuracy:83.8% 12# evaluate our model before training (just to show how horrible it is)mlp.test(reviews[-1000:],labels[-1000:]) Progress:99.9% Speed(reviews/sec):2995.% #Correct:853 #Tested:1000 Testing Accuracy:85.3% Further Noise Reduction1Image(filename='sentiment_network_sparse_2.png') 12# words most frequently seen in a review with a \"POSITIVE\" labelpos_neg_ratios.most_common() [(&apos;edie&apos;, 4.6913478822291435), (&apos;paulie&apos;, 4.0775374439057197), (&apos;felix&apos;, 3.1527360223636558), (&apos;polanski&apos;, 2.8233610476132043), (&apos;matthau&apos;, 2.8067217286092401), (&apos;victoria&apos;, 2.6810215287142909), (&apos;mildred&apos;, 2.6026896854443837), (&apos;gandhi&apos;, 2.5389738710582761), (&apos;flawless&apos;, 2.451005098112319), (&apos;superbly&apos;, 2.2600254785752498), (&apos;perfection&apos;, 2.1594842493533721), (&apos;astaire&apos;, 2.1400661634962708), (&apos;captures&apos;, 2.0386195471595809), (&apos;voight&apos;, 2.0301704926730531), (&apos;wonderfully&apos;, 2.0218960560332353), (&apos;powell&apos;, 1.9783454248084671), (&apos;brosnan&apos;, 1.9547990964725592), (&apos;lily&apos;, 1.9203768470501485), (&apos;bakshi&apos;, 1.9029851043382795), (&apos;lincoln&apos;, 1.9014583864844796), (&apos;refreshing&apos;, 1.8551812956655511), (&apos;breathtaking&apos;, 1.8481124057791867), (&apos;bourne&apos;, 1.8478489358790986), (&apos;lemmon&apos;, 1.8458266904983307), (&apos;delightful&apos;, 1.8002701588959635), (&apos;flynn&apos;, 1.7996646487351682), (&apos;andrews&apos;, 1.7764919970972666), (&apos;homer&apos;, 1.7692866133759964), (&apos;beautifully&apos;, 1.7626953362841438), (&apos;soccer&apos;, 1.7578579175523736), (&apos;elvira&apos;, 1.7397031072720019), (&apos;underrated&apos;, 1.7197859696029656), (&apos;gripping&apos;, 1.7165360479904674), (&apos;superb&apos;, 1.7091514458966952), (&apos;delight&apos;, 1.6714733033535532), (&apos;welles&apos;, 1.6677068205580761), (&apos;sadness&apos;, 1.663505133704376), (&apos;sinatra&apos;, 1.6389967146756448), (&apos;touching&apos;, 1.637217476541176), (&apos;timeless&apos;, 1.62924053973028), (&apos;macy&apos;, 1.6211339521972916), (&apos;unforgettable&apos;, 1.6177367152487956), (&apos;favorites&apos;, 1.6158688027643908), (&apos;stewart&apos;, 1.6119987332957739), (&apos;sullivan&apos;, 1.6094379124341003), (&apos;extraordinary&apos;, 1.6094379124341003), (&apos;hartley&apos;, 1.6094379124341003), (&apos;brilliantly&apos;, 1.5950491749820008), (&apos;friendship&apos;, 1.5677652160335325), (&apos;wonderful&apos;, 1.5645425925262093), (&apos;palma&apos;, 1.5553706911638245), (&apos;magnificent&apos;, 1.54663701119507), (&apos;finest&apos;, 1.5462590108125689), (&apos;jackie&apos;, 1.5439233053234738), (&apos;ritter&apos;, 1.5404450409471491), (&apos;tremendous&apos;, 1.5184661342283736), (&apos;freedom&apos;, 1.5091151908062312), (&apos;fantastic&apos;, 1.5048433868558566), (&apos;terrific&apos;, 1.5026699370083942), (&apos;noir&apos;, 1.493925025312256), (&apos;sidney&apos;, 1.493925025312256), (&apos;outstanding&apos;, 1.4910053152089213), (&apos;pleasantly&apos;, 1.4894785973551214), (&apos;mann&apos;, 1.4894785973551214), (&apos;nancy&apos;, 1.488077055429833), (&apos;marie&apos;, 1.4825711915553104), (&apos;marvelous&apos;, 1.4739999415389962), (&apos;excellent&apos;, 1.4647538505723599), (&apos;ruth&apos;, 1.4596256342054401), (&apos;stanwyck&apos;, 1.4412101187160054), (&apos;widmark&apos;, 1.4350845252893227), (&apos;splendid&apos;, 1.4271163556401458), (&apos;chan&apos;, 1.423108334242607), (&apos;exceptional&apos;, 1.4201959127955721), (&apos;tender&apos;, 1.410986973710262), (&apos;gentle&apos;, 1.4078005663408544), (&apos;poignant&apos;, 1.4022947024663317), (&apos;gem&apos;, 1.3932148039644643), (&apos;amazing&apos;, 1.3919815802404802), (&apos;chilling&apos;, 1.3862943611198906), (&apos;fisher&apos;, 1.3862943611198906), (&apos;davies&apos;, 1.3862943611198906), (&apos;captivating&apos;, 1.3862943611198906), (&apos;darker&apos;, 1.3652409519220583), (&apos;april&apos;, 1.3499267169490159), (&apos;kelly&apos;, 1.3461743673304654), (&apos;blake&apos;, 1.3418425985490567), (&apos;overlooked&apos;, 1.329135947279942), (&apos;ralph&apos;, 1.32818673031261), (&apos;bette&apos;, 1.3156767939059373), (&apos;hoffman&apos;, 1.3150668518315229), (&apos;cole&apos;, 1.3121863889661687), (&apos;shines&apos;, 1.3049487216659381), (&apos;powerful&apos;, 1.2999662776313934), (&apos;notch&apos;, 1.2950456896547455), (&apos;remarkable&apos;, 1.2883688239495823), (&apos;pitt&apos;, 1.286210902562908), (&apos;winters&apos;, 1.2833463918674481), (&apos;vivid&apos;, 1.2762934659055623), (&apos;gritty&apos;, 1.2757524867200667), (&apos;giallo&apos;, 1.2745029551317739), (&apos;portrait&apos;, 1.2704625455947689), (&apos;innocence&apos;, 1.2694300209805796), (&apos;psychiatrist&apos;, 1.2685113254635072), (&apos;favorite&apos;, 1.2668956297860055), (&apos;ensemble&apos;, 1.2656663733312759), (&apos;stunning&apos;, 1.2622417124499117), (&apos;burns&apos;, 1.259880436264232), (&apos;garbo&apos;, 1.258954938743289), (&apos;barbara&apos;, 1.2580400255962119), (&apos;philip&apos;, 1.2527629684953681), (&apos;panic&apos;, 1.2527629684953681), (&apos;holly&apos;, 1.2527629684953681), (&apos;carol&apos;, 1.2481440226390734), (&apos;perfect&apos;, 1.246742480713785), (&apos;appreciated&apos;, 1.2462482874741743), (&apos;favourite&apos;, 1.2411123512753928), (&apos;journey&apos;, 1.2367626271489269), (&apos;rural&apos;, 1.235471471385307), (&apos;bond&apos;, 1.2321436812926323), (&apos;builds&apos;, 1.2305398317106577), (&apos;brilliant&apos;, 1.2287554137664785), (&apos;brooklyn&apos;, 1.2286654169163074), (&apos;von&apos;, 1.225175011976539), (&apos;recommended&apos;, 1.2163953243244932), (&apos;unfolds&apos;, 1.2163953243244932), (&apos;daniel&apos;, 1.20215296760895), (&apos;perfectly&apos;, 1.1971931173405572), (&apos;crafted&apos;, 1.1962507582320256), (&apos;prince&apos;, 1.1939224684724346), (&apos;troubled&apos;, 1.192138346678933), (&apos;consequences&apos;, 1.1865810616140668), (&apos;haunting&apos;, 1.1814999484738773), (&apos;cinderella&apos;, 1.180052620608284), (&apos;alexander&apos;, 1.1759989522835299), (&apos;emotions&apos;, 1.1753049094563641), (&apos;boxing&apos;, 1.1735135968412274), (&apos;subtle&apos;, 1.1734135017508081), (&apos;curtis&apos;, 1.1649873576129823), (&apos;rare&apos;, 1.1566438362402944), (&apos;loved&apos;, 1.1563661500586044), (&apos;daughters&apos;, 1.1526795099383853), (&apos;courage&apos;, 1.1438688802562305), (&apos;dentist&apos;, 1.1426722784621401), (&apos;highly&apos;, 1.1420208631618658), (&apos;nominated&apos;, 1.1409146683587992), (&apos;tony&apos;, 1.1397491942285991), (&apos;draws&apos;, 1.1325138403437911), (&apos;everyday&apos;, 1.1306150197542835), (&apos;contrast&apos;, 1.1284652518177909), (&apos;cried&apos;, 1.1213405397456659), (&apos;fabulous&apos;, 1.1210851445201684), (&apos;ned&apos;, 1.120591195386885), (&apos;fay&apos;, 1.120591195386885), (&apos;emma&apos;, 1.1184149159642893), (&apos;sensitive&apos;, 1.113318436057805), (&apos;smooth&apos;, 1.1089750757036563), (&apos;dramas&apos;, 1.1080910326226534), (&apos;today&apos;, 1.1050431789984001), (&apos;helps&apos;, 1.1023091505494358), (&apos;inspiring&apos;, 1.0986122886681098), (&apos;jimmy&apos;, 1.0937696641923216), (&apos;awesome&apos;, 1.0931328229034842), (&apos;unique&apos;, 1.0881409888008142), (&apos;tragic&apos;, 1.0871835928444868), (&apos;intense&apos;, 1.0870514662670339), (&apos;stellar&apos;, 1.0857088838322018), (&apos;rival&apos;, 1.0822184788924332), (&apos;provides&apos;, 1.0797081340289569), (&apos;depression&apos;, 1.0782034170369026), (&apos;shy&apos;, 1.0775588794702773), (&apos;carrie&apos;, 1.076139432816051), (&apos;blend&apos;, 1.0753554265038423), (&apos;hank&apos;, 1.0736109864626924), (&apos;diana&apos;, 1.0726368022648489), (&apos;adorable&apos;, 1.0726368022648489), (&apos;unexpected&apos;, 1.0722255334949147), (&apos;achievement&apos;, 1.0668635903535293), (&apos;bettie&apos;, 1.0663514264498881), (&apos;happiness&apos;, 1.0632729222228008), (&apos;glorious&apos;, 1.0608719606852626), (&apos;davis&apos;, 1.0541605260972757), (&apos;terrifying&apos;, 1.0525211814678428), (&apos;beauty&apos;, 1.050410186850232), (&apos;ideal&apos;, 1.0479685558493548), (&apos;fears&apos;, 1.0467872208035236), (&apos;hong&apos;, 1.0438040521731147), (&apos;seasons&apos;, 1.0433496099930604), (&apos;fascinating&apos;, 1.0414538748281612), (&apos;carries&apos;, 1.0345904299031787), (&apos;satisfying&apos;, 1.0321225473992768), (&apos;definite&apos;, 1.0319209141694374), (&apos;touched&apos;, 1.0296194171811581), (&apos;greatest&apos;, 1.0248947127715422), (&apos;creates&apos;, 1.0241097613701886), (&apos;aunt&apos;, 1.023388867430522), (&apos;walter&apos;, 1.022328983918479), (&apos;spectacular&apos;, 1.0198314108149955), (&apos;portrayal&apos;, 1.0189810189761024), (&apos;ann&apos;, 1.0127808528183286), (&apos;enterprise&apos;, 1.0116009116784799), (&apos;musicals&apos;, 1.0096648026516135), (&apos;deeply&apos;, 1.0094845087721023), (&apos;incredible&apos;, 1.0061677561461084), (&apos;mature&apos;, 1.0060195018402847), (&apos;triumph&apos;, 0.99682959435816731), (&apos;margaret&apos;, 0.99682959435816731), (&apos;navy&apos;, 0.99493385919326827), (&apos;harry&apos;, 0.99176919305006062), (&apos;lucas&apos;, 0.990398704027877), (&apos;sweet&apos;, 0.98966110487955483), (&apos;joey&apos;, 0.98794672078059009), (&apos;oscar&apos;, 0.98721905111049713), (&apos;balance&apos;, 0.98649499054740353), (&apos;warm&apos;, 0.98485340331145166), (&apos;ages&apos;, 0.98449898190068863), (&apos;guilt&apos;, 0.98082925301172619), (&apos;glover&apos;, 0.98082925301172619), (&apos;carrey&apos;, 0.98082925301172619), (&apos;learns&apos;, 0.97881108885548895), (&apos;unusual&apos;, 0.97788374278196932), (&apos;sons&apos;, 0.97777581552483595), (&apos;complex&apos;, 0.97761897738147796), (&apos;essence&apos;, 0.97753435711487369), (&apos;brazil&apos;, 0.9769153536905899), (&apos;widow&apos;, 0.97650959186720987), (&apos;solid&apos;, 0.97537964824416146), (&apos;beautiful&apos;, 0.97326301262841053), (&apos;holmes&apos;, 0.97246100334120955), (&apos;awe&apos;, 0.97186058302896583), (&apos;vhs&apos;, 0.97116734209998934), (&apos;eerie&apos;, 0.97116734209998934), (&apos;lonely&apos;, 0.96873720724669754), (&apos;grim&apos;, 0.96873720724669754), (&apos;sport&apos;, 0.96825047080486615), (&apos;debut&apos;, 0.96508089604358704), (&apos;destiny&apos;, 0.96343751029985703), (&apos;thrillers&apos;, 0.96281074750904794), (&apos;tears&apos;, 0.95977584381389391), (&apos;rose&apos;, 0.95664202739772253), (&apos;feelings&apos;, 0.95551144502743635), (&apos;ginger&apos;, 0.95551144502743635), (&apos;winning&apos;, 0.95471810900804055), (&apos;stanley&apos;, 0.95387344302319799), (&apos;cox&apos;, 0.95343027882361187), (&apos;paris&apos;, 0.95278479030472663), (&apos;heart&apos;, 0.95238806924516806), (&apos;hooked&apos;, 0.95155887071161305), (&apos;comfortable&apos;, 0.94803943018873538), (&apos;mgm&apos;, 0.94446160884085151), (&apos;masterpiece&apos;, 0.94155039863339296), (&apos;themes&apos;, 0.94118828349588235), (&apos;danny&apos;, 0.93967118051821874), (&apos;anime&apos;, 0.93378388932167222), (&apos;perry&apos;, 0.93328830824272613), (&apos;joy&apos;, 0.93301752567946861), (&apos;lovable&apos;, 0.93081883243706487), (&apos;mysteries&apos;, 0.92953595862417571), (&apos;hal&apos;, 0.92953595862417571), (&apos;louis&apos;, 0.92871325187271225), (&apos;charming&apos;, 0.92520609553210742), (&apos;urban&apos;, 0.92367083917177761), (&apos;allows&apos;, 0.92183091224977043), (&apos;impact&apos;, 0.91815814604895041), (&apos;italy&apos;, 0.91629073187415511), (&apos;gradually&apos;, 0.91629073187415511), (&apos;lifestyle&apos;, 0.91629073187415511), (&apos;spy&apos;, 0.91289514287301687), (&apos;treat&apos;, 0.91193342650519937), (&apos;subsequent&apos;, 0.91056005716517008), (&apos;kennedy&apos;, 0.90981821736853763), (&apos;loving&apos;, 0.90967549275543591), (&apos;surprising&apos;, 0.90937028902958128), (&apos;quiet&apos;, 0.90648673177753425), (&apos;winter&apos;, 0.90624039602065365), (&apos;reveals&apos;, 0.90490540964902977), (&apos;raw&apos;, 0.90445627422715225), (&apos;funniest&apos;, 0.90078654533818991), (&apos;pleased&apos;, 0.89994159387262562), (&apos;norman&apos;, 0.89994159387262562), (&apos;thief&apos;, 0.89874642222324552), (&apos;season&apos;, 0.89827222637147675), (&apos;secrets&apos;, 0.89794159320595857), (&apos;colorful&apos;, 0.89705936994626756), (&apos;highest&apos;, 0.8967461358011849), (&apos;compelling&apos;, 0.89462923509297576), (&apos;danes&apos;, 0.89248008318043659), (&apos;castle&apos;, 0.88967708335606499), (&apos;kudos&apos;, 0.88889175768604067), (&apos;great&apos;, 0.88810470901464589), (&apos;baseball&apos;, 0.88730319500090271), (&apos;subtitles&apos;, 0.88730319500090271), (&apos;bleak&apos;, 0.88730319500090271), (&apos;winner&apos;, 0.88643776872447388), (&apos;tragedy&apos;, 0.88563699078315261), (&apos;todd&apos;, 0.88551907320740142), (&apos;nicely&apos;, 0.87924946019380601), (&apos;arthur&apos;, 0.87546873735389985), (&apos;essential&apos;, 0.87373111745535925), (&apos;gorgeous&apos;, 0.8731725250935497), (&apos;fonda&apos;, 0.87294029100054127), (&apos;eastwood&apos;, 0.87139541196626402), (&apos;focuses&apos;, 0.87082835779739776), (&apos;enjoyed&apos;, 0.87070195951624607), (&apos;natural&apos;, 0.86997924506912838), (&apos;intensity&apos;, 0.86835126958503595), (&apos;witty&apos;, 0.86824103423244681), (&apos;rob&apos;, 0.8642954367557748), (&apos;worlds&apos;, 0.86377269759070874), (&apos;health&apos;, 0.86113891179907498), (&apos;magical&apos;, 0.85953791528170564), (&apos;deeper&apos;, 0.85802182375017932), (&apos;lucy&apos;, 0.85618680780444956), (&apos;moving&apos;, 0.85566611005772031), (&apos;lovely&apos;, 0.85290640004681306), (&apos;purple&apos;, 0.8513711857748395), (&apos;memorable&apos;, 0.84801189112086062), (&apos;sings&apos;, 0.84729786038720367), (&apos;craig&apos;, 0.84342938360928321), (&apos;modesty&apos;, 0.84342938360928321), (&apos;relate&apos;, 0.84326559685926517), (&apos;episodes&apos;, 0.84223712084137292), (&apos;strong&apos;, 0.84167135777060931), (&apos;smith&apos;, 0.83959811108590054), (&apos;tear&apos;, 0.83704136022001441), (&apos;apartment&apos;, 0.83333115290549531), (&apos;princess&apos;, 0.83290912293510388), (&apos;disagree&apos;, 0.83290912293510388), (&apos;kung&apos;, 0.83173334384609199), (&apos;adventure&apos;, 0.83150561393278388), (&apos;columbo&apos;, 0.82667857318446791), (&apos;jake&apos;, 0.82667857318446791), (&apos;adds&apos;, 0.82485652591452319), (&apos;hart&apos;, 0.82472353834866463), (&apos;strength&apos;, 0.82417544296634937), (&apos;realizes&apos;, 0.82360006895738058), (&apos;dave&apos;, 0.8232003088081431), (&apos;childhood&apos;, 0.82208086393583857), (&apos;forbidden&apos;, 0.81989888619908913), (&apos;tight&apos;, 0.81883539572344199), (&apos;surreal&apos;, 0.8178506590609026), (&apos;manager&apos;, 0.81770990320170756), (&apos;dancer&apos;, 0.81574950265227764), (&apos;studios&apos;, 0.81093021621632877), (&apos;con&apos;, 0.81093021621632877), (&apos;miike&apos;, 0.80821651034473263), (&apos;realistic&apos;, 0.80807714723392232), (&apos;explicit&apos;, 0.80792269515237358), (&apos;kurt&apos;, 0.8060875917405409), (&apos;traditional&apos;, 0.80535917116687328), (&apos;deals&apos;, 0.80535917116687328), (&apos;holds&apos;, 0.80493858654806194), (&apos;carl&apos;, 0.80437281567016972), (&apos;touches&apos;, 0.80396154690023547), (&apos;gene&apos;, 0.80314807577427383), (&apos;albert&apos;, 0.8027669055771679), (&apos;abc&apos;, 0.80234647252493729), (&apos;cry&apos;, 0.80011930011211307), (&apos;sides&apos;, 0.7995275841185171), (&apos;develops&apos;, 0.79850769621777162), (&apos;eyre&apos;, 0.79850769621777162), (&apos;dances&apos;, 0.79694397424158891), (&apos;oscars&apos;, 0.79633141679517616), (&apos;legendary&apos;, 0.79600456599965308), (&apos;hearted&apos;, 0.79492987486988764), (&apos;importance&apos;, 0.79492987486988764), (&apos;portraying&apos;, 0.79356592830699269), (&apos;impressed&apos;, 0.79258107754813223), (&apos;waters&apos;, 0.79112758892014912), (&apos;empire&apos;, 0.79078565012386137), (&apos;edge&apos;, 0.789774016249017), (&apos;jean&apos;, 0.78845736036427028), (&apos;environment&apos;, 0.78845736036427028), (&apos;sentimental&apos;, 0.7864791203521645), (&apos;captured&apos;, 0.78623760362595729), (&apos;styles&apos;, 0.78592891401091158), (&apos;daring&apos;, 0.78592891401091158), (&apos;frank&apos;, 0.78275933924963248), (&apos;tense&apos;, 0.78275933924963248), (&apos;backgrounds&apos;, 0.78275933924963248), (&apos;matches&apos;, 0.78275933924963248), (&apos;gothic&apos;, 0.78209466657644144), (&apos;sharp&apos;, 0.7814397877056235), (&apos;achieved&apos;, 0.78015855754957497), (&apos;court&apos;, 0.77947526404844247), (&apos;steals&apos;, 0.7789140023173704), (&apos;rules&apos;, 0.77844476107184035), (&apos;colors&apos;, 0.77684619943659217), (&apos;reunion&apos;, 0.77318988823348167), (&apos;covers&apos;, 0.77139937745969345), (&apos;tale&apos;, 0.77010822169607374), (&apos;rain&apos;, 0.7683706017975328), (&apos;denzel&apos;, 0.76804848873306297), (&apos;stays&apos;, 0.76787072675588186), (&apos;blob&apos;, 0.76725515271366718), (&apos;maria&apos;, 0.76214005204689672), (&apos;conventional&apos;, 0.76214005204689672), (&apos;fresh&apos;, 0.76158434211317383), (&apos;midnight&apos;, 0.76096977689870637), (&apos;landscape&apos;, 0.75852993982279704), (&apos;animated&apos;, 0.75768570169751648), (&apos;titanic&apos;, 0.75666058628227129), (&apos;sunday&apos;, 0.75666058628227129), (&apos;spring&apos;, 0.7537718023763802), (&apos;cagney&apos;, 0.7537718023763802), (&apos;enjoyable&apos;, 0.75246375771636476), (&apos;immensely&apos;, 0.75198768058287868), (&apos;sir&apos;, 0.7507762933965817), (&apos;nevertheless&apos;, 0.75067102469813185), (&apos;driven&apos;, 0.74994477895307854), (&apos;performances&apos;, 0.74883252516063137), (&apos;memories&apos;, 0.74721440183022114), (&apos;nowadays&apos;, 0.74721440183022114), (&apos;simple&apos;, 0.74641420974143258), (&apos;golden&apos;, 0.74533293373051557), (&apos;leslie&apos;, 0.74533293373051557), (&apos;lovers&apos;, 0.74497224842453125), (&apos;relationship&apos;, 0.74484232345601786), (&apos;supporting&apos;, 0.74357803418683721), (&apos;che&apos;, 0.74262723782331497), (&apos;packed&apos;, 0.7410032017375805), (&apos;trek&apos;, 0.74021469141793106), (&apos;provoking&apos;, 0.73840377214806618), (&apos;strikes&apos;, 0.73759894313077912), (&apos;depiction&apos;, 0.73682224406260699), (&apos;emotional&apos;, 0.73678211645681524), (&apos;secretary&apos;, 0.7366322924996842), (&apos;influenced&apos;, 0.73511137965897755), (&apos;florida&apos;, 0.73511137965897755), (&apos;germany&apos;, 0.73288750920945944), (&apos;brings&apos;, 0.73142936713096229), (&apos;lewis&apos;, 0.73129894652432159), (&apos;elderly&apos;, 0.73088750854279239), (&apos;owner&apos;, 0.72743625403857748), (&apos;streets&apos;, 0.72666987259858895), (&apos;henry&apos;, 0.72642196944481741), (&apos;portrays&apos;, 0.72593700338293632), (&apos;bears&apos;, 0.7252354951114458), (&apos;china&apos;, 0.72489587887452556), (&apos;anger&apos;, 0.72439972406404984), (&apos;society&apos;, 0.72433010799663333), (&apos;available&apos;, 0.72415741730250549), (&apos;best&apos;, 0.72347034060446314), (&apos;bugs&apos;, 0.72270598280148979), (&apos;magic&apos;, 0.71878961117328299), (&apos;delivers&apos;, 0.71846498854423513), (&apos;verhoeven&apos;, 0.71846498854423513), (&apos;jim&apos;, 0.71783979315031676), (&apos;donald&apos;, 0.71667767797013937), (&apos;endearing&apos;, 0.71465338578090898), (&apos;relationships&apos;, 0.71393795022901896), (&apos;greatly&apos;, 0.71256526641704687), (&apos;charlie&apos;, 0.71024161391924534), (&apos;brad&apos;, 0.71024161391924534), (&apos;simon&apos;, 0.70967648251115578), (&apos;effectively&apos;, 0.70914752190638641), (&apos;march&apos;, 0.70774597998109789), (&apos;atmosphere&apos;, 0.70744773070214162), (&apos;influence&apos;, 0.70733181555190172), (&apos;genius&apos;, 0.706392407309966), (&apos;emotionally&apos;, 0.70556970055850243), (&apos;ken&apos;, 0.70526854109229009), (&apos;identity&apos;, 0.70484322032313651), (&apos;sophisticated&apos;, 0.70470800296102132), (&apos;dan&apos;, 0.70457587638356811), (&apos;andrew&apos;, 0.70329955202396321), (&apos;india&apos;, 0.70144598337464037), (&apos;roy&apos;, 0.69970458110610434), (&apos;surprisingly&apos;, 0.6995780708902356), (&apos;sky&apos;, 0.69780919366575667), (&apos;romantic&apos;, 0.69664981111114743), (&apos;match&apos;, 0.69566924999265523), (&apos;meets&apos;, 0.69314718055994529), (&apos;cowboy&apos;, 0.69314718055994529), (&apos;wave&apos;, 0.69314718055994529), (&apos;bitter&apos;, 0.69314718055994529), (&apos;patient&apos;, 0.69314718055994529), (&apos;stylish&apos;, 0.69314718055994529), (&apos;britain&apos;, 0.69314718055994529), (&apos;affected&apos;, 0.69314718055994529), (&apos;beatty&apos;, 0.69314718055994529), (&apos;love&apos;, 0.69198533541937324), (&apos;paul&apos;, 0.68980827929443067), (&apos;andy&apos;, 0.68846333124751902), (&apos;performance&apos;, 0.68797386327972465), (&apos;patrick&apos;, 0.68645819240914863), (&apos;unlike&apos;, 0.68546468438792907), (&apos;brooks&apos;, 0.68433655087779044), (&apos;refuses&apos;, 0.68348526964820844), (&apos;award&apos;, 0.6824518914431974), (&apos;complaint&apos;, 0.6824518914431974), (&apos;ride&apos;, 0.68229716453587952), (&apos;dawson&apos;, 0.68171848473632257), (&apos;luke&apos;, 0.68158635815886937), (&apos;wells&apos;, 0.68087708796813096), (&apos;france&apos;, 0.6804081547825156), (&apos;sports&apos;, 0.68007509899259255), (&apos;handsome&apos;, 0.68007509899259255), (&apos;directs&apos;, 0.67875844310784572), (&apos;rebel&apos;, 0.67875844310784572), (&apos;greater&apos;, 0.67605274720064523), (&apos;dreams&apos;, 0.67599410133369586), (&apos;effective&apos;, 0.67565402311242806), (&apos;interpretation&apos;, 0.67479804189174875), (&apos;works&apos;, 0.67445504754779284), (&apos;brando&apos;, 0.67445504754779284), (&apos;noble&apos;, 0.6737290947028437), (&apos;paced&apos;, 0.67314651385327573), (&apos;le&apos;, 0.67067432470788668), (&apos;master&apos;, 0.67015766233524654), (&apos;h&apos;, 0.6696166831497512), (&apos;rings&apos;, 0.66904962898088483), (&apos;easy&apos;, 0.66895995494594152), (&apos;city&apos;, 0.66820823221269321), (&apos;sunshine&apos;, 0.66782937257565544), (&apos;succeeds&apos;, 0.66647893347778397), (&apos;relations&apos;, 0.664159643686693), (&apos;england&apos;, 0.66387679825983203), (&apos;glimpse&apos;, 0.66329421741026418), (&apos;aired&apos;, 0.66268797307523675), (&apos;sees&apos;, 0.66263163663399482), (&apos;both&apos;, 0.66248336767382998), (&apos;definitely&apos;, 0.66199789483898808), (&apos;imaginative&apos;, 0.66139848224536502), (&apos;appreciate&apos;, 0.66083893732728749), (&apos;tricks&apos;, 0.66071190480679143), (&apos;striking&apos;, 0.66071190480679143), (&apos;carefully&apos;, 0.65999497324304479), (&apos;complicated&apos;, 0.65981076029235353), (&apos;perspective&apos;, 0.65962448852130173), (&apos;trilogy&apos;, 0.65877953705573755), (&apos;future&apos;, 0.65834665141052828), (&apos;lion&apos;, 0.65742909795786608), (&apos;douglas&apos;, 0.65540685257709819), (&apos;victor&apos;, 0.65540685257709819), (&apos;inspired&apos;, 0.65459851044271034), (&apos;marriage&apos;, 0.65392646740666405), (&apos;demands&apos;, 0.65392646740666405), (&apos;father&apos;, 0.65172321672194655), (&apos;page&apos;, 0.65123628494430852), (&apos;instant&apos;, 0.65058756614114943), (&apos;era&apos;, 0.6495567444850836), (&apos;ruthless&apos;, 0.64934455790155243), (&apos;saga&apos;, 0.64934455790155243), (&apos;joan&apos;, 0.64891392558311978), (&apos;joseph&apos;, 0.64841128671855386), (&apos;workers&apos;, 0.64829661439459352), (&apos;fantasy&apos;, 0.64726757480925168), (&apos;distant&apos;, 0.64551913157069074), (&apos;accomplished&apos;, 0.64551913157069074), (&apos;manhattan&apos;, 0.64435701639051324), (&apos;personal&apos;, 0.64355023942057321), (&apos;meeting&apos;, 0.64313675998528386), (&apos;individual&apos;, 0.64313675998528386), (&apos;pushing&apos;, 0.64313675998528386), (&apos;pleasant&apos;, 0.64250344774119039), (&apos;brave&apos;, 0.64185388617239469), (&apos;william&apos;, 0.64083139119578469), (&apos;hudson&apos;, 0.64077919504262937), (&apos;friendly&apos;, 0.63949446706762514), (&apos;eccentric&apos;, 0.63907995928966954), (&apos;awards&apos;, 0.63875310849414646), (&apos;jack&apos;, 0.63838309514997038), (&apos;seeking&apos;, 0.63808740337691783), (&apos;divorce&apos;, 0.63757732940513456), (&apos;colonel&apos;, 0.63757732940513456), (&apos;jane&apos;, 0.63443957973316734), (&apos;keeping&apos;, 0.63414883979798953), (&apos;gives&apos;, 0.63383568159497883), (&apos;ted&apos;, 0.63342794585832296), (&apos;animation&apos;, 0.63208692379869902), (&apos;progress&apos;, 0.6317782341836532), (&apos;larger&apos;, 0.63127177684185776), (&apos;concert&apos;, 0.63127177684185776), (&apos;nation&apos;, 0.6296337748376194), (&apos;albeit&apos;, 0.62739580299716491), (&apos;adapted&apos;, 0.62613647027698516), (&apos;discovers&apos;, 0.62542900650499444), (&apos;classic&apos;, 0.62504956428050518), (&apos;segment&apos;, 0.62335141862440335), (&apos;morgan&apos;, 0.62303761437291871), (&apos;mouse&apos;, 0.62294292188669675), (&apos;impressive&apos;, 0.62211140744319349), (&apos;artist&apos;, 0.62168821657780038), (&apos;ultimate&apos;, 0.62168821657780038), (&apos;griffith&apos;, 0.62117368093485603), (&apos;drew&apos;, 0.62082651898031915), (&apos;emily&apos;, 0.62082651898031915), (&apos;moved&apos;, 0.6197197120051281), (&apos;families&apos;, 0.61903920840622351), (&apos;profound&apos;, 0.61903920840622351), (&apos;innocent&apos;, 0.61851219917136446), (&apos;versions&apos;, 0.61730910416844087), (&apos;eddie&apos;, 0.61691981517206107), (&apos;criticism&apos;, 0.61651395453902935), (&apos;nature&apos;, 0.61594514653194088), (&apos;recognized&apos;, 0.61518563909023349), (&apos;sexuality&apos;, 0.61467556511845012), (&apos;contract&apos;, 0.61400986000122149), (&apos;brian&apos;, 0.61344043794920278), (&apos;remembered&apos;, 0.6131044728864089), (&apos;determined&apos;, 0.6123858239154869), (&apos;offers&apos;, 0.61207935747116349), (&apos;pleasure&apos;, 0.61195702582993206), (&apos;washington&apos;, 0.61180154110599294), (&apos;images&apos;, 0.61159731359583758), (&apos;games&apos;, 0.61067095873570676), (&apos;academy&apos;, 0.60872983874736208), (&apos;fashioned&apos;, 0.60798937221963845), (&apos;melodrama&apos;, 0.60749173598145145), (&apos;rough&apos;, 0.60613580357031549), (&apos;charismatic&apos;, 0.60613580357031549), (&apos;peoples&apos;, 0.60613580357031549), (&apos;dealing&apos;, 0.60517840761398811), (&apos;fine&apos;, 0.60496962268013299), (&apos;tap&apos;, 0.60391604683200273), (&apos;trio&apos;, 0.60157998703445481), (&apos;russell&apos;, 0.60120968523425966), (&apos;figures&apos;, 0.60077386042893011), (&apos;ward&apos;, 0.60005675749393339), (&apos;shine&apos;, 0.59911823091166894), (&apos;brady&apos;, 0.59911823091166894), (&apos;job&apos;, 0.59845562125168661), (&apos;satisfied&apos;, 0.59652034487087369), (&apos;river&apos;, 0.59637962862495086), (&apos;brown&apos;, 0.595773016534769), (&apos;believable&apos;, 0.59566072133302495), (&apos;always&apos;, 0.59470710774669278), (&apos;bound&apos;, 0.59470710774669278), (&apos;hall&apos;, 0.5933967777928858), (&apos;cook&apos;, 0.5916777203950857), (&apos;claire&apos;, 0.59136448625000293), (&apos;broadway&apos;, 0.59033768669372433), (&apos;anna&apos;, 0.58778666490211906), (&apos;peace&apos;, 0.58628403501758408), (&apos;visually&apos;, 0.58539431926349916), (&apos;morality&apos;, 0.58525821854876026), (&apos;falk&apos;, 0.58525821854876026), (&apos;growing&apos;, 0.58466653756587539), (&apos;experiences&apos;, 0.58314628534561685), (&apos;stood&apos;, 0.58314628534561685), (&apos;touch&apos;, 0.58122926435596001), (&apos;lives&apos;, 0.5810976767513224), (&apos;kubrick&apos;, 0.58066919713325493), (&apos;timing&apos;, 0.58047401805583243), (&apos;expressions&apos;, 0.57981849525294216), (&apos;struggles&apos;, 0.57981849525294216), (&apos;authentic&apos;, 0.57848427223980559), (&apos;helen&apos;, 0.57763429343810091), (&apos;pre&apos;, 0.57700753064729182), (&apos;quirky&apos;, 0.5753641449035618), (&apos;young&apos;, 0.57531672344534313), (&apos;inner&apos;, 0.57454143815209846), (&apos;mexico&apos;, 0.57443087372056334), (&apos;clint&apos;, 0.57380042292737909), (&apos;sisters&apos;, 0.57286101468544337), (&apos;realism&apos;, 0.57226528899949558), (&apos;french&apos;, 0.5720692490067093), (&apos;personalities&apos;, 0.5720692490067093), (&apos;surprises&apos;, 0.57113222999698177), (&apos;adventures&apos;, 0.57113222999698177), (&apos;overcome&apos;, 0.5697681593994407), (&apos;timothy&apos;, 0.56953322459276867), (&apos;tales&apos;, 0.56909453188996639), (&apos;war&apos;, 0.56843317302781682), (&apos;civil&apos;, 0.5679840376059393), (&apos;countries&apos;, 0.56737779327091187), (&apos;streep&apos;, 0.56710645966458029), (&apos;tradition&apos;, 0.56685345523565323), (&apos;oliver&apos;, 0.56673325570428668), (&apos;australia&apos;, 0.56580775818334383), (&apos;understanding&apos;, 0.56531380905006046), (&apos;players&apos;, 0.56509525370004821), (&apos;knowing&apos;, 0.56489284503626647), (&apos;rogers&apos;, 0.56421349718405212), (&apos;suspenseful&apos;, 0.56368911332305849), (&apos;variety&apos;, 0.56368911332305849), (&apos;true&apos;, 0.56281525180810066), (&apos;jr&apos;, 0.56220982311246936), (&apos;psychological&apos;, 0.56108745854687891), (&apos;sent&apos;, 0.55961578793542266), (&apos;grand&apos;, 0.55961578793542266), (&apos;branagh&apos;, 0.55961578793542266), (&apos;reminiscent&apos;, 0.55961578793542266), (&apos;performing&apos;, 0.55961578793542266), (&apos;wealth&apos;, 0.55961578793542266), (&apos;overwhelming&apos;, 0.55961578793542266), (&apos;odds&apos;, 0.55961578793542266), (&apos;brothers&apos;, 0.55891181043362848), (&apos;howard&apos;, 0.55811089675600245), (&apos;david&apos;, 0.55693122256475369), (&apos;generation&apos;, 0.55628799784274796), (&apos;grow&apos;, 0.55612538299565417), (&apos;survival&apos;, 0.55594605904646033), (&apos;mainstream&apos;, 0.55574731115750231), (&apos;dick&apos;, 0.55431073570572953), (&apos;charm&apos;, 0.55288175575407861), (&apos;kirk&apos;, 0.55278982286502287), (&apos;twists&apos;, 0.55244729845681018), (&apos;gangster&apos;, 0.55206858230003986), (&apos;jeff&apos;, 0.55179306225421365), (&apos;family&apos;, 0.55116244510065526), (&apos;tend&apos;, 0.55053307336110335), (&apos;thanks&apos;, 0.55049088015842218), (&apos;world&apos;, 0.54744234723432639), (&apos;sutherland&apos;, 0.54743536937855164), (&apos;life&apos;, 0.54695514434959924), (&apos;disc&apos;, 0.54654370636806993), (&apos;bug&apos;, 0.54654370636806993), (&apos;tribute&apos;, 0.5455111817538808), (&apos;europe&apos;, 0.54522705048332309), (&apos;sacrifice&apos;, 0.54430155296238014), (&apos;color&apos;, 0.54405127139431109), (&apos;superior&apos;, 0.54333490233128523), (&apos;york&apos;, 0.54318235866536513), (&apos;pulls&apos;, 0.54266622962164945), (&apos;jackson&apos;, 0.54232429082536171), (&apos;hearts&apos;, 0.54232429082536171), (&apos;enjoy&apos;, 0.54124285135906114), (&apos;redemption&apos;, 0.54056759296472823), (&apos;madness&apos;, 0.540384426007535), (&apos;stands&apos;, 0.5389965007326869), (&apos;trial&apos;, 0.5389965007326869), (&apos;greek&apos;, 0.5389965007326869), (&apos;hamilton&apos;, 0.5389965007326869), (&apos;each&apos;, 0.5388212312554177), (&apos;faithful&apos;, 0.53773307668591508), (&apos;received&apos;, 0.5372768098531604), (&apos;documentaries&apos;, 0.53714293208336406), (&apos;jealous&apos;, 0.53714293208336406), (&apos;different&apos;, 0.53709860682460819), (&apos;describes&apos;, 0.53680111016925136), (&apos;shorts&apos;, 0.53596159703753288), (&apos;brilliance&apos;, 0.53551823635636209), (&apos;mountains&apos;, 0.53492317534505118), (&apos;share&apos;, 0.53408248593025787), (&apos;dealt&apos;, 0.53408248593025787), (&apos;providing&apos;, 0.53329847961804933), (&apos;explore&apos;, 0.53329847961804933), (&apos;series&apos;, 0.5325809226575603), (&apos;fellow&apos;, 0.5323318289869543), (&apos;loves&apos;, 0.53062825106217038), (&apos;revolution&apos;, 0.53062825106217038), (&apos;olivier&apos;, 0.53062825106217038), (&apos;roman&apos;, 0.53062825106217038), (&apos;century&apos;, 0.53002783074992665), (&apos;musical&apos;, 0.52966871156747064), (&apos;heroic&apos;, 0.52925932545482868), (&apos;approach&apos;, 0.52806743020049673), (&apos;ironically&apos;, 0.52806743020049673), (&apos;temple&apos;, 0.52806743020049673), (&apos;moves&apos;, 0.5279372642387119), (&apos;gift&apos;, 0.52702030968597136), (&apos;julie&apos;, 0.52609309589677911), (&apos;tells&apos;, 0.52415107836314001), (&apos;radio&apos;, 0.52394671172868779), (&apos;uncle&apos;, 0.52354439617376536), (&apos;union&apos;, 0.52324814376454787), (&apos;deep&apos;, 0.52309571635780505), (&apos;reminds&apos;, 0.52157841554225237), (&apos;famous&apos;, 0.52118841080153722), (&apos;jazz&apos;, 0.52053443789295151), (&apos;dennis&apos;, 0.51987545928590861), (&apos;epic&apos;, 0.51919387343650736), (&apos;adult&apos;, 0.519167695083386), (&apos;shows&apos;, 0.51915322220375304), (&apos;performed&apos;, 0.5191244265806858), (&apos;demons&apos;, 0.5191244265806858), (&apos;discovered&apos;, 0.51879379341516751), (&apos;eric&apos;, 0.51879379341516751), (&apos;youth&apos;, 0.5185626062681431), (&apos;human&apos;, 0.51851411224987087), (&apos;tarzan&apos;, 0.51813827061227724), (&apos;ourselves&apos;, 0.51794309153485463), (&apos;wwii&apos;, 0.51758240622887042), (&apos;passion&apos;, 0.5162164724008671), (&apos;desire&apos;, 0.51607497965213445), (&apos;pays&apos;, 0.51581316527702981), (&apos;dirty&apos;, 0.51557622652458857), (&apos;fox&apos;, 0.51557622652458857), (&apos;sympathetic&apos;, 0.51546600332249293), (&apos;symbolism&apos;, 0.51546600332249293), (&apos;attitude&apos;, 0.51530993621331933), (&apos;appearances&apos;, 0.51466440007315639), (&apos;jeremy&apos;, 0.51466440007315639), (&apos;fun&apos;, 0.51439068993048687), (&apos;south&apos;, 0.51420972175023116), (&apos;arrives&apos;, 0.51409894911095988), (&apos;present&apos;, 0.51341965894303732), (&apos;com&apos;, 0.51326167856387173), (&apos;smile&apos;, 0.51265880484765169), (&apos;alan&apos;, 0.51082562376599072), (&apos;ring&apos;, 0.51082562376599072), (&apos;visit&apos;, 0.51082562376599072), (&apos;fits&apos;, 0.51082562376599072), (&apos;provided&apos;, 0.51082562376599072), (&apos;carter&apos;, 0.51082562376599072), (&apos;aging&apos;, 0.51082562376599072), (&apos;countryside&apos;, 0.51082562376599072), (&apos;begins&apos;, 0.51015650363396647), (&apos;success&apos;, 0.50900578704900468), (&apos;japan&apos;, 0.50900578704900468), (&apos;accurate&apos;, 0.50895471583017893), (&apos;proud&apos;, 0.50800474742434931), (&apos;daily&apos;, 0.5075946031845443), (&apos;karloff&apos;, 0.50724780241810674), (&apos;atmospheric&apos;, 0.50724780241810674), (&apos;recently&apos;, 0.50714914903668207), (&apos;fu&apos;, 0.50704490092608467), (&apos;horrors&apos;, 0.50656122497953315), (&apos;finding&apos;, 0.50637127341661037), (&apos;lust&apos;, 0.5059356384717989), (&apos;hitchcock&apos;, 0.50574947073413001), (&apos;among&apos;, 0.50334004951332734), (&apos;viewing&apos;, 0.50302139827440906), (&apos;investigation&apos;, 0.50262885656181222), (&apos;shining&apos;, 0.50262885656181222), (&apos;duo&apos;, 0.5020919437972361), (&apos;cameron&apos;, 0.5020919437972361), (&apos;finds&apos;, 0.50128303100539795), (&apos;contemporary&apos;, 0.50077528791248915), (&apos;genuine&apos;, 0.50046283673044401), (&apos;frightening&apos;, 0.49995595152908684), (&apos;plays&apos;, 0.49975983848890226), (&apos;age&apos;, 0.49941323171424595), (&apos;position&apos;, 0.49899116611898781), (&apos;continues&apos;, 0.49863035067217237), (&apos;roles&apos;, 0.49839716550752178), (&apos;james&apos;, 0.49837216269470402), (&apos;individuals&apos;, 0.49824684155913052), (&apos;brought&apos;, 0.49783842823917956), (&apos;hilarious&apos;, 0.49714551986191058), (&apos;brutal&apos;, 0.49681488669639234), (&apos;appropriate&apos;, 0.49643688631389105), (&apos;dance&apos;, 0.49581998314812048), (&apos;league&apos;, 0.49578774640145024), (&apos;helping&apos;, 0.49578774640145024), (&apos;answers&apos;, 0.49578774640145024), (&apos;stunts&apos;, 0.49561620510246196), (&apos;traveling&apos;, 0.49532143723002542), (&apos;thoroughly&apos;, 0.49414593456733524), (&apos;depicted&apos;, 0.49317068852726992), (&apos;combination&apos;, 0.49247648509779424), (&apos;honor&apos;, 0.49247648509779424), (&apos;differences&apos;, 0.49247648509779424), (&apos;fully&apos;, 0.49213349075383811), (&apos;tracy&apos;, 0.49159426183810306), (&apos;battles&apos;, 0.49140753790888908), (&apos;possibility&apos;, 0.49112055268665822), (&apos;romance&apos;, 0.4901589869574316), (&apos;initially&apos;, 0.49002249613622745), (&apos;happy&apos;, 0.4898997500608791), (&apos;crime&apos;, 0.48977221456815834), (&apos;singing&apos;, 0.4893852925281213), (&apos;especially&apos;, 0.48901267837860624), (&apos;shakespeare&apos;, 0.48754793889664511), (&apos;hugh&apos;, 0.48729512635579658), (&apos;detail&apos;, 0.48609484250827351), (&apos;julia&apos;, 0.48550781578170082), (&apos;san&apos;, 0.48550781578170082), (&apos;guide&apos;, 0.48550781578170082), (&apos;desperation&apos;, 0.48550781578170082), (&apos;companion&apos;, 0.48550781578170082), (&apos;strongly&apos;, 0.48460242866688824), (&apos;necessary&apos;, 0.48302334245403883), (&apos;humanity&apos;, 0.48265474679929443), (&apos;drama&apos;, 0.48221998493060503), (&apos;nonetheless&apos;, 0.48183808689273838), (&apos;intrigue&apos;, 0.48183808689273838), (&apos;warming&apos;, 0.48183808689273838), (&apos;cuba&apos;, 0.48183808689273838), (&apos;planned&apos;, 0.47957308026188628), (&apos;pictures&apos;, 0.47929937011921681), (&apos;broadcast&apos;, 0.47849024312305422), (&apos;nine&apos;, 0.47803580094299974), (&apos;settings&apos;, 0.47743860773325364), (&apos;history&apos;, 0.47732966933780852), (&apos;ordinary&apos;, 0.47725880012690741), (&apos;trade&apos;, 0.47692407209030935), (&apos;official&apos;, 0.47608267532211779), (&apos;primary&apos;, 0.47608267532211779), (&apos;episode&apos;, 0.47529620261150429), (&apos;role&apos;, 0.47520268270188676), (&apos;spirit&apos;, 0.47477690799839323), (&apos;grey&apos;, 0.47409361449726067), (&apos;ways&apos;, 0.47323464982718205), (&apos;cup&apos;, 0.47260441094579297), (&apos;piano&apos;, 0.47260441094579297), (&apos;familiar&apos;, 0.47241617565111949), (&apos;sinister&apos;, 0.47198579044972683), (&apos;reveal&apos;, 0.47171449364936496), (&apos;max&apos;, 0.47150852042515579), (&apos;dated&apos;, 0.47121648567094482), (&apos;losing&apos;, 0.47000362924573563), (&apos;discovery&apos;, 0.47000362924573563), (&apos;vicious&apos;, 0.47000362924573563), (&apos;genuinely&apos;, 0.46871413841586385), (&apos;hatred&apos;, 0.46734051182625186), (&apos;mistaken&apos;, 0.46702300110759781), (&apos;dream&apos;, 0.46608972992459924), (&apos;challenge&apos;, 0.46608972992459924), (&apos;crisis&apos;, 0.46575733836428446), (&apos;photographed&apos;, 0.46488852857896512), (&apos;critics&apos;, 0.46430560813109778), (&apos;bird&apos;, 0.46430560813109778), (&apos;machines&apos;, 0.46430560813109778), (&apos;born&apos;, 0.46411383518967209), (&apos;detective&apos;, 0.4636633473511525), (&apos;higher&apos;, 0.46328467899699055), (&apos;remains&apos;, 0.46262352194811296), (&apos;inevitable&apos;, 0.46262352194811296), (&apos;soviet&apos;, 0.4618180446592961), (&apos;ryan&apos;, 0.46134556650262099), (&apos;african&apos;, 0.46112595521371813), (&apos;smaller&apos;, 0.46081520319132935), (&apos;techniques&apos;, 0.46052488529119184), (&apos;information&apos;, 0.46034171833399862), (&apos;deserved&apos;, 0.45999798712841444), (&apos;lynch&apos;, 0.45953232937844013), (&apos;spielberg&apos;, 0.45953232937844013), (&apos;cynical&apos;, 0.45953232937844013), (&apos;tour&apos;, 0.45953232937844013), (&apos;francisco&apos;, 0.45953232937844013), (&apos;struggle&apos;, 0.45911782160048453), (&apos;language&apos;, 0.45902121257712653), (&apos;visual&apos;, 0.45823514408822852), (&apos;warner&apos;, 0.45724137763188427), (&apos;social&apos;, 0.45720078250735313), (&apos;reality&apos;, 0.45719346885019546), (&apos;hidden&apos;, 0.45675840249571492), (&apos;breaking&apos;, 0.45601738727099561), (&apos;sometimes&apos;, 0.45563021171182794), (&apos;modern&apos;, 0.45500247579345005), (&apos;surfing&apos;, 0.45425527227759638), (&apos;popular&apos;, 0.45410691533051023), (&apos;surprised&apos;, 0.4534409399850382), (&apos;follows&apos;, 0.45245361754408348), (&apos;keeps&apos;, 0.45234869400701483), (&apos;john&apos;, 0.4520909494482197), (&apos;mixed&apos;, 0.45198512374305722), (&apos;defeat&apos;, 0.45198512374305722), (&apos;justice&apos;, 0.45142724367280018), (&apos;treasure&apos;, 0.45083371313801535), (&apos;presents&apos;, 0.44973793178615257), (&apos;years&apos;, 0.44919197032104968), (&apos;chief&apos;, 0.44895022004790319), (&apos;shadows&apos;, 0.44802472252696035), (&apos;closely&apos;, 0.44701411102103689), (&apos;segments&apos;, 0.44701411102103689), (&apos;lose&apos;, 0.44658335503763702), (&apos;caine&apos;, 0.44628710262841953), (&apos;caught&apos;, 0.44610275383999071), (&apos;hamlet&apos;, 0.44558510189758965), (&apos;chinese&apos;, 0.44507424620321018), (&apos;welcome&apos;, 0.44438052435783792), (&apos;birth&apos;, 0.44368632092836219), (&apos;represents&apos;, 0.44320543609101143), (&apos;puts&apos;, 0.44279106572085081), (&apos;visuals&apos;, 0.44183275227903923), (&apos;fame&apos;, 0.44183275227903923), (&apos;closer&apos;, 0.44183275227903923), (&apos;web&apos;, 0.44183275227903923), (&apos;criminal&apos;, 0.4412745608048752), (&apos;minor&apos;, 0.4409224199448939), (&apos;jon&apos;, 0.44086703515908027), (&apos;liked&apos;, 0.44074991514020723), (&apos;restaurant&apos;, 0.44031183943833246), (&apos;de&apos;, 0.43983275161237217), (&apos;flaws&apos;, 0.43983275161237217), (&apos;searching&apos;, 0.4393666597838457), (&apos;rap&apos;, 0.43891304217570443), (&apos;light&apos;, 0.43884433018199892), (&apos;elizabeth&apos;, 0.43872232986464677), (&apos;marry&apos;, 0.43861731542506488), (&apos;learned&apos;, 0.43825493093115531), (&apos;controversial&apos;, 0.43825493093115531), (&apos;oz&apos;, 0.43825493093115531), (&apos;slowly&apos;, 0.43785660389939979), (&apos;comedic&apos;, 0.43721380642274466), (&apos;wayne&apos;, 0.43721380642274466), (&apos;thrilling&apos;, 0.43721380642274466), (&apos;bridge&apos;, 0.43721380642274466), (&apos;married&apos;, 0.43658501682196887), (&apos;nazi&apos;, 0.4361020775700542), (&apos;murder&apos;, 0.4353180712578455), (&apos;physical&apos;, 0.4353180712578455), (&apos;johnny&apos;, 0.43483971678806865), (&apos;michelle&apos;, 0.43445264498141672), (&apos;wallace&apos;, 0.43403848055222038), (&apos;comedies&apos;, 0.43395706390247063), (&apos;silent&apos;, 0.43395706390247063), (&apos;played&apos;, 0.43387244114515305), (&apos;international&apos;, 0.43363598507486073), (&apos;vision&apos;, 0.43286408229627887), (&apos;intelligent&apos;, 0.43196704885367099), (&apos;shop&apos;, 0.43078291609245434), (&apos;also&apos;, 0.43036720209769169), (&apos;levels&apos;, 0.4302451371066513), (&apos;miss&apos;, 0.43006426712153217), (&apos;movement&apos;, 0.4295626596872249), ...] 12# words most frequently seen in a review with a \"NEGATIVE\" labellist(reversed(pos_neg_ratios.most_common()))[0:30] [(&apos;boll&apos;, -4.0778152602708904), (&apos;uwe&apos;, -3.9218753018711578), (&apos;seagal&apos;, -3.3202501058581921), (&apos;unwatchable&apos;, -3.0269848170580955), (&apos;stinker&apos;, -2.9876839403711624), (&apos;mst&apos;, -2.7753833211707968), (&apos;incoherent&apos;, -2.7641396677532537), (&apos;unfunny&apos;, -2.5545257844967644), (&apos;waste&apos;, -2.4907515123361046), (&apos;blah&apos;, -2.4475792789485005), (&apos;horrid&apos;, -2.3715779644809971), (&apos;pointless&apos;, -2.3451073877136341), (&apos;atrocious&apos;, -2.3187369339642556), (&apos;redeeming&apos;, -2.2667790015910296), (&apos;prom&apos;, -2.2601040980178784), (&apos;drivel&apos;, -2.2476029585766928), (&apos;lousy&apos;, -2.2118080125207054), (&apos;worst&apos;, -2.1930856334332267), (&apos;laughable&apos;, -2.172468615469592), (&apos;awful&apos;, -2.1385076866397488), (&apos;poorly&apos;, -2.1326133844207011), (&apos;wasting&apos;, -2.1178155545614512), (&apos;remotely&apos;, -2.111046881095167), (&apos;existent&apos;, -2.0024805005437076), (&apos;boredom&apos;, -1.9241486572738005), (&apos;miserably&apos;, -1.9216610938019989), (&apos;sucks&apos;, -1.9166645809588516), (&apos;uninspired&apos;, -1.9131499212248517), (&apos;lame&apos;, -1.9117232884159072), (&apos;insult&apos;, -1.9085323769376259)] 1234from bokeh.models import ColumnDataSource, LabelSetfrom bokeh.plotting import figure, show, output_filefrom bokeh.io import output_notebookoutput_notebook() &lt;div class=&quot;bk-root&quot;&gt; &lt;a href=&quot;http://bokeh.pydata.org&quot; target=&quot;_blank&quot; class=&quot;bk-logo bk-logo-small bk-logo-notebook&quot;&gt;&lt;/a&gt; &lt;span id=&quot;67a2b150-b2f3-44b7-a401-91d2f2e64a0b&quot;&gt;Loading BokehJS ...&lt;/span&gt; &lt;/div&gt; 1234567hist, edges = np.histogram(list(map(lambda x:x[1],pos_neg_ratios.most_common())), density=True, bins=100, normed=True)p = figure(tools=\"pan,wheel_zoom,reset,save\", toolbar_location=\"above\", title=\"Word Positive/Negative Affinity Distribution\")p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\")show(p) &lt;div class=&quot;bk-root&quot;&gt; &lt;div class=&quot;bk-plotdiv&quot; id=&quot;09af843a-8ba0-4f13-a758-59453c5e2213&quot;&gt;&lt;/div&gt; &lt;/div&gt; (function(global) { function now() { return new Date(); } var force = false; if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) { window._bokeh_onload_callbacks = []; window._bokeh_is_loading = undefined; } if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) { window._bokeh_timeout = Date.now() + 0; window._bokeh_failed_load = false; } var NB_LOAD_WARNING = {'data': {'text/html': \"\\n\"+ \"\\n\"+ \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+ \"may be due to a slow or bad network connection. Possible fixes:\\n\"+ \"\\n\"+ \"\\n\"+ \"re-rerun `output_notebook()` to attempt to load from CDN again, or\\n\"+ \"use INLINE resources instead, as so:\\n\"+ \"\\n\"+ \"\\n\"+ \"from bokeh.resources import INLINE\\n\"+ \"output_notebook(resources=INLINE)\\n\"+ \"\\n\"+ \"\"}}; function display_loaded() { if (window.Bokeh !== undefined) { document.getElementById(\"09af843a-8ba0-4f13-a758-59453c5e2213\").textContent = \"BokehJS successfully loaded.\"; } else if (Date.now() < window._bokeh_timeout) { setTimeout(display_loaded, 100) } } function run_callbacks() { window._bokeh_onload_callbacks.forEach(function(callback) { callback() }); delete window._bokeh_onload_callbacks console.info(\"Bokeh: all callbacks have finished\"); } function load_libs(js_urls, callback) { window._bokeh_onload_callbacks.push(callback); if (window._bokeh_is_loading > 0) { console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now()); return null; } if (js_urls == null || js_urls.length === 0) { run_callbacks(); return null; } console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now()); window._bokeh_is_loading = js_urls.length; for (var i = 0; i < js_urls.length; i++) { var url = js_urls[i]; var s = document.createElement('script'); s.src = url; s.async = false; s.onreadystatechange = s.onload = function() { window._bokeh_is_loading--; if (window._bokeh_is_loading === 0) { console.log(\"Bokeh: all BokehJS libraries loaded\"); run_callbacks() } }; s.onerror = function() { console.warn(\"failed to load library \" + url); }; console.log(\"Bokeh: injecting script tag for BokehJS library: \", url); document.getElementsByTagName(\"head\")[0].appendChild(s); } };var element = document.getElementById(\"09af843a-8ba0-4f13-a758-59453c5e2213\"); if (element == null) { console.log(\"Bokeh: ERROR: autoload.js configured with elementid '09af843a-8ba0-4f13-a758-59453c5e2213' but no matching script tag was found. \") return false; } var js_urls = []; var inline_js = [ function(Bokeh) { (function() { var fn = function() { var docs_json = {\"932bbd7d-9cfa-4f24-a68d-65a2f3b9d077\":{\"roots\":{\"references\":[{\"attributes\":{\"plot\":{\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"fceb939b-a091-488b-89d4-4a1289254b2b\",\"type\":\"BasicTicker\"}},\"id\":\"bd6c6269-c94e-423b-a26b-41c0a1d3da33\",\"type\":\"Grid\"},{\"attributes\":{\"formatter\":{\"id\":\"56803c7f-e6cd-4496-8898-175419c75307\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"a401f008-4871-4894-80ee-d6b81cbac492\",\"type\":\"BasicTicker\"}},\"id\":\"873f13d5-ad53-41cc-9e68-558b3625adb1\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"771947ef-d589-49ae-98f5-6ef45a8c1024\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"a401f008-4871-4894-80ee-d6b81cbac492\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"a401f008-4871-4894-80ee-d6b81cbac492\",\"type\":\"BasicTicker\"}},\"id\":\"98985a39-f4f7-447e-90da-8df0b5326fe4\",\"type\":\"Grid\"},{\"attributes\":{\"data_source\":{\"id\":\"7ffb53d7-2954-42ee-8006-9db7d370a58a\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"9ccfdd0e-2895-4f93-b5bb-b0f46e0d1dba\",\"type\":\"Quad\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"c19a3f86-aa36-40c1-b111-a8f7593ec01f\",\"type\":\"Quad\"},\"selection_glyph\":null},\"id\":\"458f8b66-92f2-4945-8953-5ff8544870b6\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":{\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"29bf3458-2214-4e1c-8371-cf99a3e3d1ee\",\"type\":\"PanTool\"},{\"attributes\":{\"plot\":null,\"text\":\"Word Positive/Negative Affinity Distribution\"},\"id\":\"5e6aebc1-300c-4468-bae9-518ac28ca463\",\"type\":\"Title\"},{\"attributes\":{\"plot\":{\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"08797199-4344-44d6-a585-ecd6eec0e0e5\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"plot\":{\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"c922a12f-3b41-4cfe-ad44-30cb288bd738\",\"type\":\"ResetTool\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"c19a3f86-aa36-40c1-b111-a8f7593ec01f\",\"type\":\"Quad\"},{\"attributes\":{\"plot\":{\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"c8c8f3b7-92a6-48de-a5a7-a7c8745d236d\",\"type\":\"SaveTool\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_color\":{\"value\":\"#555555\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"9ccfdd0e-2895-4f93-b5bb-b0f46e0d1dba\",\"type\":\"Quad\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"left\",\"right\",\"top\"],\"data\":{\"left\":{\"__ndarray__\":\"Cvm3za5PEMCZHufvxesPwB5LXkQuOA/AonfVmJaEDsAnpEzt/tANwKzQw0FnHQ3AMf06ls9pDMC2KbLqN7YLwDpWKT+gAgvAv4KgkwhPCsBErxfocJsJwMnbjjzZ5wjATggGkUE0CMDSNH3lqYAHwFhh9DkSzQbA3I1rjnoZBsBhuuLi4mUFwObmWTdLsgTAahPRi7P+A8DwP0jgG0sDwHRsvzSElwLA+Zg2iezjAcB+xa3dVDABwAPyJDK9fADAED04DUuS/78Ylia2Gyv+vyLvFF/sw/y/LEgDCL1c+782ofGwjfX5v0D631lejvi/SFPOAi8n979SrLyr/7/1v1wFq1TQWPS/Zl6Z/aDx8r9wt4emcYrxv3gQdk9CI/C/BNPI8CV47b8YhaVCx6nqvyw3gpRo2+e/QOle5gkN5b9Qmzs4qz7iv8iaMBSZ4N6/8P7pt9tD2b8YY6NbHqfTv4COuf7BFMy/0FYsRkfbwL+AfHw2Moemv4BiuKu4XqY/QFB74yjRwD8AiAicowrMP+DfSioPotM/sHuRhsw+2T+QF9jiidveP7BZj58jPOI/oKeyTYIK5T+Q9dX74NjnP3hD+ak/p+o/aJEcWJ517T+o7x+D/iHwP6CWMdotifE/mD1DMV3w8j+M5FSIjFf0P4SLZt+7vvU/eDJ4Nusl9z9w2YmNGo34P2iAm+RJ9Pk/XCetO3lb+z9Uzr6SqML8P0h10OnXKf4/QBziQAeR/z+c4flLG3wAQBa1gveyLwFAkogLo0rjAUAMXJRO4pYCQIgvHfp5SgNABAOmpRH+A0B+1i5RqbEEQPqpt/xAZQVAdH1AqNgYBkDwUMlTcMwGQGwkUv8HgAdA5vfaqp8zCEBiy2NWN+cIQNye7AHPmglAWHJ1rWZOCkDSRf5Y/gELQE4ZhwSWtQtAyuwPsC1pDEBEwJhbxRwNQMCTIQdd0A1AOmeqsvSDDkC2OjNejDcPQDAOvAkk6w9A1nCi2l1PEECU2mawKakQQFJEK4b1AhFADq7vW8FcEUDMF7QxjbYRQIqBeAdZEBJASOs83SRqEkA=\",\"dtype\":\"float64\",\"shape\":[100]},\"right\":{\"__ndarray__\":\"mR7n78XrD8AeS15ELjgPwKJ31ZiWhA7AJ6RM7f7QDcCs0MNBZx0NwDH9OpbPaQzAtimy6je2C8A6Vik/oAILwL+CoJMITwrARK8X6HCbCcDJ24482ecIwE4IBpFBNAjA0jR95amAB8BYYfQ5Es0GwNyNa456GQbAYbri4uJlBcDm5lk3S7IEwGoT0Yuz/gPA8D9I4BtLA8B0bL80hJcCwPmYNons4wHAfsWt3VQwAcAD8iQyvXwAwBA9OA1Lkv+/GJYmthsr/r8i7xRf7MP8vyxIAwi9XPu/NqHxsI31+b9A+t9ZXo74v0hTzgIvJ/e/Uqy8q/+/9b9cBatU0Fj0v2Zemf2g8fK/cLeHpnGK8b94EHZPQiPwvwTTyPAleO2/GIWlQsep6r8sN4KUaNvnv0DpXuYJDeW/UJs7OKs+4r/ImjAUmeDev/D+6bfbQ9m/GGOjWx6n07+Ajrn+wRTMv9BWLEZH28C/gHx8NjKHpr+AYriruF6mP0BQe+Mo0cA/AIgInKMKzD/g30oqD6LTP7B7kYbMPtk/kBfY4onb3j+wWY+fIzziP6Cnsk2CCuU/kPXV++DY5z94Q/mpP6fqP2iRHFiede0/qO8fg/4h8D+gljHaLYnxP5g9QzFd8PI/jORUiIxX9D+Ei2bfu771P3gyeDbrJfc/cNmJjRqN+D9ogJvkSfT5P1wnrTt5W/s/VM6+kqjC/D9IddDp1yn+P0Ac4kAHkf8/nOH5Sxt8AEAWtYL3si8BQJKIC6NK4wFADFyUTuKWAkCILx36eUoDQAQDpqUR/gNAftYuUamxBED6qbf8QGUFQHR9QKjYGAZA8FDJU3DMBkBsJFL/B4AHQOb32qqfMwhAYstjVjfnCEDcnuwBz5oJQFhyda1mTgpA0kX+WP4BC0BOGYcElrULQMrsD7AtaQxARMCYW8UcDUDAkyEHXdANQDpnqrL0gw5AtjozXow3D0AwDrwJJOsPQNZwotpdTxBAlNpmsCmpEEBSRCuG9QIRQA6u71vBXBFAzBe0MY22EUCKgXgHWRASQEjrPN0kahJABlUBs/DDEkA=\",\"dtype\":\"float64\",\"shape\":[100]},\"top\":{\"__ndarray__\":\"s6auGMn1ZT+zpq4YyfVlPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALOmrhjJ9WU/AAAAAAAAAAAAAAAAAAAAALOmrhjJ9WU/k6auGMn1ZT8AAAAAAAAAAJOmrhjJ9XU/AAAAAAAAAAAAAAAAAAAAAJOmrhjJ9WU/0qauGMn1dT+Tpq4YyfV1P7OmrhjJ9YU/Bf2C0lZ4gD+zpq4YyfWFP7OmrhjJ9WU/wdGY9Q83kz9fUNpeO3ObPwX9gtJWeJA/s6auGMn1lT/d0Zj1DzeTP8HRmPUPN6M/9GVPzd4Tqj+Je8Q7grSoP/RlT83eE6o/O3JIGwUosT9zD3sTUZGvP/RlT83eE7o/9GVPzd4Tuj+ht+X2LdDAP+gbdGF3pcY/2lQY73k5zz+nXNOsYYfSP6wBwWKVPtQ/xJV3OmQb2z9Z/6EadlviPyTgAZkQXOU/7sBhF6tc6D9YgiEU4F3uP2El8IH0Me4/XSz8TJet6j/wnIMFB5fnP/EzCBg6Kec/9UZePr7m4z89itzRx6vhP+ED4Kq0IdY/9UZePr7m0z+XmrXKouHOP/nrS/TxncU/Zbwjh2yWxD+hOmXwl9K8P/VGXj6+5rM/mzHpzxpGtT/8kDmqJVWnP2W8I4dslqQ/wdGY9Q83oz8qvCOHbJakP/jRmPUPN5M/wdGY9Q83kz8d/YLSVniQP5OmrhjJ9YU/k6auGMn1hT/Spq4YyfVlP5OmrhjJ9WU/0qauGMn1ZT8AAAAAAAAAAJOmrhjJ9WU/0qauGMn1ZT+Tpq4YyfVlP9KmrhjJ9WU/k6auGMn1dT8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADSpq4YyfVlPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAk6auGMn1ZT8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAk6auGMn1ZT8=\",\"dtype\":\"float64\",\"shape\":[100]}}},\"id\":\"7ffb53d7-2954-42ee-8006-9db7d370a58a\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"below\":[{\"id\":\"d0189600-2bc8-4f4f-8e59-7b111ff4e7ca\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"873f13d5-ad53-41cc-9e68-558b3625adb1\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"d0189600-2bc8-4f4f-8e59-7b111ff4e7ca\",\"type\":\"LinearAxis\"},{\"id\":\"bd6c6269-c94e-423b-a26b-41c0a1d3da33\",\"type\":\"Grid\"},{\"id\":\"873f13d5-ad53-41cc-9e68-558b3625adb1\",\"type\":\"LinearAxis\"},{\"id\":\"98985a39-f4f7-447e-90da-8df0b5326fe4\",\"type\":\"Grid\"},{\"id\":\"458f8b66-92f2-4945-8953-5ff8544870b6\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"5e6aebc1-300c-4468-bae9-518ac28ca463\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"6fdc9754-e9b6-488e-a2fa-c6b29785e844\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"da89543f-9ed9-4bd5-9afc-56343213fc08\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"339bef66-25cb-4439-919f-916b91ddb00d\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"176af089-47aa-4cd2-a1f1-bf4fa136c326\",\"type\":\"DataRange1d\"}},\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null},\"id\":\"339bef66-25cb-4439-919f-916b91ddb00d\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"56803c7f-e6cd-4496-8898-175419c75307\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"29bf3458-2214-4e1c-8371-cf99a3e3d1ee\",\"type\":\"PanTool\"},{\"id\":\"08797199-4344-44d6-a585-ecd6eec0e0e5\",\"type\":\"WheelZoomTool\"},{\"id\":\"c922a12f-3b41-4cfe-ad44-30cb288bd738\",\"type\":\"ResetTool\"},{\"id\":\"c8c8f3b7-92a6-48de-a5a7-a7c8745d236d\",\"type\":\"SaveTool\"}]},\"id\":\"da89543f-9ed9-4bd5-9afc-56343213fc08\",\"type\":\"Toolbar\"},{\"attributes\":{\"formatter\":{\"id\":\"771947ef-d589-49ae-98f5-6ef45a8c1024\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"fceb939b-a091-488b-89d4-4a1289254b2b\",\"type\":\"BasicTicker\"}},\"id\":\"d0189600-2bc8-4f4f-8e59-7b111ff4e7ca\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"6fdc9754-e9b6-488e-a2fa-c6b29785e844\",\"type\":\"ToolEvents\"},{\"attributes\":{\"callback\":null},\"id\":\"176af089-47aa-4cd2-a1f1-bf4fa136c326\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"fceb939b-a091-488b-89d4-4a1289254b2b\",\"type\":\"BasicTicker\"}],\"root_ids\":[\"7ae81053-1f24-48a9-85e4-b30704bbc24b\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.4\"}}; var render_items = [{\"docid\":\"932bbd7d-9cfa-4f24-a68d-65a2f3b9d077\",\"elementid\":\"09af843a-8ba0-4f13-a758-59453c5e2213\",\"modelid\":\"7ae81053-1f24-48a9-85e4-b30704bbc24b\"}]; Bokeh.embed.embed_items(docs_json, render_items); }; if (document.readyState != \"loading\") fn(); else document.addEventListener(\"DOMContentLoaded\", fn); })(); }, function(Bokeh) { } ]; function run_inline_js() { if ((window.Bokeh !== undefined) || (force === true)) { for (var i = 0; i < inline_js.length; i++) { inline_js[i](window.Bokeh); }if (force === true) { display_loaded(); }} else if (Date.now() < window._bokeh_timeout) { setTimeout(run_inline_js, 100); } else if (!window._bokeh_failed_load) { console.log(\"Bokeh: BokehJS failed to load within specified timeout.\"); window._bokeh_failed_load = true; } else if (force !== true) { var cell = $(document.getElementById(\"09af843a-8ba0-4f13-a758-59453c5e2213\")).parents('.cell').data().cell; cell.output_area.append_execute_result(NB_LOAD_WARNING) } } if (window._bokeh_is_loading === 0) { console.log(\"Bokeh: BokehJS loaded, going straight to plotting\"); run_inline_js(); } else { load_libs(js_urls, function() { console.log(\"Bokeh: BokehJS plotting callback run at\", now()); run_inline_js(); }); } }(this)); 1234frequency_frequency = Counter()for word, cnt in total_counts.most_common(): frequency_frequency[cnt] += 1 1234567hist, edges = np.histogram(list(map(lambda x:x[1],frequency_frequency.most_common())), density=True, bins=100, normed=True)p = figure(tools=\"pan,wheel_zoom,reset,save\", toolbar_location=\"above\", title=\"The frequency distribution of the words in our corpus\")p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\")show(p) &lt;div class=&quot;bk-root&quot;&gt; &lt;div class=&quot;bk-plotdiv&quot; id=&quot;b7f2e30a-9d83-4ebd-a216-d384f5ef713f&quot;&gt;&lt;/div&gt; &lt;/div&gt; (function(global) { function now() { return new Date(); } var force = false; if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) { window._bokeh_onload_callbacks = []; window._bokeh_is_loading = undefined; } if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) { window._bokeh_timeout = Date.now() + 0; window._bokeh_failed_load = false; } var NB_LOAD_WARNING = {'data': {'text/html': \"\\n\"+ \"\\n\"+ \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+ \"may be due to a slow or bad network connection. Possible fixes:\\n\"+ \"\\n\"+ \"\\n\"+ \"re-rerun `output_notebook()` to attempt to load from CDN again, or\\n\"+ \"use INLINE resources instead, as so:\\n\"+ \"\\n\"+ \"\\n\"+ \"from bokeh.resources import INLINE\\n\"+ \"output_notebook(resources=INLINE)\\n\"+ \"\\n\"+ \"\"}}; function display_loaded() { if (window.Bokeh !== undefined) { document.getElementById(\"b7f2e30a-9d83-4ebd-a216-d384f5ef713f\").textContent = \"BokehJS successfully loaded.\"; } else if (Date.now() < window._bokeh_timeout) { setTimeout(display_loaded, 100) } } function run_callbacks() { window._bokeh_onload_callbacks.forEach(function(callback) { callback() }); delete window._bokeh_onload_callbacks console.info(\"Bokeh: all callbacks have finished\"); } function load_libs(js_urls, callback) { window._bokeh_onload_callbacks.push(callback); if (window._bokeh_is_loading > 0) { console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now()); return null; } if (js_urls == null || js_urls.length === 0) { run_callbacks(); return null; } console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now()); window._bokeh_is_loading = js_urls.length; for (var i = 0; i < js_urls.length; i++) { var url = js_urls[i]; var s = document.createElement('script'); s.src = url; s.async = false; s.onreadystatechange = s.onload = function() { window._bokeh_is_loading--; if (window._bokeh_is_loading === 0) { console.log(\"Bokeh: all BokehJS libraries loaded\"); run_callbacks() } }; s.onerror = function() { console.warn(\"failed to load library \" + url); }; console.log(\"Bokeh: injecting script tag for BokehJS library: \", url); document.getElementsByTagName(\"head\")[0].appendChild(s); } };var element = document.getElementById(\"b7f2e30a-9d83-4ebd-a216-d384f5ef713f\"); if (element == null) { console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'b7f2e30a-9d83-4ebd-a216-d384f5ef713f' but no matching script tag was found. \") return false; } var js_urls = []; var inline_js = [ function(Bokeh) { (function() { var fn = function() { var docs_json = {\"71ebf26b-1b16-419a-bfb4-fb3b12fe4b8e\":{\"roots\":{\"references\":[{\"attributes\":{\"plot\":{\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"80d5047d-6cdd-4f06-a373-f044d2ccf874\",\"type\":\"BasicTicker\"}},\"id\":\"b0566e40-18b7-4f47-9b7f-5b869bd83c36\",\"type\":\"Grid\"},{\"attributes\":{\"formatter\":{\"id\":\"e4009fd6-6d49-4acb-96da-1513ad73804e\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"841c781a-33a2-4267-b957-c0789a86bd74\",\"type\":\"BasicTicker\"}},\"id\":\"c1a5ddee-bdad-49fd-b476-10a86fd93113\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null},\"id\":\"79df4143-c27c-44df-8806-613cb968be37\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"left\",\"right\",\"top\"],\"data\":{\"left\":{\"__ndarray__\":\"AAAAAAAA8D/NzMzMzFhxQM3MzMzMUIFANDMzMzP1iUDNzMzMzEyRQAAAAAAAn5VANDMzMzPxmUBnZmZmZkOeQM3MzMzMSqFAZ2ZmZuZzo0AAAAAAAJ2lQJqZmZkZxqdANDMzMzPvqUDNzMzMTBisQGdmZmZmQa5AAAAAAEA1sEDNzMzMzEmxQJqZmZlZXrJAZ2ZmZuZys0AzMzMzc4e0QAAAAAAAnLVAzczMzIywtkCamZmZGcW3QGdmZmam2bhANDMzMzPuuUAAAAAAwAK7QM3MzMxMF7xAmpmZmdkrvUBnZmZmZkC+QDQzMzPzVL9AAAAAAMA0wEBnZmZmBr/AQM3MzMxMScFAMzMzM5PTwUCamZmZ2V3CQAAAAAAg6MJAZ2ZmZmZyw0DNzMzMrPzDQDMzMzPzhsRAmpmZmTkRxUAAAAAAgJvFQGdmZmbGJcZAzczMzAywxkAzMzMzUzrHQJqZmZmZxMdAAAAAAOBOyEBnZmZmJtnIQM3MzMxsY8lANDMzM7PtyUCamZmZ+XfKQAAAAABAAstAZ2ZmZoaMy0DNzMzMzBbMQDQzMzMTocxAmpmZmVkrzUAAAAAAoLXNQGdmZmbmP85AzczMzCzKzkA0MzMzc1TPQJqZmZm53s9AAAAAAIA00EAzMzMzo3nQQGdmZmbGvtBAmpmZmekD0UDNzMzMDEnRQAAAAAAwjtFAMzMzM1PT0UBnZmZmdhjSQJqZmZmZXdJAzczMzLyi0kAAAAAA4OfSQDMzMzMDLdNAZ2ZmZiZy00CamZmZSbfTQM3MzMxs/NNAAAAAAJBB1EAzMzMzs4bUQGdmZmbWy9RAmpmZmfkQ1UDNzMzMHFbVQAAAAABAm9VAMzMzM2Pg1UBnZmZmhiXWQJqZmZmpatZAzczMzMyv1kAAAAAA8PTWQDMzMzMTOtdAZ2ZmZjZ/10CamZmZWcTXQM3MzMx8CdhAAAAAAKBO2EAzMzMzw5PYQGdmZmbm2NhAmpmZmQke2UDNzMzMLGPZQAAAAABQqNlANDMzM3Pt2UBnZmZmljLaQJqZmZm5d9pAzczMzNy82kA=\",\"dtype\":\"float64\",\"shape\":[100]},\"right\":{\"__ndarray__\":\"zczMzMxYcUDNzMzMzFCBQDQzMzMz9YlAzczMzMxMkUAAAAAAAJ+VQDQzMzMz8ZlAZ2ZmZmZDnkDNzMzMzEqhQGdmZmbmc6NAAAAAAACdpUCamZmZGcanQDQzMzMz76lAzczMzEwYrEBnZmZmZkGuQAAAAABANbBAzczMzMxJsUCamZmZWV6yQGdmZmbmcrNAMzMzM3OHtEAAAAAAAJy1QM3MzMyMsLZAmpmZmRnFt0BnZmZmptm4QDQzMzMz7rlAAAAAAMACu0DNzMzMTBe8QJqZmZnZK71AZ2ZmZmZAvkA0MzMz81S/QAAAAADANMBAZ2ZmZga/wEDNzMzMTEnBQDMzMzOT08FAmpmZmdldwkAAAAAAIOjCQGdmZmZmcsNAzczMzKz8w0AzMzMz84bEQJqZmZk5EcVAAAAAAICbxUBnZmZmxiXGQM3MzMwMsMZAMzMzM1M6x0CamZmZmcTHQAAAAADgTshAZ2ZmZibZyEDNzMzMbGPJQDQzMzOz7clAmpmZmfl3ykAAAAAAQALLQGdmZmaGjMtAzczMzMwWzEA0MzMzE6HMQJqZmZlZK81AAAAAAKC1zUBnZmZm5j/OQM3MzMwsys5ANDMzM3NUz0CamZmZud7PQAAAAACANNBAMzMzM6N50EBnZmZmxr7QQJqZmZnpA9FAzczMzAxJ0UAAAAAAMI7RQDMzMzNT09FAZ2ZmZnYY0kCamZmZmV3SQM3MzMy8otJAAAAAAODn0kAzMzMzAy3TQGdmZmYmctNAmpmZmUm300DNzMzMbPzTQAAAAACQQdRAMzMzM7OG1EBnZmZm1svUQJqZmZn5ENVAzczMzBxW1UAAAAAAQJvVQDMzMzNj4NVAZ2ZmZoYl1kCamZmZqWrWQM3MzMzMr9ZAAAAAAPD01kAzMzMzEzrXQGdmZmY2f9dAmpmZmVnE10DNzMzMfAnYQAAAAACgTthAMzMzM8OT2EBnZmZm5tjYQJqZmZkJHtlAzczMzCxj2UAAAAAAUKjZQDQzMzNz7dlAZ2ZmZpYy2kCamZmZuXfaQM3MzMzcvNpAAAAAAAAC20A=\",\"dtype\":\"float64\",\"shape\":[100]},\"top\":{\"__ndarray__\":\"TDhFg5YVbT+JTZDME4n7PtMKDQpDB+Y+1QoNCkMH1j7VCg0KQwfWPgAAAAAAAAAA1QoNCkMHxj7VCg0KQwfGPgAAAAAAAAAA2goNCkMHxj4AAAAAAAAAAAAAAAAAAAAA2goNCkMHxj4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA5QoNCkMHxj4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOUKDQpDB8Y+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA5QoNCkMHxj4=\",\"dtype\":\"float64\",\"shape\":[100]}}},\"id\":\"1dcf734c-3ee8-4820-b255-a6d09b3f6630\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"plot\":null,\"text\":\"The frequency distribution of the words in our corpus\"},\"id\":\"32291e3c-3b99-4022-a205-eb50ee2e3791\",\"type\":\"Title\"},{\"attributes\":{\"plot\":{\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"c1650719-b9f0-4219-a6ba-d65320df6105\",\"type\":\"ResetTool\"},{\"attributes\":{\"plot\":{\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"c916bd7e-aa63-4038-a897-ae996b97b15f\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"5011db81-ef2c-4076-80e0-1e1ad0235528\",\"type\":\"ToolEvents\"},{\"attributes\":{\"below\":[{\"id\":\"f06a62f8-4479-4ddb-8c38-ba4cfd716307\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"c1a5ddee-bdad-49fd-b476-10a86fd93113\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"f06a62f8-4479-4ddb-8c38-ba4cfd716307\",\"type\":\"LinearAxis\"},{\"id\":\"b0566e40-18b7-4f47-9b7f-5b869bd83c36\",\"type\":\"Grid\"},{\"id\":\"c1a5ddee-bdad-49fd-b476-10a86fd93113\",\"type\":\"LinearAxis\"},{\"id\":\"f46df637-6370-4ad2-8c4c-e660fbdafc69\",\"type\":\"Grid\"},{\"id\":\"0498b1d6-9a31-4c8c-8f03-cf575e7040a5\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"32291e3c-3b99-4022-a205-eb50ee2e3791\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"5011db81-ef2c-4076-80e0-1e1ad0235528\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"f0ea0e05-5534-4c03-9a92-502f9ffba541\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"79df4143-c27c-44df-8806-613cb968be37\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"51c6ffd2-1fde-4648-89ee-ee00cd39a672\",\"type\":\"DataRange1d\"}},\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"formatter\":{\"id\":\"4dc029e2-a490-4afe-9d77-8d9d7ee08260\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"80d5047d-6cdd-4f06-a373-f044d2ccf874\",\"type\":\"BasicTicker\"}},\"id\":\"f06a62f8-4479-4ddb-8c38-ba4cfd716307\",\"type\":\"LinearAxis\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"8d2223a7-1e09-4dec-bddc-b1ee2cf4c729\",\"type\":\"PanTool\"},{\"id\":\"5bead03e-6766-4265-abc1-96776fa9c176\",\"type\":\"WheelZoomTool\"},{\"id\":\"c1650719-b9f0-4219-a6ba-d65320df6105\",\"type\":\"ResetTool\"},{\"id\":\"c916bd7e-aa63-4038-a897-ae996b97b15f\",\"type\":\"SaveTool\"}]},\"id\":\"f0ea0e05-5534-4c03-9a92-502f9ffba541\",\"type\":\"Toolbar\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"aecb1e69-3dd3-4661-9a1c-d12b0e5174f4\",\"type\":\"Quad\"},{\"attributes\":{\"callback\":null},\"id\":\"51c6ffd2-1fde-4648-89ee-ee00cd39a672\",\"type\":\"DataRange1d\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_color\":{\"value\":\"#555555\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"73151402-98e3-4d15-b1c1-9905289b200d\",\"type\":\"Quad\"},{\"attributes\":{},\"id\":\"80d5047d-6cdd-4f06-a373-f044d2ccf874\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"841c781a-33a2-4267-b957-c0789a86bd74\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"841c781a-33a2-4267-b957-c0789a86bd74\",\"type\":\"BasicTicker\"}},\"id\":\"f46df637-6370-4ad2-8c4c-e660fbdafc69\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"4dc029e2-a490-4afe-9d77-8d9d7ee08260\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"e4009fd6-6d49-4acb-96da-1513ad73804e\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"1dcf734c-3ee8-4820-b255-a6d09b3f6630\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"73151402-98e3-4d15-b1c1-9905289b200d\",\"type\":\"Quad\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"aecb1e69-3dd3-4661-9a1c-d12b0e5174f4\",\"type\":\"Quad\"},\"selection_glyph\":null},\"id\":\"0498b1d6-9a31-4c8c-8f03-cf575e7040a5\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":{\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"5bead03e-6766-4265-abc1-96776fa9c176\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"plot\":{\"id\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"8d2223a7-1e09-4dec-bddc-b1ee2cf4c729\",\"type\":\"PanTool\"}],\"root_ids\":[\"419cc3b9-0bfc-4618-b282-20eddfbff215\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.4\"}}; var render_items = [{\"docid\":\"71ebf26b-1b16-419a-bfb4-fb3b12fe4b8e\",\"elementid\":\"b7f2e30a-9d83-4ebd-a216-d384f5ef713f\",\"modelid\":\"419cc3b9-0bfc-4618-b282-20eddfbff215\"}]; Bokeh.embed.embed_items(docs_json, render_items); }; if (document.readyState != \"loading\") fn(); else document.addEventListener(\"DOMContentLoaded\", fn); })(); }, function(Bokeh) { } ]; function run_inline_js() { if ((window.Bokeh !== undefined) || (force === true)) { for (var i = 0; i < inline_js.length; i++) { inline_js[i](window.Bokeh); }if (force === true) { display_loaded(); }} else if (Date.now() < window._bokeh_timeout) { setTimeout(run_inline_js, 100); } else if (!window._bokeh_failed_load) { console.log(\"Bokeh: BokehJS failed to load within specified timeout.\"); window._bokeh_failed_load = true; } else if (force !== true) { var cell = $(document.getElementById(\"b7f2e30a-9d83-4ebd-a216-d384f5ef713f\")).parents('.cell').data().cell; cell.output_area.append_execute_result(NB_LOAD_WARNING) } } if (window._bokeh_is_loading === 0) { console.log(\"Bokeh: BokehJS loaded, going straight to plotting\"); run_inline_js(); } else { load_libs(js_urls, function() { console.log(\"Bokeh: BokehJS plotting callback run at\", now()); run_inline_js(); }); } }(this)); 12 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231import timeimport sys# Let's tweak our network from before to model these phenomenaclass SentimentNetwork: def __init__(self, reviews,labels, min_count = 10, polarity_cutoff = 0.1, hidden_nodes = 10, learning_rate = 0.1): np.random.seed(1) self.pre_process_data(reviews, polarity_cutoff, min_count) self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate) def pre_process_data(self,reviews): review_vocab = set() for review in reviews: for word in review.split(\" \"): review_vocab.add(word) self.review_vocab = list(review_vocab) label_vocab = set() for label in labels: label_vocab.add(label) self.label_vocab = list(label_vocab) self.review_vocab_size = len(self.review_vocab) self.label_vocab_size = len(self.label_vocab) self.word2index = &#123;&#125; for i, word in enumerate(self.review_vocab): self.word2index[word] = i self.label2index = &#123;&#125; for i, label in enumerate(self.label_vocab): self.label2index[label] = i def pre_process_data(self, revies, polarity_cutoff, min_count): positive_counts = Counter() negative_counts = Counter() total_counts = Counter() for i in range(len(reviews)): if(labels[i] == 'POSITIVE'): for word in reviews[i].split(' '): positive_counts[word] += 1 total_counts[word] += 1 else: for word in reviews[i].split(' '): negative_counts[word] += 1 total_counts[word] += 1 pos_neg_ratios = Counter() for term, cnt in list(total_counts.most_common()): if(cnt &gt;= 50): pos_neg_ratio = positive_counts[term] / float(negative_counts[term] + 1) pos_neg_ratios[term] = pos_neg_ratio for word, ratio in pos_neg_ratios.most_common(): if(ratio &gt; 1): pos_neg_ratios[word] = np.log(ratio) else: pos_neg_ratios[word] = -np.log(1 / (ratio + 0.01)) review_vocab = set() for review in reviews: for word in review.split(' '): if(total_counts[word] &gt; min_count): if(word in pos_neg_ratios.keys()): if((pos_neg_ratios[word] &gt;= polarity_cutoff) or (pos_neg_ratios[word] &lt;= -polarity_cutoff)): review_vocab.add(word) else: review_vocab.add(word) self.review_vocab = list(review_vocab) label_vocab = set() for label in labels: label_vocab.add(label) self.label_vocab = list(label_vocab) self.review_vocab_size = len(self.review_vocab) self.label_vocab_size = len(self.label_vocab) self.word2index = &#123;&#125; for i, word in enumerate(self.review_vocab): self.word2index[word] = i self.label2index = &#123;&#125; for i, label in enumerate(self.label_vocab): self.label2index[label] = i def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate): # Set number of nodes in input, hidden and output layers. self.input_nodes = input_nodes self.hidden_nodes = hidden_nodes self.output_nodes = output_nodes # Initialize weights self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes)) self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes)) self.learning_rate = learning_rate self.layer_0 = np.zeros((1,input_nodes)) self.layer_1 = np.zeros((1,hidden_nodes)) def sigmoid(self,x): return 1 / (1 + np.exp(-x)) def sigmoid_output_2_derivative(self,output): return output * (1 - output) def update_input_layer(self,review): # clear out previous state, reset the layer to be all 0s self.layer_0 *= 0 for word in review.split(\" \"): self.layer_0[0][self.word2index[word]] = 1 def get_target_for_label(self,label): if(label == 'POSITIVE'): return 1 else: return 0 def train(self, training_reviews_raw, training_labels): training_reviews = list() for review in training_reviews_raw: indices = set() for word in review.split(\" \"): if(word in self.word2index.keys()): indices.add(self.word2index[word]) training_reviews.append(list(indices)) assert(len(training_reviews) == len(training_labels)) correct_so_far = 0 start = time.time() for i in range(len(training_reviews)): review = training_reviews[i] label = training_labels[i] #### Implement the forward pass here #### ### Forward pass ### # Input Layer # Hidden layer# layer_1 = self.layer_0.dot(self.weights_0_1) self.layer_1 *= 0 for index in review: self.layer_1 += self.weights_0_1[index] # Output layer layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2)) #### Implement the backward pass here #### ### Backward pass ### # Output error layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output. layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2) # Backpropagated error layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error # Update the weights self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step for index in review: self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate # update input-to-hidden weights with gradient descent step if(np.abs(layer_2_error) &lt; 0.5): correct_so_far += 1 reviews_per_second = i / float(time.time() - start) sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\") def test(self, testing_reviews, testing_labels): correct = 0 start = time.time() for i in range(len(testing_reviews)): pred = self.run(testing_reviews[i]) if(pred == testing_labels[i]): correct += 1 reviews_per_second = i / float(time.time() - start) sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\ + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\ + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\") def run(self, review): # Input Layer # Hidden layer self.layer_1 *= 0 unique_indices = set() for word in review.lower().split(\" \"): if word in self.word2index.keys(): unique_indices.add(self.word2index[word]) for index in unique_indices: self.layer_1 += self.weights_0_1[index] # Output layer layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2)) if(layer_2[0] &gt; 0.5): return \"POSITIVE\" else: return \"NEGATIVE\" 1mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], min_count=20, polarity_cutoff=0.05, learning_rate=0.01) 1mlp.train(reviews[:-1000],labels[:-1000]) Progress:99.9% Speed(reviews/sec):2550. #Correct:20282 #Trained:24000 Training Accuracy:84.5% 1mlp.test(reviews[-1000:],labels[-1000:]) Progress:99.9% Speed(reviews/sec):3239. #Correct:855 #Tested:1000 Testing Accuracy:85.5% 1mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], min_count=20, polarity_cutoff=0.8, learning_rate=0.01) 1mlp.train(reviews[:-1000],labels[:-1000]) Progress:99.9% Speed(reviews/sec):2566. #Correct:20282 #Trained:24000 Training Accuracy:84.5% 1mlp.test(reviews[-1000:],labels[-1000:]) Progress:99.9% Speed(reviews/sec):3238. #Correct:855 #Tested:1000 Testing Accuracy:85.5% 博客地址：52ml.me","tags":[{"name":"纳米学位","slug":"纳米学位","permalink":"http://52ml.me/tags/纳米学位/"},{"name":"DLND","slug":"DLND","permalink":"http://52ml.me/tags/DLND/"},{"name":"机器学习","slug":"机器学习","permalink":"http://52ml.me/tags/机器学习/"}]},{"title":"Predicting daily bike rental ridership","date":"2017-02-05T05:36:30.000Z","path":"2017/02/05/Predicting-daily-bike-rental-ridership/","text":"Your first neural networkIn this project, you’ll build your first neural network and use it to predict daily bike rental ridership. We’ve provided some of the code, but left the implementation of the neural network up to you (for the most part). After you’ve submitted this project, feel free to explore the data and the model more. 123456%matplotlib inline%config InlineBackend.figure_format = 'retina'import numpy as npimport pandas as pdimport matplotlib.pyplot as plt Load and prepare the dataA critical step in working with neural networks is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights. Below, we’ve written the code to load and prepare the data. You’ll learn more about this soon! 123data_path = 'Bike-Sharing-Dataset/hour.csv'rides = pd.read_csv(data_path) 1rides.head() instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 Checking out the dataThis dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the cnt column. You can see the first few rows of the data above. Below is a plot showing the number of bike riders over the first 10 days in the data set. You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You’ll be trying to capture all this with your model. 1rides[:24*10].plot(x='dteday', y='cnt') &lt;matplotlib.axes._subplots.AxesSubplot at 0x10878d780&gt; Dummy variablesHere we have some categorical variables like season, weather, month. To include these in our model, we’ll need to make binary dummy variables. This is simple to do with Pandas thanks to get_dummies(). 123456789dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']for each in dummy_fields: dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False) rides = pd.concat([rides, dummies], axis=1)fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', 'weekday', 'atemp', 'mnth', 'workingday', 'hr']data = rides.drop(fields_to_drop, axis=1)data.head() yr holiday temp hum windspeed casual registered cnt season_1 season_2 … hr_21 hr_22 hr_23 weekday_0 weekday_1 weekday_2 weekday_3 weekday_4 weekday_5 weekday_6 0 0 0 0.24 0.81 0.0 3 13 16 1 0 … 0 0 0 0 0 0 0 0 0 1 1 0 0 0.22 0.80 0.0 8 32 40 1 0 … 0 0 0 0 0 0 0 0 0 1 2 0 0 0.22 0.80 0.0 5 27 32 1 0 … 0 0 0 0 0 0 0 0 0 1 3 0 0 0.24 0.75 0.0 3 10 13 1 0 … 0 0 0 0 0 0 0 0 0 1 4 0 0 0.24 0.75 0.0 0 1 1 1 0 … 0 0 0 0 0 0 0 0 0 1 5 rows × 59 columns Scaling target variablesTo make training the network easier, we’ll standardize each of the continuous variables. That is, we’ll shift and scale the variables such that they have zero mean and a standard deviation of 1. The scaling factors are saved so we can go backwards when we use the network for predictions. 1234567quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']# Store scalings in a dictionary so we can convert back laterscaled_features = &#123;&#125;for each in quant_features: mean, std = data[each].mean(), data[each].std() scaled_features[each] = [mean, std] data.loc[:, each] = (data[each] - mean)/std Splitting the data into training, testing, and validation setsWe’ll save the last 21 days of the data to use as a test set after we’ve trained the network. We’ll use this set to make predictions and compare them with the actual number of riders. 12345678# Save the last 21 days test_data = data[-21*24:]data = data[:-21*24]# Separate the data into features and targetstarget_fields = ['cnt', 'casual', 'registered']features, targets = data.drop(target_fields, axis=1), data[target_fields]test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields] We’ll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we’ll train on historical data, then try to predict on future data (the validation set). 123# Hold out the last 60 days of the remaining data as a validation settrain_features, train_targets = features[:-60*24], targets[:-60*24]val_features, val_targets = features[-60*24:], targets[-60*24:] Time to build the networkBelow you’ll build your network. We’ve built out the structure and the backwards pass. You’ll implement the forward pass through the network. You’ll also set the hyperparameters: the learning rate, the number of hidden units, and the number of training passes. The network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is $f(x)=x$. A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called forward propagation. We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called backpropagation. Hint: You’ll need the derivative of the output activation function ($f(x) = x$) for the backpropagation implementation. If you aren’t familiar with calculus, this function is equivalent to the equation $y = x$. What is the slope of that equation? That is the derivative of $f(x)$. Below, you have these tasks: Implement the sigmoid function to use as the activation function. Set self.activation_function in __init__ to your sigmoid function. Implement the forward pass in the train method. Implement the backpropagation algorithm in the train method, including calculating the output error. Implement the forward pass in the run method. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class NeuralNetwork(object): def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate): # Set number of nodes in input, hidden and output layers. self.input_nodes = input_nodes self.hidden_nodes = hidden_nodes self.output_nodes = output_nodes # Initialize weights self.weights_input_to_hidden = np.random.normal(0.0, self.hidden_nodes**-0.5, (self.hidden_nodes, self.input_nodes)) self.weights_hidden_to_output = np.random.normal(0.0, self.output_nodes**-0.5, (self.output_nodes, self.hidden_nodes)) self.lr = learning_rate #### Set this to your implemented sigmoid function #### # Activation function is the sigmoid function self.activation_function = lambda x: (1/(1+np.exp(-x))) def train(self, inputs_list, targets_list): # Convert inputs list to 2d array inputs = np.array(inputs_list, ndmin=2).T targets = np.array(targets_list, ndmin=2).T #### Implement the forward pass here #### ### Forward pass ### # TODO: Hidden layer hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)# signals into hidden layer hidden_outputs = self.activation_function(hidden_inputs)# signals from hidden layer #print(hidden_inputs) # TODO: Output layer final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)# signals into final output layer final_outputs = final_inputs# signals from final output layer #### Implement the backward pass here #### ### Backward pass ### # TODO: Output error output_errors = (targets - final_outputs) * 1# Output layer error is the difference between desired target and actual output. # TODO: Backpropagated error hidden_errors = np.dot(self.weights_hidden_to_output.T, output_errors)# errors propagated to the hidden layer hidden_grad = hidden_outputs * (1 - hidden_outputs)# hidden layer gradients # TODO: Update the weights self.weights_hidden_to_output += self.lr * np.dot(output_errors, hidden_outputs.T)# update hidden-to-output weights with gradient descent step self.weights_input_to_hidden += self.lr * np.dot(hidden_errors * hidden_grad, inputs.T)# update input-to-hidden weights with gradient descent step def run(self, inputs_list): # Run a forward pass through the network inputs = np.array(inputs_list, ndmin=2).T #### Implement the forward pass here #### # TODO: Hidden layer hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)# signals into hidden layer hidden_outputs = self.activation_function(hidden_inputs)# signals from hidden layer # TODO: Output layer final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)# signals into final output layer final_outputs = final_inputs# signals from final output layer return final_outputs 12def MSE(y, Y): return np.mean((y-Y)**2) Training the networkHere you’ll set the hyperparameters for the network. The strategy here is to find hyperparameters such that the error on the training set is low, but you’re not overfitting to the data. If you train the network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. That is, the loss on the validation set will start increasing as the training set loss drops. You’ll also be using a method know as Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently. You’ll learn more about SGD later. Choose the number of epochsThis is the number of times the dataset will pass through the network, each time updating the weights. As the number of epochs increases, the network becomes better and better at predicting the targets in the training set. You’ll need to choose enough epochs to train the network well but not too many or you’ll be overfitting. Choose the learning rateThis scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. A good choice to start at is 0.1. If the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge. Choose the number of hidden nodesThe more hidden nodes you have, the more accurate predictions the model will make. Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden units is too low, then the model won’t have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden units you choose. 12345678910111213141516171819202122232425262728import sys### Set the hyperparameters here ###epochs = 2000learning_rate = 0.01hidden_nodes = 20output_nodes = 1N_i = train_features.shape[1]network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)losses = &#123;'train':[], 'validation':[]&#125;for e in range(epochs): # Go through a random batch of 128 records from the training data set batch = np.random.choice(train_features.index, size=128) for record, target in zip(train_features.ix[batch].values, train_targets.ix[batch]['cnt']): network.train(record, target) # Printing out the training progress train_loss = MSE(network.run(train_features), train_targets['cnt'].values) val_loss = MSE(network.run(val_features), val_targets['cnt'].values) sys.stdout.write(\"\\rProgress: \" + str(100 * e/float(epochs))[:4] \\ + \"% ... Training loss: \" + str(train_loss)[:5] \\ + \" ... Validation loss: \" + str(val_loss)[:5]) losses['train'].append(train_loss) losses['validation'].append(val_loss) Progress: 99.9% ... Training loss: 0.071 ... Validation loss: 0.249 1234plt.plot(losses['train'], label='Training loss')plt.plot(losses['validation'], label='Validation loss')plt.legend()#plt.ylim(ymax=0.5) &lt;matplotlib.legend.Legend at 0x110c93898&gt; Check out your predictionsHere, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly. 12345678910111213fig, ax = plt.subplots(figsize=(8,4))mean, std = scaled_features['cnt']predictions = network.run(test_features)*std + meanax.plot(predictions[0], label='Prediction')ax.plot((test_targets['cnt']*std + mean).values, label='Data')ax.set_xlim(right=len(predictions))ax.legend()dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])dates = dates.apply(lambda d: d.strftime('%b %d'))ax.set_xticks(np.arange(len(dates))[12::24])_ = ax.set_xticklabels(dates[12::24], rotation=45) Thinking about your resultsAnswer these questions about your results. How well does the model predict the data? Where does it fail? Why does it fail where it does? Note: You can edit the text in this cell by double clicking on it. When you want to render the text, press control + enter Your answer belowIt looks good in the plot. But there are some errors. At some points, the prediction is negative. It’s unresonable. And it predicts very bad from Dec 22. Maybe it’s because the datas from Dec 22 are unusual. Unit testsRun these unit tests to check the correctness of your network implementation. These tests must all be successful to pass the project. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import unittestinputs = [0.5, -0.2, 0.1]targets = [0.4]test_w_i_h = np.array([[0.1, 0.4, -0.3], [-0.2, 0.5, 0.2]])test_w_h_o = np.array([[0.3, -0.1]])class TestMethods(unittest.TestCase): ########## # Unit tests for data loading ########## def test_data_path(self): # Test that file path to dataset has been unaltered self.assertTrue(data_path.lower() == 'bike-sharing-dataset/hour.csv') def test_data_loaded(self): # Test that data frame loaded self.assertTrue(isinstance(rides, pd.DataFrame)) ########## # Unit tests for network functionality ########## def test_activation(self): network = NeuralNetwork(3, 2, 1, 0.5) # Test that the activation function is a sigmoid self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5)))) def test_train(self): # Test that weights are updated correctly on training network = NeuralNetwork(3, 2, 1, 0.5) network.weights_input_to_hidden = test_w_i_h.copy() network.weights_hidden_to_output = test_w_h_o.copy() network.train(inputs, targets) self.assertTrue(np.allclose(network.weights_hidden_to_output, np.array([[ 0.37275328, -0.03172939]]))) self.assertTrue(np.allclose(network.weights_input_to_hidden, np.array([[ 0.10562014, 0.39775194, -0.29887597], [-0.20185996, 0.50074398, 0.19962801]]))) def test_run(self): # Test correctness of run method network = NeuralNetwork(3, 2, 1, 0.5) network.weights_input_to_hidden = test_w_i_h.copy() network.weights_hidden_to_output = test_w_h_o.copy() self.assertTrue(np.allclose(network.run(inputs), 0.09998924))suite = unittest.TestLoader().loadTestsFromModule(TestMethods())unittest.TextTestRunner().run(suite) ..... ---------------------------------------------------------------------- Ran 5 tests in 0.004s OK &lt;unittest.runner.TextTestResult run=5 errors=0 failures=0&gt; 12 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[{"name":"纳米学位","slug":"纳米学位","permalink":"http://52ml.me/tags/纳米学位/"},{"name":"DLND","slug":"DLND","permalink":"http://52ml.me/tags/DLND/"},{"name":"机器学习","slug":"机器学习","permalink":"http://52ml.me/tags/机器学习/"}]},{"title":"预测波士顿房价","date":"2017-01-21T07:02:02.000Z","path":"2017/01/21/预测波士顿房价/","text":"模型评价与验证项目 1: 预测波士顿房价欢迎来到机器学习工程师纳米学位的第一个项目！在此文件中，有些示例代码已经提供给你，但你还需要实现更多的功能来让项目成功运行。除非有明确要求，你无须修改任何已给出的代码。以‘练习’开始的标题表示接下来的内容中有需要你必须实现的功能。每一部分都会有详细的指导，需要实现的部分也会在注释中以‘TODO’标出。请仔细阅读所有的提示！ 除了实现代码外，你还必须回答一些与项目和实现有关的问题。每一个需要你回答的问题都会以‘问题 X’为标题。请仔细阅读每个问题，并且在问题后的‘回答’文字框中写出完整的答案。你的项目将会根据你对问题的回答和撰写代码所实现的功能来进行评分。 提示：Code 和 Markdown 区域可通过 Shift + Enter 快捷键运行。此外，Markdown可以通过双击进入编辑模式。 开始 在这个项目中，你将利用马萨诸塞州波士顿郊区的房屋信息数据训练和测试一个模型，并对模型的性能和预测能力进行测试。通过该数据训练后的好的模型可以被用来对房屋做特定预测—尤其是对房屋的价值。对于房地产经纪等人的日常工作来说，这样的预测模型被证明非常有价值。 此项目的数据集来自UCI机器学习知识库。波士顿房屋这些数据于1978年开始统计，共506个数据点，涵盖了麻省波士顿不同郊区房屋14种特征的信息。本项目对原始数据集做了以下处理： 有16个&#39;MEDV&#39; 值为50.0的数据点被移除。 这很可能是由于这些数据点包含遗失或看不到的值。 有1个数据点的 &#39;RM&#39; 值为8.78. 这是一个异常值，已经被移除。 对于本项目，房屋的&#39;RM&#39;， &#39;LSTAT&#39;，&#39;PTRATIO&#39;以及&#39;MEDV&#39;特征是必要的，其余不相关特征已经被移除。 &#39;MEDV&#39;特征的值已经过必要的数学转换，可以反映35年来市场的通货膨胀效应。 运行下面区域的代码以载入波士顿房屋数据集，以及一些此项目所需的Python库。如果成功返回数据集的大小，表示数据集已载入成功。 1234567891011121314151617181920# Import libraries necessary for this project# 载入此项目所需要的库import numpy as npimport pandas as pdimport visuals as vs # Supplementary codefrom sklearn.cross_validation import ShuffleSplit# Pretty display for notebooks# 让结果在notebook中显示%matplotlib inline# Load the Boston housing dataset# 载入波士顿房屋的数据集data = pd.read_csv('housing.csv')prices = data['MEDV']features = data.drop('MEDV', axis = 1) # Success# 完成print \"Boston housing dataset has &#123;&#125; data points with &#123;&#125; variables each.\".format(*data.shape) Boston housing dataset has 489 data points with 4 variables each. 分析数据在项目的第一个部分，你会对波士顿房地产数据进行初步的观察并给出你的分析。通过对数据的探索来熟悉数据可以让你更好地理解和解释你的结果。 由于这个项目的最终目标是建立一个预测房屋价值的模型，我们需要将数据集分为特征(features)和目标变量(target variable)。特征 &#39;RM&#39;， &#39;LSTAT&#39;，和 &#39;PTRATIO&#39;，给我们提供了每个数据点的数量相关的信息。目标变量：&#39;MEDV&#39;，是我们希望预测的变量。他们分别被存在features和prices两个变量名中。 练习：基础统计运算你的第一个编程练习是计算有关波士顿房价的描述统计数据。我们已为你导入了numpy，你需要使用这个库来执行必要的计算。这些统计数据对于分析模型的预测结果非常重要的。在下面的代码中，你要做的是： 计算prices中的&#39;MEDV&#39;的最小值、最大值、均值、中值和标准差； 将运算结果储存在相应的变量中。 123456789101112131415161718192021222324252627282930# TODO: Minimum price of the data#目标：计算价值的最小值minimum_price = np.min(prices)# TODO: Maximum price of the data#目标：计算价值的最大值maximum_price = np.max(prices)# TODO: Mean price of the data#目标：计算价值的平均值mean_price = np.mean(prices)# TODO: Median price of the data#目标：计算价值的中值print type(prices)median_price = np.median(prices)# TODO: Standard deviation of prices of the data#目标：计算价值的标准差std_price = np.std(prices)# Show the calculated statistics#目标：输出计算的结果print \"Statistics for Boston housing dataset:\\n\"print \"Minimum price: $&#123;:,.2f&#125;\".format(minimum_price)print \"Maximum price: $&#123;:,.2f&#125;\".format(maximum_price)print \"Mean price: $&#123;:,.2f&#125;\".format(mean_price)print \"Median price $&#123;:,.2f&#125;\".format(median_price)print \"Standard deviation of prices: $&#123;:,.2f&#125;\".format(std_price) &lt;class &apos;pandas.core.series.Series&apos;&gt; Statistics for Boston housing dataset: Minimum price: $105,000.00 Maximum price: $1,024,800.00 Mean price: $454,342.94 Median price $438,900.00 Standard deviation of prices: $165,171.13 问题1 - 特征观察如前文所述，本项目中我们关注的是其中三个值:&#39;RM&#39;、&#39;LSTAT&#39; 和&#39;PTRATIO&#39;，对每一个数据点: &#39;RM&#39; 是该地区中每个房屋的平均房间数量； &#39;LSTAT&#39; 是指该地区有多少百分比的房东属于是低收入阶层（有工作但收入微薄）； &#39;PTRATIO&#39; 是该地区的中学和小学里，学生和老师的数目比（学生/老师）。 凭直觉，上述三个特征中对每一个来说，你认为增大该特征的数值，&#39;MEDV&#39;的值会是增大还是减小呢？每一个答案都需要你给出理由。 提示：你预期一个&#39;RM&#39; 值是6的房屋跟&#39;RM&#39; 值是7的房屋相比，价值更高还是更低呢？ 回答: 增大RM，MEDV会增大，因为房间多了，房子的总面积会增大，因此MEDV会增大 增大LATAT，MEDV会减小，因为低收入阶层多了，该地区的消费能力就会降低，因此MEDV会减少 增大PTRATIO，MEDV会减小，因为学生多老师少，说明该地区教育投入有限，家长不愿意让小孩在这个地方上学，因此MEDV会减少 建模在项目的第二部分中，你需要了解必要的工具和技巧来让你的模型进行预测。用这些工具和技巧对每一个模型的表现做精确的衡量可以极大地增强你预测的信心。 练习：定义衡量标准如果不能对模型的训练和测试的表现进行量化地评估，我们就很难衡量模型的好坏。通常我们会定义一些衡量标准，这些标准可以通过对某些误差或者拟合程度的计算来得到。在这个项目中，你将通过运算决定系数R2 来量化模型的表现。模型的决定系数是回归分析中十分常用的统计信息，经常被当作衡量模型预测能力好坏的标准。 R2的数值范围从0至1，表示目标变量的预测值和实际值之间的相关程度平方的百分比。一个模型的R2 值为0还不如直接用平均值来预测效果好；而一个R2 值为1的模型则可以对目标变量进行完美的预测。从0至1之间的数值，则表示该模型中目标变量中有百分之多少能够用特征来解释。_模型也可能出现负值的R2，这种情况下模型所做预测有时会比直接计算目标变量的平均值差很多。 在下方代码的 performance_metric 函数中，你要实现： 使用 sklearn.metrics 中的 r2_score 来计算 y_true 和 y_predict的R2值，作为对其表现的评判。 将他们的表现评分储存到score变量中。 1234567891011# TODO: Import 'r2_score'from sklearn.metrics import r2_scoredef performance_metric(y_true, y_predict): \"\"\" Calculates and returns the performance score between true and predicted values based on the metric chosen. \"\"\" # TODO: Calculate the performance score between 'y_true' and 'y_predict' score = r2_score(y_true, y_predict) # Return the score return score 问题2 - 拟合程度假设一个数据集有五个数据且一个模型做出下列目标变量的预测： 真实数值 预测数值 3.0 2.5 -0.5 0.0 2.0 2.1 7.0 7.8 4.2 5.3 你会觉得这个模型已成功地描述了目标变量的变化吗？如果成功，请解释为什么，如果没有，也请给出原因。 运行下方的代码，使用performance_metric函数来计算模型的决定系数。 123# Calculate the performance of this modelscore = performance_metric([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])print \"Model has a coefficient of determination, R^2, of &#123;:.3f&#125;.\".format(score) Model has a coefficient of determination, R^2, of 0.923. 回答: 已经成功的描述了目标变量的变化，因为决定系数已经非常高了。 练习: 数据分割与重排接下来，你需要把波士顿房屋数据集分成训练和测试两个子集。通常在这个过程中，数据也会被重新排序，以消除数据集中由于排序而产生的偏差。在下面的代码中，你需要： 使用 sklearn.cross_validation 中的 train_test_split， 将features和prices的数据都分成用于训练的数据子集和用于测试的数据子集。 分割比例为：80%的数据用于训练，20%用于测试； 选定一个数值以设定 train_test_split 中的 random_state ，这会确保结果的一致性； 最终分离出的子集为X_train,X_test,y_train,和y_test。 12345678# TODO: Import 'train_test_split'from sklearn.cross_validation import train_test_split# TODO: Shuffle and split the data into training and testing subsetsX_train, X_test, y_train, y_test = train_test_split(features, prices, test_size=0.2, random_state=30)# Successprint \"Training and testing split was successful.\" Training and testing split was successful. 问题 3- 训练及测试将数据集按一定比例分为训练用的数据集和测试用的数据集对学习算法有什么好处？ 提示： 如果没有数据来对模型进行测试，会出现什么问题？ 答案: 测试可以用来评估模型对未知数据的预测能力。如果没有进行测试，就无法得知对未知数据的预测是否可靠。 分析模型的表现在项目的第三部分，我们来看一下几个模型针对不同的数据集在学习和测试上的表现。另外，你需要专注于一个特定的算法，用全部训练集训练时，提高它的&#39;max_depth&#39; 参数，观察这一参数的变化如何影响模型的表现。把你模型的表现画出来对于分析过程十分有益。可视化可以让我们看到一些单看结果看不到的行为。 学习曲线下方区域内的代码会输出四幅图像，它们是一个决策树模型在不同最大深度下的表现。每一条曲线都直观的显示了随着训练数据量的增加，模型学习曲线的训练评分和测试评分的变化。注意，曲线的阴影区域代表的是该曲线的不确定性（用标准差衡量）。这个模型的训练和测试部分都使用决定系数R2来评分。 运行下方区域中的代码，并利用输出的图形回答下面的问题。 12# Produce learning curves for varying training set sizes and maximum depthsvs.ModelLearning(features, prices) /Users/vincent/anaconda/envs/dato-env/lib/python2.7/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison if self._edgecolors == str(&apos;face&apos;): 问题 4 - 学习数据选择上述图像中的其中一个，并给出其最大深度。随着训练数据量的增加，训练曲线的评分有怎样的变化？测试曲线呢？如果有更多的训练数据，是否能有效提升模型的表现呢？提示：学习曲线的评分是否最终会收敛到特定的值？ 答案: max_depth=3时，开始随着训练数据量增加，训练数据0-50时，训练曲线的评分逐渐减少，测试曲线开始阶段增加很快；训练数据50之后，测试曲线逐渐趋于某个特定的数值0.8。数据量足够大时再增加训练数据不能有效提升模型的表现。 复杂度曲线下列代码内的区域会输出一幅图像，它展示了一个已经经过训练和验证的决策树模型在不同最大深度条件下的表现。这个图形将包含两条曲线，一个是训练的变化，一个是测试的变化。跟学习曲线相似，阴影区域代表该曲线的不确定性，模型训练和测试部分的评分都用的 performance_metric 函数。 运行下方区域中的代码，并利用输出的图形并回答下面的两个问题。 1vs.ModelComplexity(X_train, y_train) 问题 5- 偏差与方差之间的权衡取舍当模型以最大深度 1训练时，模型的预测是出现很大的偏差还是出现了很大的方差？当模型以最大深度10训练时，情形又如何呢？图形中的哪些特征能够支持你的结论？ 提示： 你如何得知模型是否出现了偏差很大或者方差很大的问题？ 答案: 最大深度1训练时，出现了很大的偏差，因为评分最低阴影区域相对集中并且训练和测试得分都不高，最大深度10训练时，出现了很大的方差，因为评分相对比较高，但是阴影区域范围大，并且训练得分和测试得分相差比较大。 问题 6- 最优模型的猜测你认为最大深度是多少的模型能够最好地对未见过的数据进行预测？为什么你会得出了这个答案？ 答案: 最大深度为4时可以最好的对未见过的数据进行预测，因为这是评分最高，而且阴影区域波动相对较小。 评价模型表现在这个项目的最后，你将自己建立模型，并使用最优化的fit_model函数，基于客户房子的特征来预测该房屋的价值。 问题 7- 网格搜索（Grid Search）什么是网格搜索法？如何用它来优化学习算法？ 回答: 用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。 问题 8- 交叉验证什么是K折交叉验证法（k-fold cross-validation）？优化模型时，使用这种方法对网格搜索有什么好处？ 提示： 跟为何需要一组训练集的原因差不多，网格搜索时如果不使用交叉验证会有什么问题？ 答案: 将数据集分成K份，每一次训练都选取其中一份用来做测试，其他用来训练，训练K次，最后求出平均值。可以选择最佳的参数组合。K折交叉验证法可以验证网格搜索到的参数是否可以达到最佳效果。 练习：训练模型在最后一个练习中，你将需要将所学到的内容整合，使用决策树演算法训练一个模型。为了保证你得出的是一个最优模型，你需要使用网格搜索法训练模型，以找到最佳的 &#39;max_depth&#39; 参数。你可以把&#39;max_depth&#39; 参数理解为决策树算法在做出预测前，允许其对数据提出问题的数量。决策树是监督学习算法中的一种。 此外，你会发现你的实现使用的是 ShuffleSplit() 。它也是交叉验证的一种方式（见变量 &#39;cv_sets&#39;）。虽然这不是问题8中描述的 K-Fold 交叉验证，这个教程验证方法也很有用！这里 ShuffleSplit() 会创造十个混洗过的集合，每个集合中20%(&#39;test_size&#39;)的数据会被用作验证集。当你在实现的时候，想一想这跟 K-Fold 交叉验证有哪些相同点，哪些不同点？ 在下方 fit_model 函数中，你需要做的是： 使用 sklearn.tree 中的 DecisionTreeRegressor 创建一个决策树的回归函数； 将这个回归函数储存到 &#39;regressor&#39; 变量中； 为 &#39;max_depth&#39; 创造一个字典，它的值是从1至10的数组，并储存到 &#39;params&#39; 变量中； 使用 sklearn.metrics 中的 make_scorer 创建一个评分函数； 将 performance_metric 作为参数传至这个函数中； 将评分函数储存到 &#39;scoring_fnc&#39; 变量中； 使用 sklearn.grid_search 中的 GridSearchCV 创建一个网格搜索对象； 将变量&#39;regressor&#39;, &#39;params&#39;, &#39;scoring_fnc&#39;, 和 &#39;cv_sets&#39; 作为参数传至这个对象中； 将 GridSearchCV 存到 &#39;grid&#39; 变量中。 1234567891011121314151617181920212223242526272829# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'from sklearn.tree import DecisionTreeRegressorfrom sklearn.metrics import make_scorerfrom sklearn.grid_search import GridSearchCVdef fit_model(X, y): \"\"\" Performs grid search over the 'max_depth' parameter for a decision tree regressor trained on the input data [X, y]. \"\"\" # Create cross-validation sets from the training data cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0) # TODO: Create a decision tree regressor object regressor = DecisionTreeRegressor(random_state=30) # TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10 params = &#123;'max_depth':[i+1 for i in range(10)]&#125; # TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' scoring_fnc = make_scorer(performance_metric) # TODO: Create the grid search object grid = GridSearchCV(regressor, params, scoring=scoring_fnc, cv=cv_sets) # Fit the grid search object to the data to compute the optimal model grid = grid.fit(X, y) # Return the optimal model after fitting the data return grid.best_estimator_ 做出预测当我们用数据训练出一个模型，它现在就可用于对新的数据进行预测。在决策树回归函数中，模型已经学会对新输入的数据提问，并返回对目标变量的预测值。你可以用这个预测来获取数据未知目标变量的信息，这些数据必须是不包含在训练数据之内的。 问题 9- 最优模型最优模型的最大深度（maximum depth）是多少？此答案与你在问题 6所做的猜测是否相同？ 运行下方区域内的代码，将决策树回归函数代入训练数据的集合，以得到最优化的模型。 12345# Fit the training data to the model using grid searchreg = fit_model(X_train, y_train)# Produce the value for 'max_depth'print \"Parameter 'max_depth' is &#123;&#125; for the optimal model.\".format(reg.get_params()['max_depth']) Parameter &apos;max_depth&apos; is 4 for the optimal model. Answer: 4，与问题6中的猜测相同。 问题 10 - 预测销售价格想像你是一个在波士顿地区的房屋经纪人，并期待使用此模型以帮助你的客户评估他们想出售的房屋。你已经从你的三个客户收集到以下的资讯: 特征 客戶 1 客戶 2 客戶 3 房屋内房间总数 5 间房间 4 间房间 8 间房间 社区贫困指数（％被认为是贫困阶层） 17% 32% 3% 邻近学校的学生-老师比例 15：1 22：1 12：1 你会建议每位客户的房屋销售的价格为多少？从房屋特征的数值判断，这样的价格合理吗？ 提示：用你在分析数据部分计算出来的统计信息来帮助你证明你的答案。 运行下列的代码区域，使用你优化的模型来为每位客户的房屋价值做出预测。 12345678# Produce a matrix for client dataclient_data = [[5, 17, 15], # Client 1 [4, 32, 22], # Client 2 [8, 3, 12]] # Client 3# Show predictionsfor i, price in enumerate(reg.predict(client_data)): print \"Predicted selling price for Client &#123;&#125;'s home: $&#123;:,.2f&#125;\".format(i+1, price) Predicted selling price for Client 1&apos;s home: $409,752.00 Predicted selling price for Client 2&apos;s home: $220,886.84 Predicted selling price for Client 3&apos;s home: $937,650.00 答案: 客户1 409,752.00 合理，有5间房间，社会贫困指数不到1/5，学生老师比中等水平 客户2 220,886.84 合理，只有4间房间，社会贫困指数比较高，学生老师比最高，该地区对教育投入有限，家长不愿意在这里买房 客户3 937,650.00 合理，有8间房，贫困指数非常低，学生老师比最低，该地区应该是富人区，所以房间贵 敏感度一个最优的模型不一定是一个健壮模型。有的时候模型会过于复杂或者过于简单，以致于难以泛化新增添的数据；有的时候模型采用的学习算法并不适用于特定的数据结构；有的时候样本本身可能有太多噪点或样本过少，使得模型无法准确地预测目标变量。这些情况下我们会说模型是欠拟合的。执行下方区域中的代码，采用不同的训练和测试集执行 fit_model 函数10次。注意观察对一个特定的客户来说，预测是如何随训练数据的变化而变化的。 1vs.PredictTrials(features, prices, fit_model, client_data) Trial 1: $391,183.33 Trial 2: $411,417.39 Trial 3: $415,800.00 Trial 4: $428,316.00 Trial 5: $413,334.78 Trial 6: $411,931.58 Trial 7: $399,663.16 Trial 8: $407,232.00 Trial 9: $402,531.82 Trial 10: $413,700.00 Range in prices: $37,132.67 问题 11 - 实用性探讨简单地讨论一下你建构的模型能否在现实世界中使用？ 提示： 回答几个问题： 1978年所采集的数据，在今天是否仍然适用？ 数据中呈现的特征是否足够描述一个房屋？ 模型是否足够健壮来保证预测的一致性？ 在波士顿这样的大都市采集的数据，能否应用在其它乡镇地区？ 答案: 不能够，要考虑到通货膨胀等因素 不够，还要有房子的新旧程度，房子面子，是否有车库游泳池等 可以保证一致性，因为波动范围比较小 不适用其他乡镇地区 综上，该模型不适合在现实世界中使用 12 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[{"name":"纳米学位","slug":"纳米学位","permalink":"http://52ml.me/tags/纳米学位/"},{"name":"机器学习","slug":"机器学习","permalink":"http://52ml.me/tags/机器学习/"},{"name":"MLND","slug":"MLND","permalink":"http://52ml.me/tags/MLND/"}]},{"title":"Android 彩蛋路径","date":"2016-01-28T06:59:11.000Z","path":"2016/01/28/Android-彩蛋路径/","text":"彩蛋路径是1android/frameworks/base/packages/SystemUI/src/com/android/systemui/egg 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[]},{"title":"some tips","date":"2016-01-22T07:57:24.000Z","path":"2016/01/22/some-tips/","text":"adb模拟点击ADB中通过input来实现，用于输入touch，key等事件：The sources are: trackball joystick touchnavigation mouse keyboard gamepad touchpad dpad stylus touchscreenThe commands and default sources are: text (Default: touchscreen) keyevent [–longpress] … (Default: keyboard) tap (Default: touchscreen) swipe [duration(ms)] (Default: touchscreen) press (Default: trackball) roll (Default: trackball)1adb shell input touchscreen tap 10 10 adb+python截图screencap 本身支持标准输出，所以可以用管道符链接。但是 adb shell 会将结果中的 LF 转换为 CR+LF，会将 png 的格式破坏。于是这里将LF前的CR移除。12adb shell screencap -p | sed &apos;s/\\r$//&apos; &gt; screen.pngadb shell screencap -p | perl -pe &apos;s/\\x0D\\x0A/\\x0A/g&apos; &gt; screen.png 123import subprocessreturnValue = subprocess.check_output([\"adb\", \"shell\", \"screencap\", \"-p\"])returnValue.replace('\\x0D\\x0A', '\\x0A') 1234567891011121314151617#!/usr/bin/pythonimport subprocessimport timefor i in range(5): tic = time.time() subprocess.call([\"adb\", \"shell\", \"input\", \"touchscreen\", \"tap\", \"10\", \"10\"]) toc = time.time() tic1 = time.time() returnValue = subprocess.check_output([\"adb\", \"shell\", \"screencap\", \"-p\"]).replace('\\x0D\\x0A', '\\x0A') toc1 = time.time() print 'time = ', toc-tic print 'time1 = ', toc1-tic1 #returnValue = returnValue.replace('\\x0D\\x0A', '\\x0A') fileName = 'screenshot-%d.png' % i screenshot = open(fileName, 'w') screenshot.write(returnValue) screenshot.close() 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[{"name":"tips","slug":"tips","permalink":"http://52ml.me/tags/tips/"}]},{"title":"Image Classification -- Data-driven Approach, k-Nearest Neighbor, train/val/test splits","date":"2016-01-16T14:04:43.000Z","path":"2016/01/16/classification/","text":"前言本文翻译的是这篇CS231n Convolutional Neural Networks for Visual Recognition文章。 图像分类原因 这一节我们将介绍图像分类问题，这是一个从一组固定类别中指定输入图像标签的任务。这是计算机视觉中的一个核心问题，尽管它简单，在实际中有各种各样的应用。此外，我们会在使用过程中看到，许多其他看似不同的计算机视觉任务（如目标检测，分割）可以归结为图像分类。 例子 例如，下图中一个图像分类模型获取一张图片然后指定四个标签的{cat, dog, hat, mug}概率。如图所示，请记住，图像在计算机中表示为一个大的三维数组。在这个例子中，猫的图片宽度为248像素，高度为400像素，并且有三个颜色通道：Red，Green，Blue（或者缩写为RGB）。因此，图像由248×400×3或总共297600个数字组成。每个数字是一个整数，范围从0（黑色）到255（白色）。我们的任务是将百万数据中得四分之一标记为单一的标签，如“猫”。 图像分类的任务是对于一个给定的图像预测一个标签（或这里显示的不同标签下概率分布，用以表示我们的信任）。图像是整数三维数组，整数的范围从0到255，图像的大小是宽度x高度x3。3表示三个颜色通道：Red，Green，Blue。 挑战 由于识别视觉概念（如猫）这个任务对于人来说非常容易做到，这是一个值得思考的涉及到计算机视觉算法视角的挑战。正如我们以下提出的一系列挑战，请记住，图像的原始表示是一个三维阵列: 视点变化 一个物体的可以多个角度观察。 比例变化 可见的物体通常在尺寸上表现出变化（现实世界中的尺寸，不仅是其在图像中的区域）。 形变 许多关注的对象不是刚体，可以以极端的方式变形。 遮挡 关注的对象会被遮挡。有时可能一个对象中的一小部分（甚至几个像素）是可见的。 光照条件 光照的影响在像素级是非常剧烈的。 背景混乱 关注的对象可能融入周围的环境，使他们难以确定。 类内变化 关注的对象往往是比较宽泛的，比如椅子。这些对象中有许多不同的类型，每种类型都有自己的外观。 一个好的图像分类模型所有变量的叉积必须是不变的，同时对于类内变化保持敏感性。 数据驱动方法 我们要写一个什么样的算法才能将图像分为不同的类别？例如，不像写一个排序算法，人们写一个用于识别图像中猫的算法并不是显而易见的。因此，我们采取的方法和你会对小孩子采用的方法一样，而不是试图直接在代码中指定关注的物体的每一类是什么样的。我们给计算机提供每一类物体的很多样本，然后制定学习算法，看看这些例子，了解每个物体的视觉外观。这种方法被称为数据驱动方法，它依赖于已标记图像的训练数据集的最初积累。这里有一个关于这种数据集可能的例子： 这是四个类别训练集的例子。在实践中，我们可能有数以千计的类别，每个类别有成千上万个图像。 图像分类流程 我们看到，图像分类的任务是，输入代表一张图片的像素数组，并为其分配一个标签。我们完整的流程可以按以下形式表示： 输入 我们的输入N个图像，每个图像标为K个不同的类别中的一个。我们将此数据作为训练集。 学习 我们的任务就是利用训练集学习每一个类别是什么样子。我们把这个步骤叫做训练分类器，或学习一个模型。 评估 最后，我们通过让分类器预测一组它从未见过的新图像来评估分类器的质量。我们将用这些图像的真实标签和那些由分类预测的标签进行比较。直观地说，我们希望有大量的预测同真正答案（我们称之为基础事实）相匹配。 近邻分类器作为我们的第一个方法，我们将开发一个被称为近邻分类器的算法。这种分类和卷积神经网络没有关系，并且在实践中很少使用，但它使我们明白图像分类问题的基本方法。 图像分类数据集：CIFAR-10示例 一种流行的微型图像分类数据集CIFAR-10 dataset。此数据集包括60,000个微小的图像，这些图像是32像素x32像素。每个图像都标为十类（例如“飞机，汽车，鸟等”）中的一类。这60,000个图像被划分成包含50,000个图像的训练集和包含10,000个图像的测试集。下图中可以看到从十个类的每个类中都随机选取十个示例图片： 左：CIFAR-10 dataset中的示例图片，右：第一列显示了一些测试图像，接下来，根据根据逐像素差我们显示这些测试图像在训练集中前10个近邻。 假设现在我们得到CIFAR-10训练集中的50,000个图像（每个标签有5,000个图像），我们希望标注剩余的10,000。最近邻分类器将测试图像同训练集中的的每一个图像比较，并预测与其最接近的训练集图像的标签。在上图右侧，你可以看到10个测试图像处理后的结果。注意，仅约3/10的图像被正确检索到，而剩余的7/10没有被正确检索。例如，第八行中与马的头部最近邻的训练图像是一部红色的汽车，大概是由于汽车背景过于黑。其结果导致了一匹马在这种情况下被被误认为成一辆车。 你可能已经注意到，我们并未详细说明是如何比较两幅图片的，这个例子中图片的大小都是32×32×3。其中一个最简单的办法是按照像素进行比较，并把差值相加。换句话说，给定的两个图片并把他们当作矢量$I_1$，$I_2$，比较这两个图片一个合理的选择是L1 distance：$$d_1 (I_1, I_2) = \\sum_p \\left| I^p_1 - I^p_2 \\right|$$这里把所有差值相加，一下是具体的步骤： 这是采用逐像素差异来比较两个图像的L1 distance（这是一个颜色通道）的一个例子。两个图像按像素相减，然后所有差异相加。如果两个图像相同，结果将是零。但是如果图像有很大的不同，结果将非常大。 让我们看看如何用代码实现分类器。首先，加载CIFAR-10数据集到内存中，存储为4个数组：训练数据集及标签和测试数据集及标签。在下面的代码，Xtr（大小是50,000×32×32×3）存储训练集中的所有图像，相应的1维阵列Ytr（长度50,000）存储训练集的标签（从0到9）： 1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072 现在，我们把所有的图像都延展成行向量，以下是我们如何训练、评估分类器： 123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) ) 注意，作为评价的标准，通常使用精确度衡量正确预测的百分比。请注意，我们要建立的所有分类器，将使用这一个通用的API：train(X,y)，该函数获取数据及相应标签用以训练。分类器应该建立某种标签的模型，该模型可以用数据进行预测。接着一个predict(X)函数，该函数获取新的数据并预测其标签。当然，我们已经忽略了事物的主体–分类器本身。下面是一个简单的L1 distance近邻分类器的实现模板： 123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor: def __init__(self): pass def train(self, X, y): \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\" # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\" num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred 如果你运行这段代码，你会看到该分类器在CIFAR-10上只有38.6％的精确度。这是比随机猜测（10％的准确度，因为有10个类）要好一点，但是远不及人的行为（估计约为94％）或最好的卷积神经网络可以达到95％，和人的准确率相匹配（参考Kaggle比赛上最近CIFAR-10的排行榜）。选择distance 有许多计算两个向量之间距离的方法。另一种常见的选择是L2 distance，其几何解释就是计算两个向量之间的欧几里得距离。公式如下：$$d_2 (I_1, I_2) = \\sqrt{\\sum_p \\left( I^p_1 - I^p_2 \\right)^2}$$换句话说，我们会像以前那样计算逐像素的差异，但这次我们将所有差异值取平方然后相加，最后取平方根。在numpy中，使用上面代码我们只需要替换其中的一行。下面这行计算L2 distance：1distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) 注意，上述代码调用了np.sqrt，但在一个实用的近邻应用中，我们可以忽略求平方根操作，因为平方根是一个单调函数。也就是说，它扩展了距离的绝对值大小，但它保留了排序，所以有没有取平方根近邻都是相同的。如果你用这个距离的近邻分类器在CIFAR-10上运行，你将获得35.4％的准确度（略低于L1 distance的结果）。L1 vs. L2 考虑两个度量之间的差异很有意义。具体地，当涉及到两个向量之间的差异时L2比L1更严厉。也就是说L2相对于中等的差异更喜欢大的差异。L1和L2是p-norm特殊情况中最常用的。 k-近邻分类你可能已经注意到了，当我们做预测时，只用最近邻图像的标签，这非常奇怪。事实上，使用K-近邻分类器几乎总是可以得到更好的结果。我们的想法很简单：找到训练集中前k个最接近的图像，而不是找训练集中最接近的图像，并让它们和测试图像的标签一一对应。特别是，当k= 1时，得到最近邻分类器。直观地看，K值越高产生平滑作用，使分类更加耐异常值： 这是使用2维点和3个类（红，蓝，绿）展示的最近邻和5-近邻分类器之间的差异的例子。着色区域显示的决策边界是由使用L2 distance分类器产生的。白色区域中的点分类不清晰（即图中的点至少和两个类对应）。请注意，在最近邻分类器中异常数据点生成可能不正确的预测区域（例如蓝色区域中的绿点），而5-近邻分类器抚平了这些不正确的地方，可能会对测试数据产生更好的泛化（未示出）。 在实践中，你将几乎总是想用k近邻。但是，你应该使用什么样的k值？我们下面介绍这个问题。 超参数调整的验证集k-近邻分类器需要对设置k的值。但是值为多少效果最好？此外，我们看到，我们可以使用有许多不同距离函数，：L1，L2，还有很多其他我们甚至没有考虑的选择（如：点积）。这些选择被称为超参数，在设计很多机器学习算法时它们都会经常出现。但是选择什么样的值通常并不清楚。 你也许会认为，我们应该尝试许多不同的值，看看哪个效果最好。这是一个很好的想法，这确实是我们会做的，但这一定要非常谨慎。特别是，不能用测试集调整超参数。无论何时你设计机器学习算法，你应该把测试集当做非常宝贵的资源，应该树立只有在最后时刻才能使用它们的观念。否则，真正的危险是你可能在测试集上把你的超参数调整的很好，但一旦你部署你的模型，你会看到性能显著降低。在实践中，我们会说，你的测试集过拟合。另外一个看待这个问题的角度是，如果你在测试集上调整你的超参数，你实际上是把测试集当做训练集在使用，因此你获得的性能同你部署模型后真正观察到的性能相比会过于乐观。但是，如果你只在最后使用一次测试集，它仍然是能非常好的衡量你的分类器泛化能力（稍后我们会看到更多关于泛化能力的讨论）。Evaluate on the test set only a single time, at the very end.幸运的是，这有调整超参数的正确方式，并且不会触及测试集。想法是把训练集分成两部分：略小的训练集和我们所说的验证集。以CIFAR-10为例，我们可以使用训练集中49,000个图像进行训练，并留下1000个图像作为验证集。这个验证集基本上是一个用来调整超参数假的测试集。 以下是在CIFAR-10这个例子中用法： 123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc)) 这段程序结束后，我们可以绘制曲线图，显示其中效果最好的K值。然后，我们会一直使用这个值，并且在真正的测试集上评估一次。 把训练集分割成训练集和验证集。使用验证集调整所有的超参数。最后在测试集上运行一次并报告性能。 交叉验证 在训练数据集（因此也是验证数据集）规模较小的情况下，人们有时会使用更加先进的方法进行超参数调整，这种方法被称为交叉验证。继续前面的例子，这个想法是，为了避免随意挑选1,000个数据点作为验证集和余下的作为训练集，你可以通过遍历确定K值的不同验证集得到平均的性能，这样可以得到更好的更低噪音的估值。例如，在5次交叉验证中，我们将在训练数据分成5等份，使用其中份进行训练，1份进行验证。然后，我们将依次将每个等份作为验证集，评估性能，最后求不同等份性能的平均值。 参数k为5时，5倍交叉验证运行例子。Example of a 5-fold cross-validation run for the parameter k. 对于每个k值，我们在训练4倍和评估5日。因此，对于每K个收到5精度上验证倍（精度为y轴，每个结果是一个点）。For each value of k we train on 4 folds and evaluate on the 5th. Hence, for each k we receive 5 accuracies on the validation fold (accuracy is the y-axis, each result is a point). The trend line is drawn through the average of the results for each k and the error bars indicate the standard deviation. Note that in this particular case, the cross-validation suggests that a value of about k = 7 works best on this particular dataset (corresponding to the peak in the plot). If we used more than 5 folds, we might expect to see a smoother (i.e. less noisy) curve. 实践 在实践中，人们更喜欢用一次验证，而不是交叉验证，因为交叉验证的运算量非常大。人们倾向于使用50％-90％的训练数据进行训练，剩余的验证。然而，这取决于多种因素：例如，如果超参数的数目大则可能更喜欢使用更大验证集。如果在验证集合样本的数目小（可能只有几百个左右），使用交叉验证则更加安全。实践中典型的交叉验证数目是3倍，5倍或10倍交叉验证。 常用数据集。列出了训练和测试集。训练集被分成若干等份（例如这里5份）。1-4等份成为训练集。一个等份（如这里黄色的第5等份）作为验证集并且被用于调整超参数。交叉验证会分别从1-5等份挑选验证集。这被称为5-倍交叉验证。最后一旦训练好模型，确定好所有超参数，就用测试数据（红色）对该模型进行一次评价。 最近邻分类的优缺点 最近邻分类的优缺点值得思考。显然，一个优点是，很容易实现和理解。此外，这个分类器不花时间来训练，因为所需要的就是存储训练数据，有可能需要索引训练数据。但是，我们在测试时会付出计算成本，因为把测试集分类时需要和每一个训练样本进行比对。这是退步，因为在实践中，我们经常更加关心的测试的效率而不是训练时的效率。事实上，我们在后面课程中会涉及的深度神经网络把这个平衡推向了另一个极端：训练时代价很高，但是一旦训练结束后把一个新的测试样本分类就没有什么代价了。这种操作方式在实践中更加可取。 顺便说一句，最近邻分类器的计算复杂度是一个活跃的研究领域，并且几个近似最近邻（ANN）的算法和库存在，可以加速最近邻在数据集的查询（例如FLANN）。这些算法允许折中最邻近算法的正确率和算法的时空复杂度，算法通常依赖于预处理/索引阶段，涉及构建kd树，或运行k-means算法。最近邻分类有时在某些情况下（特别是如果数据是低维的）是一个不错的选择，但很少适合于图像分类问题。一个原因是图像是高维的对象（即它们通常含有许多像素），以及高维空间的距离是非常反直觉的。下图说明了这一点，我们上面开发的基于像素的L2相似度和感官的相似度有很大不同： 高维数据（尤其是图像）中基于像素的距离非常的不直观。原始图像（左）和它旁边三个图像的L2距离非常远。显然，逐像素距离在感官或语义相似度上根本不适用。 下面是多个图像来说服大家，用像素差异比较图像是不够的。我们可以使用一个名为t-SNE可视化技术处理CIFAR-10图像并将其嵌入二维中，and embed them in two dimensions so that their (local) pairwise distances are best preserved. 在该可视化中，显示出来相近的图像在上述我们开发出来的L2距离上非常接近： 使用t-SNE将CIFAR-10图像以二维方式展示。这幅图片中相邻的图片在L2像素距离上也是相近的。请注意背景的影响比真实的意义大的多。点击这里查看一个更大的可视化版本。 具体地，注意，互相接近的图片随着颜色或者背景不同而变动，而不是它们真实的意义。例如，可以看到一条狗非常靠近一个青蛙因为两者正好是白色背景。理想情况下，我们希望所有的10个类别的图像形成自己的集群，使同一类的图片都在附近彼此不分，而与其他的特点和变化（如背景）无关。然而，要达到这个性能，我们将必须超越原始像素。 总结 我们介绍了图像分类问题，这里我们有一组都标有一个单一类别的图像。我们接着预测新的测试图像的类别，并测量预测的准确性。 我们引入了一个简单的分类器叫做最近邻分类器。我们看到，和这个分类器相关的多个超参数（如k的值，或例子中用来比较的不同类型的距离），没有明显的办法选择他们。。 我们看到，正确设置这些超参数的方法是将训练数据分为两个：一个训练集和一个我们称之为验证集的假的测试集。我们尝试不同的超参数值，并保存在验证集上有最佳性能的值。 如果有缺乏训练数据的问题，我们讨论了一个叫做交叉验证的程序，它可以在评估超参数时帮助减少噪音。 一旦发现最佳超参数，我们解决了这些问题，并在实际的测试集执行一次评估。 我们看到，最近邻分类可以在CIFAR-10上得到约40％的精确度。这是简单的实现，但是要求我们存储整个训练集，并且在测试集评估时消耗非常大。 最后，我们看到，在原始像素上使用L1或L2距离并不合适，因为距离和背景、图像的颜色分布的相关性比它们的实际内容更强。 在接下来的课程，我们将着手解决这些挑战，并获得最终解决方案，可以达到90％的精确度，可以让我们在一次训练完成之后就彻底放弃训练集，使得我们能够在不到一毫秒内评估一个测试图像。 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://52ml.me/tags/cs231n/"}]},{"title":"Mac sed 坑","date":"2016-01-16T02:47:46.000Z","path":"2016/01/16/Mac-sed-坑/","text":"坑1执行1sed -i \"s/xxx/yyy/g\" filename 出现错误1sed: 1: invalid command code B 这种在Linux的用法不可以在Mac下直接使用，需要加上’’12sed -i '' \"s/xxx/yyy/g\" filename (不需要备份)sed -i '.bak' \"s/xxx/yyy/g\" filename (需要备份，备份为filename.bak) 坑2上述命令执行后仍有错误1sed: RE error: illegal byte sequence 需要在shell中加上：12export LC_COLLATE=&apos;C&apos;export LC_CTYPE=&apos;C&apos; 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[]},{"title":"学习计划","date":"2016-01-15T08:03:01.000Z","path":"2016/01/15/Learning-Plan/","text":"课程 课程 机构 进度 链接 课程记录 Machine Learning Stanford University(Cousera) 100% 机器学习 Machine Learning Washington University(Cousera) 33% 机器学习 Algorithms, Part I Princeton University (Cousera) 0% 算法 Convolutional Neural Networks for Visual Recognition Stanford University 0% 卷积神经网络的视觉识别 Deep Learning Udacity 0% 深度学习 Machine Learning: Reinforcement Learning Udacity 0% 强化学习 Statistical Learning Stanford University(edX) 0% 统计学习 Deep Learning for Natural Language Processing Stanford University 0% 在自然语言处理中使用深度学习 FP101x Introduction to Functional Programming Delft University of Technology(edX) 0% FP101x函数式编程介绍 scikit-learn 0% sklearn Kaggle 0% Kaggle 教材 博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[{"name":"学习计划","slug":"学习计划","permalink":"http://52ml.me/tags/学习计划/"}]},{"title":"Generalized Linear Models","date":"2016-01-13T14:47:02.000Z","path":"2016/01/13/Generalized-Linear-Models/","text":"博客地址：52ml.me原创文章，版权声明：自由转载-非商用-非衍生-保持署名 | Creative Commons BY-NC-ND 3.0","tags":[{"name":"ml","slug":"ml","permalink":"http://52ml.me/tags/ml/"}]}]